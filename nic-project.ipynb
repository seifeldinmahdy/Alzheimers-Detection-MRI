{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5962731,"sourceType":"datasetVersion","datasetId":3419493}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"239a2687","cell_type":"markdown","source":"## Install Dependencies\n\nInstall required packages (timm for EfficientNet, SHAP for explainability).","metadata":{}},{"id":"360c8365-780e-4f9e-b76a-76b0c3ef6ed7","cell_type":"code","source":"!pip install -q timm shap","metadata":{"language":"python","trusted":true,"execution":{"iopub.status.busy":"2025-12-24T06:27:10.221770Z","iopub.execute_input":"2025-12-24T06:27:10.221999Z","iopub.status.idle":"2025-12-24T06:27:16.484445Z","shell.execute_reply.started":"2025-12-24T06:27:10.221965Z","shell.execute_reply":"2025-12-24T06:27:16.483636Z"}},"outputs":[],"execution_count":1},{"id":"5c449f6a","cell_type":"markdown","source":"## GPU/CUDA Verification\n\nVerify GPU availability and CUDA configuration for accelerated training.","metadata":{}},{"id":"8dfacc6b-01ac-4bfa-904f-276a944853e4","cell_type":"code","source":"import torch\nimport subprocess\nimport platform\n\nprint('=' * 50)\nprint('SYSTEM INFO')\nprint('=' * 50)\nprint(f'Python: {platform.python_version()}')\nprint(f'PyTorch: {torch.__version__}')\nprint(f'CUDA available: {torch.cuda.is_available()}')\nprint(f'GPU count: {torch.cuda.device_count()}')\n\nif torch.cuda.is_available():\n    for i in range(torch.cuda.device_count()):\n        print(f'GPU {i}: {torch.cuda.get_device_name(i)}')\n    print(f'Current GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB')\n    \nprint('=' * 50)\n!nvidia-smi","metadata":{"language":"python","trusted":true,"execution":{"iopub.status.busy":"2025-12-24T06:27:16.486273Z","iopub.execute_input":"2025-12-24T06:27:16.486602Z","iopub.status.idle":"2025-12-24T06:27:23.757160Z","shell.execute_reply.started":"2025-12-24T06:27:16.486572Z","shell.execute_reply":"2025-12-24T06:27:23.756433Z"}},"outputs":[{"name":"stdout","text":"==================================================\nSYSTEM INFO\n==================================================\nPython: 3.12.12\nPyTorch: 2.8.0+cu126\nCUDA available: True\nGPU count: 2\nGPU 0: Tesla T4\nGPU 1: Tesla T4\nCurrent GPU memory: 15.83 GB\n==================================================\nWed Dec 24 06:27:23 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 570.172.08             Driver Version: 570.172.08     CUDA Version: 12.8     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   36C    P8             11W /   70W |       3MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   36C    P8              9W /   70W |       3MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":2},{"id":"43539d1b","cell_type":"markdown","source":"## Imports & Global Setup\n\nImport all required libraries and define global utility functions for reproducibility and device management.","metadata":{}},{"id":"3c5ea52f","cell_type":"code","source":"import os\nimport sys\nimport json\nimport time\nimport random\nimport hashlib\nimport warnings\nimport traceback\nimport gc\nimport shutil\nfrom datetime import datetime\nfrom dataclasses import dataclass, field, asdict\nfrom typing import Dict, List, Tuple, Optional, Callable, Any, Union\nfrom pathlib import Path\nfrom collections import Counter\nfrom copy import deepcopy\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib\nmatplotlib.use('Agg')  # Non-interactive backend for Kaggle\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Subset, WeightedRandomSampler\nfrom torch.cuda.amp import autocast, GradScaler\nimport torchvision.transforms as T\n\n# Attempt to import timm for EfficientNet\ntry:\n    import timm\n    TIMM_AVAILABLE = True\nexcept ImportError:\n    TIMM_AVAILABLE = False\n    print(\"Warning: timm not available. Will use torchvision EfficientNet.\")\n\n# Attempt to import torchvision models as fallback\nfrom torchvision import models as tv_models\n\n# sklearn for metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    accuracy_score, balanced_accuracy_score, f1_score,\n    precision_score, recall_score, confusion_matrix, classification_report\n)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\n\n# SHAP for explanations\ntry:\n    import shap\n    SHAP_AVAILABLE = True\nexcept ImportError:\n    SHAP_AVAILABLE = False\n    print(\"Warning: SHAP not available. SHAP explanations will be skipped.\")\n\n# CV2 for image processing (optional, use PIL as fallback)\ntry:\n    import cv2\n    CV2_AVAILABLE = True\nexcept ImportError:\n    CV2_AVAILABLE = False\n\n# Suppress warnings\nwarnings.filterwarnings('ignore')\n\n# Version info\nSCRIPT_VERSION = \"1.0.0\"\nprint(f\"Script version: {SCRIPT_VERSION}\")\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"NumPy version: {np.__version__}\")\nprint(f\"TIMM available: {TIMM_AVAILABLE}\")\nprint(f\"SHAP available: {SHAP_AVAILABLE}\")\n\ndef set_global_seed(seed: int):\n    \"\"\"Set all random seeds for reproducibility.\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\ndef get_device(preferred: str = \"auto\") -> torch.device:\n    \"\"\"Get computation device.\"\"\"\n    if preferred == \"auto\":\n        if torch.cuda.is_available():\n            return torch.device(\"cuda\")\n        else:\n            return torch.device(\"cpu\")\n    return torch.device(preferred)\n\n# Global constants\nCLASS_NAMES = [\"Mild Demented\", \"Moderate Demented\", \"Non Demented\", \"Very Mild Demented\"]\nNUM_CLASSES = 4\n\nprint(f\"\\nDevice: {get_device()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T06:27:23.758525Z","iopub.execute_input":"2025-12-24T06:27:23.759135Z","iopub.status.idle":"2025-12-24T06:27:38.701835Z","shell.execute_reply.started":"2025-12-24T06:27:23.759104Z","shell.execute_reply":"2025-12-24T06:27:38.701241Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Script version: 1.0.0\nPyTorch version: 2.8.0+cu126\nNumPy version: 2.0.2\nTIMM available: True\nSHAP available: True\n\nDevice: cuda\n","output_type":"stream"}],"execution_count":3},{"id":"88f9231c","cell_type":"markdown","source":"## Configuration\n\nDefine all hyperparameters, paths, and settings for the experiment. The `Config` dataclass centralizes all configuration options.","metadata":{}},{"id":"8ee83ca4","cell_type":"code","source":"\n\n@dataclass\nclass Config:\n    \"\"\"Centralized configuration for all experiments.\"\"\"\n    # Mode\n    TEST_RUN: bool = False  # Set to True for quick test, False for full run\n    \n    # Paths - KAGGLE SPECIFIC\n    DATA_DIR: str = \"/kaggle/input/imagesoasis\"  # Kaggle dataset path\n    OUTPUT_DIR: str = \"/kaggle/working/runs\"\n    \n    # Model\n    IMG_SIZE: int = 224\n    BACKBONE: str = \"efficientnet_b0\"\n    PRETRAINED: bool = True\n    DROPOUT: float = 0.3\n    \n    # Training\n    BATCH_SIZE: int = 32\n    EPOCHS: int = 5  # Balanced for quality vs speed\n    LR: float = 1e-3\n    WEIGHT_DECAY: float = 1e-4\n    OPTIMIZER: str = \"AdamW\"\n    USE_AMP: bool = True\n    NUM_WORKERS: int = 4  # Keep at 4 for Kaggle\n    \n    # Early stopping\n    EARLY_STOPPING_PATIENCE: int = 3  # Reasonable patience\n    \n    # Imbalance handling\n    USE_WEIGHTED_SAMPLER: bool = True\n    USE_CLASS_WEIGHTS: bool = True\n    USE_FOCAL_LOSS: bool = False\n    FOCAL_LOSS_GAMMA: float = 2.0\n    \n    # Augmentation\n    AUGMENT_STRENGTH: float = 1.0\n    \n    # Freeze/unfreeze\n    FREEZE_BACKBONE_EPOCHS: int = 2\n    \n    META_POPULATION: int = 12\n    META_ITERATIONS: int = 4  \n    META_EVAL_EPOCHS: int = 1 \n    \n    # ACO specific\n    ACO_ANTS: int = 20\n    ACO_ITERATIONS: int = 10\n    ACO_ALPHA: float = 1.0\n    ACO_BETA: float = 2.0\n    ACO_EVAPORATION: float = 0.5\n    ACO_FEATURE_FRACTION: float = 0.5\n    \n    # SA specific\n    SA_INITIAL_TEMP: float = 10.0\n    SA_COOLING_RATE: float = 0.8\n    SA_MIN_TEMP: float = 1.0\n    SA_ITERATIONS: int = 3\n    SKIP_PHASE2: bool = False\n    \n    # XAI\n    XAI_SAMPLES_PER_CLASS: int = 2  # 2 samples per class\n    GRADCAM_TARGET_LAYER: str = \"auto\"\n    SHAP_BACKGROUND_SIZE: int = 10  # Slightly increased\n    SHAP_NSAMPLES: int = 20  # Slightly increased\n    \n    # Reproducibility\n    SEED: int = 42\n    \n    # Subset for TEST_RUN\n    TEST_RUN_MAX_IMAGES: int = 500\n    TEST_RUN_IMAGES_PER_CLASS: int = 100\n    \n    # NEW: Balanced subset for full run (10,000 images)\n    USE_BALANCED_SUBSET: bool = True  # Enable balanced 10k subset\n    BALANCED_SUBSET_CONFIG: Dict = None  # Will be set in __post_init__\n\n    def __post_init__(self):\n        \"\"\"Apply TEST_RUN overrides and set balanced subset config.\"\"\"\n        # Set balanced subset configuration (7200 images total)\n        # Class distribution: Mild=1800, Moderate=488(all), Non=2712, VeryMild=2000\n        self.BALANCED_SUBSET_CONFIG = {\n            'Mild Dementia': 3512,\n            'Moderate Dementia': 488,  # Use all available\n            'Non Demented': 3000,\n            'Very mild Dementia': 3000\n        }\n        \n        if self.TEST_RUN:\n            self.EPOCHS = 1\n            self.META_POPULATION = 4  # Minimum 4 for DE algorithm\n            self.META_ITERATIONS = 1\n            self.META_EVAL_EPOCHS = 1\n            self.ACO_ANTS = 4\n            self.ACO_ITERATIONS = 1\n            self.EARLY_STOPPING_PATIENCE = 2\n            self.SA_ITERATIONS = 1\n            self.XAI_SAMPLES_PER_CLASS = 2\n            self.SHAP_BACKGROUND_SIZE = 10\n            self.SHAP_NSAMPLES = 20\n            self.USE_BALANCED_SUBSET = False  # Don't use balanced subset in test mode\n        else:\n            # Full run mode - use balanced subset\n            pass\n\n    def to_dict(self) -> Dict:\n        result = asdict(self)\n        # Convert BALANCED_SUBSET_CONFIG to serializable format\n        return result\n\n    def save(self, path: str):\n        with open(path, 'w') as f:\n            json.dump(self.to_dict(), f, indent=2)\n\n    @classmethod\n    def load(cls, path: str) -> 'Config':\n        with open(path, 'r') as f:\n            data = json.load(f)\n        return cls(**data)\n\ndef create_run_directory(base_dir: str) -> str:\n    \"\"\"Create timestamped run directory.\"\"\"\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    run_id = hashlib.md5(str(time.time()).encode()).hexdigest()[:8]\n    run_dir = os.path.join(base_dir, f\"{timestamp}_{run_id}\")\n    subdirs = ['checkpoints', 'logs', 'xai_outputs', 'plots']\n    for subdir in subdirs:\n        os.makedirs(os.path.join(run_dir, subdir), exist_ok=True)\n    return run_dir\n\n# ============================================================\n# CONFIGURATION - MODIFY THESE SETTINGS AS NEEDED\n# ============================================================\nTEST_RUN = False  # Set to True for quick test, False for full training\n\nconfig = Config(TEST_RUN=TEST_RUN)\nset_global_seed(config.SEED)\ndevice = get_device(\"auto\")\n\nprint(\"=\" * 50)\nprint(\"CONFIGURATION\")\nprint(\"=\" * 50)\nprint(f\"TEST_RUN: {config.TEST_RUN}\")\nprint(f\"DATA_DIR: {config.DATA_DIR}\")\nprint(f\"OUTPUT_DIR: {config.OUTPUT_DIR}\")\nprint(f\"EPOCHS: {config.EPOCHS}\")\nprint(f\"BATCH_SIZE: {config.BATCH_SIZE}\")\nprint(f\"Device: {device}\")\nprint(\"=\" * 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T06:27:38.702786Z","iopub.execute_input":"2025-12-24T06:27:38.703179Z","iopub.status.idle":"2025-12-24T06:27:38.726611Z","shell.execute_reply.started":"2025-12-24T06:27:38.703149Z","shell.execute_reply":"2025-12-24T06:27:38.725923Z"}},"outputs":[{"name":"stdout","text":"==================================================\nCONFIGURATION\n==================================================\nTEST_RUN: False\nDATA_DIR: /kaggle/input/imagesoasis\nOUTPUT_DIR: /kaggle/working/runs\nEPOCHS: 5\nBATCH_SIZE: 32\nDevice: cuda\n==================================================\n","output_type":"stream"}],"execution_count":4},{"id":"998639bc","cell_type":"markdown","source":"## Dataset & DataLoader Classes\n\nDefine custom dataset classes for loading OASIS Alzheimer MRI images and creating PyTorch DataLoaders with data augmentation.","metadata":{}},{"id":"b8da182b","cell_type":"code","source":"def find_data_directory(base_path: str) -> str:\n    \"\"\"Find the actual directory containing class folders.\"\"\"\n    possible_paths = [\n        base_path,\n        os.path.join(base_path, \"Data\"),\n        os.path.join(base_path, \"dataset\"),\n        os.path.join(base_path, \"train\"),\n        os.path.join(base_path, \"Dataset\"),\n    ]\n    for path in possible_paths:\n        if os.path.exists(path):\n            subdirs = [d for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]\n            if any('demented' in d.lower() for d in subdirs):\n                return path\n            if 'train' in subdirs:\n                train_path = os.path.join(path, 'train')\n                return train_path\n    for root, dirs, files in os.walk(base_path):\n        if any('demented' in d.lower() for d in dirs):\n            return root\n    return base_path\n\nclass OASISDataset(Dataset):\n    \"\"\"Custom dataset for OASIS Alzheimer MRI images.\"\"\"\n    def __init__(self, image_paths: List[str], labels: List[int], transform=None):\n        self.image_paths = image_paths\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        label = self.labels[idx]\n        try:\n            image = Image.open(img_path).convert('RGB')\n        except Exception as e:\n            print(f\"Error loading {img_path}: {e}\")\n            image = Image.new('RGB', (224, 224), color='black')\n        if self.transform:\n            image = self.transform(image)\n        return image, label\n\ndef load_dataset(config: Config, data_dir: str) -> Tuple[Dict, Dict, Dict]:\n    \"\"\"Load and split dataset.\"\"\"\n    print(\"\\n\" + \"=\"*50)\n    print(\"LOADING DATASET\")\n    print(\"=\"*50)\n    data_dir = find_data_directory(data_dir)\n    print(f\"Using data directory: {data_dir}\")\n\n    image_paths = []\n    labels = []\n    class_to_idx = {}\n    subdirs = sorted([d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))])\n    print(f\"Found subdirectories: {subdirs}\")\n\n    idx = 0\n    for subdir in subdirs:\n        subdir_lower = subdir.lower().replace('_', ' ').replace('-', ' ')\n        if 'demented' in subdir_lower or 'dementia' in subdir_lower:\n            class_to_idx[subdir] = idx\n            idx += 1\n    if not class_to_idx:\n        class_to_idx = {d: i for i, d in enumerate(subdirs)}\n    print(f\"Class mapping: {class_to_idx}\")\n\n    idx_to_class = {v: k for k, v in class_to_idx.items()}\n\n    for class_name, class_idx in class_to_idx.items():\n        class_dir = os.path.join(data_dir, class_name)\n        if not os.path.isdir(class_dir):\n            continue\n        for img_name in os.listdir(class_dir):\n            if img_name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n                img_path = os.path.join(class_dir, img_name)\n                image_paths.append(img_path)\n                labels.append(class_idx)\n\n    print(f\"\\nTotal images found: {len(image_paths)}\")\n    class_counts = Counter(labels)\n    print(\"\\nClass distribution:\")\n    for idx, count in sorted(class_counts.items()):\n        print(f\"  {idx_to_class.get(idx, idx)}: {count}\")\n\n    if config.TEST_RUN:\n        print(f\"\\nTEST_RUN mode: limiting to {config.TEST_RUN_MAX_IMAGES} images\")\n        subset_paths = []\n        subset_labels = []\n        for class_idx in class_to_idx.values():\n            class_mask = [i for i, l in enumerate(labels) if l == class_idx]\n            n_take = min(len(class_mask), config.TEST_RUN_IMAGES_PER_CLASS)\n            if n_take > 0:\n                selected = random.sample(class_mask, n_take)\n                for i in selected:\n                    subset_paths.append(image_paths[i])\n                    subset_labels.append(labels[i])\n        image_paths = subset_paths\n        labels = subset_labels\n        print(f\"Subset size: {len(image_paths)}\")\n    \n    # Apply balanced subset for full run (10,000 images)\n    elif config.USE_BALANCED_SUBSET and config.BALANCED_SUBSET_CONFIG:\n        print(f\"\\nApplying balanced subset (target: 10,000 images)\")\n        subset_paths = []\n        subset_labels = []\n        \n        for class_name, class_idx in class_to_idx.items():\n            class_mask = [i for i, l in enumerate(labels) if l == class_idx]\n            \n            # Get target count for this class\n            target_count = config.BALANCED_SUBSET_CONFIG.get(class_name, len(class_mask))\n            n_take = min(len(class_mask), target_count)\n            \n            if n_take > 0:\n                selected = random.sample(class_mask, n_take)\n                for i in selected:\n                    subset_paths.append(image_paths[i])\n                    subset_labels.append(labels[i])\n            \n            print(f\"  {class_name}: {n_take} images (target: {target_count}, available: {len(class_mask)})\")\n        \n        image_paths = subset_paths\n        labels = subset_labels\n        \n        # Recalculate class distribution\n        class_counts = Counter(labels)\n        print(f\"\\nBalanced subset - Total: {len(image_paths)} images\")\n        print(\"New class distribution:\")\n        for idx, count in sorted(class_counts.items()):\n            print(f\"  {idx_to_class.get(idx, idx)}: {count}\")\n\n    if not config.TEST_RUN and len(image_paths) < 7000:\n        print(f\"Note: Using {len(image_paths)} samples for training.\")\n\n    train_paths, temp_paths, train_labels, temp_labels = train_test_split(\n        image_paths, labels, test_size=0.3, stratify=labels, random_state=config.SEED\n    )\n    val_paths, test_paths, val_labels, test_labels = train_test_split(\n        temp_paths, temp_labels, test_size=0.5, stratify=temp_labels, random_state=config.SEED\n    )\n\n    print(f\"\\nSplit sizes:\")\n    print(f\"  Train: {len(train_paths)}\")\n    print(f\"  Val: {len(val_paths)}\")\n    print(f\"  Test: {len(test_paths)}\")\n\n    normalize = T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    aug_strength = config.AUGMENT_STRENGTH\n\n    train_transform = T.Compose([\n        T.Resize((config.IMG_SIZE + 32, config.IMG_SIZE + 32)),\n        T.RandomResizedCrop(config.IMG_SIZE, scale=(0.8, 1.0)),\n        T.RandomHorizontalFlip(p=0.5 * aug_strength),\n        T.RandomRotation(degrees=int(15 * aug_strength)),\n        T.ColorJitter(\n            brightness=0.2 * aug_strength,\n            contrast=0.2 * aug_strength,\n            saturation=0.1 * aug_strength\n        ),\n        T.ToTensor(),\n        normalize\n    ])\n\n    val_transform = T.Compose([\n        T.Resize((config.IMG_SIZE, config.IMG_SIZE)),\n        T.ToTensor(),\n        normalize\n    ])\n\n    train_dataset = OASISDataset(train_paths, train_labels, train_transform)\n    val_dataset = OASISDataset(val_paths, val_labels, val_transform)\n    test_dataset = OASISDataset(test_paths, test_labels, val_transform)\n\n    class_counts_train = Counter(train_labels)\n    total_train = len(train_labels)\n    class_weights = torch.zeros(len(class_to_idx))\n    for idx in range(len(class_to_idx)):\n        count = class_counts_train.get(idx, 1)\n        class_weights[idx] = total_train / (len(class_to_idx) * count)\n\n    sample_weights = [class_weights[l].item() for l in train_labels]\n\n    datasets = {\n        'train': train_dataset,\n        'val': val_dataset,\n        'test': test_dataset\n    }\n\n    metadata = {\n        'class_to_idx': class_to_idx,\n        'idx_to_class': idx_to_class,\n        'class_weights': class_weights,\n        'sample_weights': sample_weights,\n        'class_counts': dict(class_counts),\n        'train_paths': train_paths,\n        'train_labels': train_labels,\n        'val_paths': val_paths,\n        'val_labels': val_labels,\n        'test_paths': test_paths,\n        'test_labels': test_labels\n    }\n\n    transforms = {\n        'train': train_transform,\n        'val': val_transform,\n        'test': val_transform\n    }\n\n    return datasets, metadata, transforms\n\ndef create_dataloaders(datasets: Dict, metadata: Dict, config: Config) -> Dict[str, DataLoader]:\n    \"\"\"Create data loaders with optional weighted sampling.\"\"\"\n    loaders = {}\n\n    if config.USE_WEIGHTED_SAMPLER and 'sample_weights' in metadata:\n        sampler = WeightedRandomSampler(\n            weights=metadata['sample_weights'],\n            num_samples=len(metadata['sample_weights']),\n            replacement=True\n        )\n        loaders['train'] = DataLoader(\n            datasets['train'],\n            batch_size=config.BATCH_SIZE,\n            sampler=sampler,\n            num_workers=config.NUM_WORKERS,\n            pin_memory=True\n        )\n    else:\n        loaders['train'] = DataLoader(\n            datasets['train'],\n            batch_size=config.BATCH_SIZE,\n            shuffle=True,\n            num_workers=config.NUM_WORKERS,\n            pin_memory=True\n        )\n\n    loaders['val'] = DataLoader(\n        datasets['val'],\n        batch_size=config.BATCH_SIZE,\n        shuffle=False,\n        num_workers=config.NUM_WORKERS,\n        pin_memory=True\n    )\n\n    loaders['test'] = DataLoader(\n        datasets['test'],\n        batch_size=config.BATCH_SIZE,\n        shuffle=False,\n        num_workers=config.NUM_WORKERS,\n        pin_memory=True\n    )\n\n    return loaders\n\nprint(\"Dataset and DataLoader classes defined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T06:27:38.727642Z","iopub.execute_input":"2025-12-24T06:27:38.728086Z","iopub.status.idle":"2025-12-24T06:27:38.758571Z","shell.execute_reply.started":"2025-12-24T06:27:38.728061Z","shell.execute_reply":"2025-12-24T06:27:38.757959Z"}},"outputs":[{"name":"stdout","text":"Dataset and DataLoader classes defined.\n","output_type":"stream"}],"execution_count":5},{"id":"f92a2c5f","cell_type":"markdown","source":"## Model Definition\n\nDefine the EfficientNet-B0 based classifier with feature masking capability for ACO feature selection.","metadata":{}},{"id":"0e63f25f","cell_type":"code","source":"class FocalLoss(nn.Module):\n    \"\"\"Focal Loss for imbalanced classification.\"\"\"\n    def __init__(self, alpha: Optional[torch.Tensor] = None, gamma: float = 2.0):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n\n    def forward(self, inputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n        ce_loss = F.cross_entropy(inputs, targets, weight=self.alpha, reduction='none')\n        pt = torch.exp(-ce_loss)\n        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n        return focal_loss.mean()\n\nclass AlzheimerClassifier(nn.Module):\n    \"\"\"EfficientNet-B0 based classifier for Alzheimer's classification.\"\"\"\n    def __init__(self, num_classes: int = 4, dropout: float = 0.3, pretrained: bool = True):\n        super().__init__()\n        if TIMM_AVAILABLE:\n            self.backbone = timm.create_model('efficientnet_b0', pretrained=pretrained)\n            if hasattr(self.backbone, 'classifier'):\n                in_features = self.backbone.classifier.in_features\n                self.backbone.classifier = nn.Identity()\n            elif hasattr(self.backbone, 'fc'):\n                in_features = self.backbone.fc.in_features\n                self.backbone.fc = nn.Identity()\n            else:\n                in_features = self.backbone.num_features\n        else:\n            self.backbone = tv_models.efficientnet_b0(\n                weights=tv_models.EfficientNet_B0_Weights.IMAGENET1K_V1 if pretrained else None\n            )\n            in_features = self.backbone.classifier[1].in_features\n            self.backbone.classifier = nn.Identity()\n\n        self.classifier = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Linear(in_features, 256),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout / 2),\n            nn.Linear(256, num_classes)\n        )\n        self.in_features = in_features\n        self._feature_mask = None\n\n    def set_feature_mask(self, mask: Optional[torch.Tensor]):\n        \"\"\"Set feature mask for ACO feature selection.\"\"\"\n        self._feature_mask = mask\n\n    def get_features(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"Extract backbone features.\"\"\"\n        return self.backbone(x)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        features = self.backbone(x)\n        if self._feature_mask is not None:\n            features = features * self._feature_mask.to(features.device)\n        return self.classifier(features)\n\n    def freeze_backbone(self):\n        \"\"\"Freeze backbone parameters.\"\"\"\n        for param in self.backbone.parameters():\n            param.requires_grad = False\n\n    def unfreeze_backbone(self):\n        \"\"\"Unfreeze backbone parameters.\"\"\"\n        for param in self.backbone.parameters():\n            param.requires_grad = True\n\ndef count_parameters(model: nn.Module) -> Tuple[int, int]:\n    \"\"\"Count total and trainable parameters.\"\"\"\n    total = sum(p.numel() for p in model.parameters())\n    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    return total, trainable\n\ndef create_model(config: Config, device: torch.device) -> nn.Module:\n    \"\"\"Create and initialize model (single GPU).\"\"\"\n    model = AlzheimerClassifier(\n        num_classes=NUM_CLASSES,\n        dropout=config.DROPOUT,\n        pretrained=config.PRETRAINED\n    )\n    total, trainable = count_parameters(model)\n    print(f\"\\nModel created:\")\n    print(f\"  Total parameters: {total:,}\")\n    print(f\"  Trainable parameters: {trainable:,}\")\n    \n    model = model.to(device)\n    \n    return model\n\nprint(\"Model classes defined.\")    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T06:27:38.759295Z","iopub.execute_input":"2025-12-24T06:27:38.759756Z","iopub.status.idle":"2025-12-24T06:27:38.783231Z","shell.execute_reply.started":"2025-12-24T06:27:38.759734Z","shell.execute_reply":"2025-12-24T06:27:38.782565Z"}},"outputs":[{"name":"stdout","text":"Model classes defined.\n","output_type":"stream"}],"execution_count":6},{"id":"c33f2217","cell_type":"markdown","source":"## Training & Evaluation Functions\n\nDefine functions for training epochs, model evaluation, and the complete training loop with early stopping.","metadata":{}},{"id":"e33b12f1","cell_type":"code","source":"def train_one_epoch(\n    model: nn.Module,\n    loader: DataLoader,\n    criterion: nn.Module,\n    optimizer: torch.optim.Optimizer,\n    device: torch.device,\n    scaler: Optional[GradScaler] = None,\n    use_amp: bool = True\n) -> Dict[str, float]:\n    \"\"\"Train for one epoch.\"\"\"\n    model.train()\n    total_loss = 0.0\n    all_preds = []\n    all_labels = []\n    num_batches = len(loader)\n\n    for batch_idx, (images, labels) in enumerate(loader):\n        if batch_idx == 0:\n            print(f\"    First batch loaded, starting training...\", flush=True)\n        \n        images = images.to(device)\n        labels = labels.to(device)\n\n        optimizer.zero_grad()\n\n        if use_amp and scaler is not None:\n            with autocast():\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n        else:\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n        total_loss += loss.item()\n\n        preds = outputs.argmax(dim=1).cpu().numpy()\n        all_preds.extend(preds)\n        all_labels.extend(labels.cpu().numpy())\n        \n        # Progress indicator every 10 batches\n        if (batch_idx + 1) % 10 == 0 or batch_idx == num_batches - 1:\n            print(f\"    Batch {batch_idx + 1}/{num_batches}, Loss: {loss.item():.4f}\", flush=True)\n\n    avg_loss = total_loss / len(loader)\n    accuracy = accuracy_score(all_labels, all_preds)\n    balanced_acc = balanced_accuracy_score(all_labels, all_preds)\n    macro_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n\n    return {\n        'loss': avg_loss,\n        'accuracy': accuracy,\n        'balanced_accuracy': balanced_acc,\n        'macro_f1': macro_f1\n    }\n\ndef evaluate(\n    model: nn.Module,\n    loader: DataLoader,\n    criterion: nn.Module,\n    device: torch.device,\n    use_amp: bool = True\n) -> Dict[str, Any]:\n    \"\"\"Evaluate model on a dataset.\"\"\"\n    model.eval()\n    total_loss = 0.0\n    all_preds = []\n    all_labels = []\n    all_probs = []\n\n    with torch.no_grad():\n        for images, labels in loader:\n            images = images.to(device)\n            labels = labels.to(device)\n\n            if use_amp:\n                with autocast():\n                    outputs = model(images)\n                    loss = criterion(outputs, labels)\n            else:\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n\n            total_loss += loss.item()\n            probs = F.softmax(outputs, dim=1).cpu().numpy()\n            preds = outputs.argmax(dim=1).cpu().numpy()\n            all_probs.extend(probs)\n            all_preds.extend(preds)\n            all_labels.extend(labels.cpu().numpy())\n\n    avg_loss = total_loss / len(loader)\n    accuracy = accuracy_score(all_labels, all_preds)\n    balanced_acc = balanced_accuracy_score(all_labels, all_preds)\n    macro_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n    precision = precision_score(all_labels, all_preds, average=None, zero_division=0)\n    recall = recall_score(all_labels, all_preds, average=None, zero_division=0)\n    f1_per_class = f1_score(all_labels, all_preds, average=None, zero_division=0)\n    cm = confusion_matrix(all_labels, all_preds)\n\n    return {\n        'loss': avg_loss,\n        'accuracy': accuracy,\n        'balanced_accuracy': balanced_acc,\n        'macro_f1': macro_f1,\n        'precision_per_class': precision.tolist(),\n        'recall_per_class': recall.tolist(),\n        'f1_per_class': f1_per_class.tolist(),\n        'confusion_matrix': cm.tolist(),\n        'predictions': all_preds,\n        'labels': all_labels,\n        'probabilities': np.array(all_probs)\n    }\n\ndef predict_proba(\n    model: nn.Module,\n    loader: DataLoader,\n    device: torch.device\n) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Get prediction probabilities.\"\"\"\n    model.eval()\n    all_probs = []\n    all_labels = []\n\n    with torch.no_grad():\n        for images, labels in loader:\n            images = images.to(device)\n            outputs = model(images)\n            probs = F.softmax(outputs, dim=1).cpu().numpy()\n            all_probs.extend(probs)\n            all_labels.extend(labels.numpy())\n\n    return np.array(all_probs), np.array(all_labels)\n\ndef train_model(\n    model: nn.Module,\n    loaders: Dict[str, DataLoader],\n    config: Config,\n    metadata: Dict,\n    device: torch.device,\n    run_dir: str,\n    epochs: Optional[int] = None,\n    verbose: bool = True\n) -> Tuple[nn.Module, Dict, List[Dict]]:\n    \"\"\"Full training loop with early stopping.\"\"\"\n    epochs = epochs or config.EPOCHS\n\n    class_weights = metadata['class_weights'].to(device) if config.USE_CLASS_WEIGHTS else None\n    if config.USE_FOCAL_LOSS:\n        criterion = FocalLoss(alpha=class_weights, gamma=config.FOCAL_LOSS_GAMMA)\n    else:\n        criterion = nn.CrossEntropyLoss(weight=class_weights)\n\n    if config.OPTIMIZER.lower() == 'adamw':\n        optimizer = torch.optim.AdamW(\n            model.parameters(),\n            lr=config.LR,\n            weight_decay=config.WEIGHT_DECAY\n        )\n    else:\n        optimizer = torch.optim.Adam(\n            model.parameters(),\n            lr=config.LR,\n            weight_decay=config.WEIGHT_DECAY\n        )\n\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='max', factor=0.5, patience=2\n    )\n\n    scaler = GradScaler() if config.USE_AMP and device.type == 'cuda' else None\n\n    history = []\n    best_val_f1 = 0.0\n    best_epoch = 0\n    patience_counter = 0\n    best_model_state = None\n\n    if config.FREEZE_BACKBONE_EPOCHS > 0:\n        model.freeze_backbone()\n        if verbose:\n            print(f\"Backbone frozen for first {config.FREEZE_BACKBONE_EPOCHS} epochs\")\n\n    for epoch in range(epochs):\n        start_time = time.time()\n        if verbose:\n            print(f\"\\n  Starting Epoch {epoch + 1}/{epochs}...\", flush=True)\n\n        if epoch == config.FREEZE_BACKBONE_EPOCHS:\n            model.unfreeze_backbone()\n            if verbose:\n                print(\"Backbone unfrozen\")\n\n        train_metrics = train_one_epoch(\n            model, loaders['train'], criterion, optimizer, device,\n            scaler=scaler, use_amp=config.USE_AMP\n        )\n\n        val_metrics = evaluate(\n            model, loaders['val'], criterion, device, use_amp=config.USE_AMP\n        )\n\n        epoch_time = time.time() - start_time\n        scheduler.step(val_metrics['macro_f1'])\n\n        history.append({\n            'epoch': epoch + 1,\n            'train_loss': train_metrics['loss'],\n            'train_acc': train_metrics['accuracy'],\n            'train_f1': train_metrics['macro_f1'],\n            'val_loss': val_metrics['loss'],\n            'val_acc': val_metrics['accuracy'],\n            'val_balanced_acc': val_metrics['balanced_accuracy'],\n            'val_f1': val_metrics['macro_f1'],\n            'epoch_time': epoch_time,\n            'lr': optimizer.param_groups[0]['lr']\n        })\n\n        if verbose:\n            print(f\"Epoch {epoch+1}/{epochs} ({epoch_time:.1f}s) - \"\n                  f\"Train Loss: {train_metrics['loss']:.4f}, \"\n                  f\"Val F1: {val_metrics['macro_f1']:.4f}, \"\n                  f\"Val Acc: {val_metrics['accuracy']:.4f}\")\n\n        # Memory cleanup\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        gc.collect()\n\n        if val_metrics['macro_f1'] > best_val_f1:\n            best_val_f1 = val_metrics['macro_f1']\n            best_epoch = epoch + 1\n            best_model_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n            patience_counter = 0\n            checkpoint_path = os.path.join(run_dir, 'checkpoints', 'best.pt')\n            torch.save({\n                'epoch': epoch + 1,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'val_f1': best_val_f1,\n                'config': config.to_dict()\n            }, checkpoint_path)\n        else:\n            patience_counter += 1\n\n        if patience_counter >= config.EARLY_STOPPING_PATIENCE:\n            if verbose:\n                print(f\"Early stopping at epoch {epoch + 1}\")\n            break\n\n    if best_model_state is not None:\n        model.load_state_dict(best_model_state)\n\n    test_metrics = evaluate(model, loaders['test'], criterion, device, use_amp=config.USE_AMP)\n\n    final_metrics = {\n        'best_epoch': best_epoch,\n        'best_val_f1': best_val_f1,\n        'test_accuracy': test_metrics['accuracy'],\n        'test_balanced_accuracy': test_metrics['balanced_accuracy'],\n        'test_macro_f1': test_metrics['macro_f1'],\n        'test_f1_per_class': test_metrics['f1_per_class'],\n        'test_confusion_matrix': test_metrics['confusion_matrix']\n    }\n\n    save_training_curves(history, run_dir)\n    pd.DataFrame(history).to_csv(\n        os.path.join(run_dir, 'logs', 'training_history.csv'),\n        index=False\n    )\n\n    return model, final_metrics, history\n\ndef save_training_curves(history: List[Dict], run_dir: str):\n    \"\"\"Save training curves as plots.\"\"\"\n    df = pd.DataFrame(history)\n    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n    axes[0, 0].plot(df['epoch'], df['train_loss'], label='Train')\n    axes[0, 0].plot(df['epoch'], df['val_loss'], label='Val')\n    axes[0, 0].set_xlabel('Epoch')\n    axes[0, 0].set_ylabel('Loss')\n    axes[0, 0].set_title('Loss Curves')\n    axes[0, 0].legend()\n    axes[0, 0].grid(True)\n\n    axes[0, 1].plot(df['epoch'], df['train_acc'], label='Train')\n    axes[0, 1].plot(df['epoch'], df['val_acc'], label='Val')\n    axes[0, 1].set_xlabel('Epoch')\n    axes[0, 1].set_ylabel('Accuracy')\n    axes[0, 1].set_title('Accuracy Curves')\n    axes[0, 1].legend()\n    axes[0, 1].grid(True)\n\n    axes[1, 0].plot(df['epoch'], df['train_f1'], label='Train')\n    axes[1, 0].plot(df['epoch'], df['val_f1'], label='Val')\n    axes[1, 0].set_xlabel('Epoch')\n    axes[1, 0].set_ylabel('Macro F1')\n    axes[1, 0].set_title('F1 Score Curves')\n    axes[1, 0].legend()\n    axes[1, 0].grid(True)\n\n    axes[1, 1].plot(df['epoch'], df['lr'])\n    axes[1, 1].set_xlabel('Epoch')\n    axes[1, 1].set_ylabel('Learning Rate')\n    axes[1, 1].set_title('Learning Rate Schedule')\n    axes[1, 1].grid(True)\n\n    plt.tight_layout()\n    plt.savefig(os.path.join(run_dir, 'plots', 'training_curves.png'), dpi=150)\n    plt.close()\n\nprint(\"Training & Evaluation functions defined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T06:27:38.786021Z","iopub.execute_input":"2025-12-24T06:27:38.786234Z","iopub.status.idle":"2025-12-24T06:27:38.815532Z","shell.execute_reply.started":"2025-12-24T06:27:38.786215Z","shell.execute_reply":"2025-12-24T06:27:38.814999Z"}},"outputs":[{"name":"stdout","text":"Training & Evaluation functions defined.\n","output_type":"stream"}],"execution_count":7},{"id":"fc24f6ef","cell_type":"markdown","source":"## Visualization & Comparison Plots\n\nDefine functions for plotting algorithm comparison charts, convergence curves, and performance visualizations.","metadata":{}},{"id":"f2e882f6","cell_type":"code","source":"def plot_algorithm_comparison(results_df: pd.DataFrame, run_dir: str, title: str = \"Phase 1\"):\n    \"\"\"Create bar chart comparing algorithm performance.\"\"\"\n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    \n    # Sort by score for better visualization\n    df_sorted = results_df.sort_values('best_score', ascending=False)\n    algorithms = df_sorted['algorithm'].tolist()\n    scores = df_sorted['best_score'].tolist()\n    times = df_sorted['time_seconds'].tolist()\n    \n    # Color palette\n    colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(algorithms)))\n    \n    # Plot 1: Scores\n    bars1 = axes[0].bar(algorithms, scores, color=colors, edgecolor='black', linewidth=1.2)\n    axes[0].set_xlabel('Algorithm', fontsize=12)\n    axes[0].set_ylabel('Best Score (Macro F1)', fontsize=12)\n    axes[0].set_title(f'{title}: Algorithm Performance Comparison', fontsize=14, fontweight='bold')\n    axes[0].set_ylim(0, max(scores) * 1.15 if scores else 1)\n    axes[0].grid(axis='y', alpha=0.3)\n    \n    # Add value labels on bars\n    for bar, score in zip(bars1, scores):\n        axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n                     f'{score:.4f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n    \n    # Highlight best algorithm\n    if scores:\n        best_idx = scores.index(max(scores))\n        bars1[best_idx].set_edgecolor('gold')\n        bars1[best_idx].set_linewidth(3)\n    \n    # Plot 2: Time\n    bars2 = axes[1].bar(algorithms, times, color=colors, edgecolor='black', linewidth=1.2)\n    axes[1].set_xlabel('Algorithm', fontsize=12)\n    axes[1].set_ylabel('Time (seconds)', fontsize=12)\n    axes[1].set_title(f'{title}: Algorithm Execution Time', fontsize=14, fontweight='bold')\n    axes[1].grid(axis='y', alpha=0.3)\n    \n    # Add value labels on bars\n    for bar, t in zip(bars2, times):\n        axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n                     f'{t:.1f}s', ha='center', va='bottom', fontsize=10)\n    \n    plt.tight_layout()\n    filename = f'{title.lower().replace(\" \", \"_\")}_algorithm_comparison.png'\n    plt.savefig(os.path.join(run_dir, 'plots', filename), dpi=150, bbox_inches='tight')\n    plt.close()\n    print(f\"  Saved: plots/{filename}\")\n\n\ndef plot_convergence_curves(algo_histories: Dict[str, List[float]], run_dir: str, title: str = \"Convergence\"):\n    \"\"\"Plot convergence curves for multiple algorithms.\"\"\"\n    if not algo_histories:\n        return\n    \n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    colors = plt.cm.tab10(np.linspace(0, 1, len(algo_histories)))\n    markers = ['o', 's', '^', 'D', 'v', '<', '>', 'p']\n    \n    for i, (algo_name, history) in enumerate(algo_histories.items()):\n        if history:\n            iterations = range(1, len(history) + 1)\n            ax.plot(iterations, history, label=algo_name, color=colors[i],\n                   marker=markers[i % len(markers)], markersize=6, linewidth=2)\n    \n    ax.set_xlabel('Iteration', fontsize=12)\n    ax.set_ylabel('Best Score (Macro F1)', fontsize=12)\n    ax.set_title(f'{title} Curves', fontsize=14, fontweight='bold')\n    ax.legend(loc='lower right', fontsize=10)\n    ax.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    filename = f'{title.lower().replace(\" \", \"_\")}_curves.png'\n    plt.savefig(os.path.join(run_dir, 'plots', filename), dpi=150, bbox_inches='tight')\n    plt.close()\n    print(f\"  Saved: plots/{filename}\")\n\n\ndef plot_metaheuristic_ranking(rankings_df: pd.DataFrame, run_dir: str):\n    \"\"\"Create comprehensive ranking visualization.\"\"\"\n    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n    \n    # Prepare data\n    df = rankings_df.sort_values('rank').reset_index(drop=True)\n    algorithms = df['algorithm'].tolist()\n    scores = df['score'].tolist()\n    times = df['time'].tolist()\n    efficiency = df['efficiency'].tolist() if 'efficiency' in df.columns else [s/(t+1e-6) for s, t in zip(scores, times)]\n    \n    colors = plt.cm.RdYlGn(np.linspace(0.2, 0.8, len(algorithms)))[::-1]  # Green for best\n    \n    # Plot 1: Horizontal bar chart of scores (ranked)\n    bars1 = axes[0, 0].barh(algorithms[::-1], scores[::-1], color=colors[::-1], edgecolor='black')\n    axes[0, 0].set_xlabel('Score (Macro F1)', fontsize=11)\n    axes[0, 0].set_title('Algorithm Ranking by Score', fontsize=12, fontweight='bold')\n    axes[0, 0].grid(axis='x', alpha=0.3)\n    for bar, score in zip(bars1, scores[::-1]):\n        axes[0, 0].text(bar.get_width() + 0.005, bar.get_y() + bar.get_height()/2,\n                        f'{score:.4f}', va='center', fontsize=9)\n    \n    # Plot 2: Scatter plot (Score vs Time)\n    scatter = axes[0, 1].scatter(times, scores, c=range(len(algorithms)), cmap='RdYlGn_r', \n                                  s=200, edgecolor='black', linewidth=1.5)\n    for i, algo in enumerate(algorithms):\n        axes[0, 1].annotate(algo, (times[i], scores[i]), xytext=(5, 5), \n                            textcoords='offset points', fontsize=9)\n    axes[0, 1].set_xlabel('Time (seconds)', fontsize=11)\n    axes[0, 1].set_ylabel('Score (Macro F1)', fontsize=11)\n    axes[0, 1].set_title('Score vs Execution Time', fontsize=12, fontweight='bold')\n    axes[0, 1].grid(True, alpha=0.3)\n    \n    # Plot 3: Efficiency bar chart\n    eff_sorted_idx = np.argsort(efficiency)[::-1]\n    eff_algos = [algorithms[i] for i in eff_sorted_idx]\n    eff_vals = [efficiency[i] for i in eff_sorted_idx]\n    eff_colors = plt.cm.coolwarm(np.linspace(0.2, 0.8, len(algorithms)))\n    \n    bars3 = axes[1, 0].bar(eff_algos, eff_vals, color=eff_colors, edgecolor='black')\n    axes[1, 0].set_xlabel('Algorithm', fontsize=11)\n    axes[1, 0].set_ylabel('Efficiency (Score/Time)', fontsize=11)\n    axes[1, 0].set_title('Algorithm Efficiency', fontsize=12, fontweight='bold')\n    axes[1, 0].tick_params(axis='x', rotation=45)\n    axes[1, 0].grid(axis='y', alpha=0.3)\n    \n    # Plot 4: Summary table as text\n    axes[1, 1].axis('off')\n    table_data = []\n    for i, (algo, score, t, eff) in enumerate(zip(algorithms, scores, times, efficiency)):\n        table_data.append([i+1, algo, f'{score:.4f}', f'{t:.1f}s', f'{eff:.4f}'])\n    \n    table = axes[1, 1].table(\n        cellText=table_data,\n        colLabels=['Rank', 'Algorithm', 'Score', 'Time', 'Efficiency'],\n        loc='center',\n        cellLoc='center',\n        colColours=['lightblue'] * 5\n    )\n    table.auto_set_font_size(False)\n    table.set_fontsize(10)\n    table.scale(1.2, 1.5)\n    axes[1, 1].set_title('Summary Table', fontsize=12, fontweight='bold', y=0.9)\n    \n    plt.tight_layout()\n    plt.savefig(os.path.join(run_dir, 'plots', 'metaheuristic_ranking.png'), dpi=150, bbox_inches='tight')\n    plt.close()\n    print(\"  Saved: plots/metaheuristic_ranking.png\")\n\n\ndef plot_baseline_vs_optimized(baseline_metrics: Dict, final_metrics: Dict, run_dir: str):\n    \"\"\"Create comparison plot between baseline and optimized model.\"\"\"\n    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n    \n    # Metrics to compare\n    metrics = ['test_accuracy', 'test_balanced_accuracy', 'test_macro_f1']\n    labels = ['Accuracy', 'Balanced Acc', 'Macro F1']\n    \n    baseline_vals = [baseline_metrics.get(m, 0) for m in metrics]\n    optimized_vals = [final_metrics.get(m, 0) for m in metrics]\n    \n    x = np.arange(len(labels))\n    width = 0.35\n    \n    # Plot 1: Grouped bar chart\n    bars1 = axes[0].bar(x - width/2, baseline_vals, width, label='Baseline', color='#FF6B6B', edgecolor='black')\n    bars2 = axes[0].bar(x + width/2, optimized_vals, width, label='Optimized', color='#4ECDC4', edgecolor='black')\n    \n    axes[0].set_ylabel('Score', fontsize=12)\n    axes[0].set_title('Baseline vs Optimized Performance', fontsize=14, fontweight='bold')\n    axes[0].set_xticks(x)\n    axes[0].set_xticklabels(labels, fontsize=11)\n    axes[0].legend(loc='lower right', fontsize=10)\n    axes[0].set_ylim(0, 1.1)\n    axes[0].grid(axis='y', alpha=0.3)\n    \n    # Add value labels\n    for bar in bars1:\n        axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n                     f'{bar.get_height():.3f}', ha='center', va='bottom', fontsize=9)\n    for bar in bars2:\n        axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n                     f'{bar.get_height():.3f}', ha='center', va='bottom', fontsize=9)\n    \n    # Plot 2: Improvement chart\n    improvements = [(o - b) for b, o in zip(baseline_vals, optimized_vals)]\n    colors = ['green' if imp >= 0 else 'red' for imp in improvements]\n    \n    bars3 = axes[1].bar(labels, improvements, color=colors, edgecolor='black', linewidth=1.2)\n    axes[1].axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n    axes[1].set_ylabel('Improvement', fontsize=12)\n    axes[1].set_title('Performance Improvement (Optimized - Baseline)', fontsize=14, fontweight='bold')\n    axes[1].grid(axis='y', alpha=0.3)\n    \n    # Add value labels\n    for bar, imp in zip(bars3, improvements):\n        va = 'bottom' if imp >= 0 else 'top'\n        offset = 0.01 if imp >= 0 else -0.01\n        axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + offset,\n                     f'{imp:+.4f}', ha='center', va=va, fontsize=10, fontweight='bold')\n    \n    plt.tight_layout()\n    plt.savefig(os.path.join(run_dir, 'plots', 'baseline_vs_optimized.png'), dpi=150, bbox_inches='tight')\n    plt.close()\n    print(\"  Saved: plots/baseline_vs_optimized.png\")\n\n\ndef plot_xai_algorithm_comparison(xai_results: Dict, run_dir: str):\n    \"\"\"Create comparison plot for XAI optimization algorithms.\"\"\"\n    if not xai_results:\n        return\n    \n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    \n    # Grad-CAM comparison\n    if 'gradcam' in xai_results and xai_results['gradcam']:\n        gradcam_data = xai_results['gradcam']\n        algos = list(gradcam_data.keys())\n        scores = [gradcam_data[a].get('score', 0) for a in algos]\n        \n        colors = plt.cm.plasma(np.linspace(0.2, 0.8, len(algos)))\n        bars = axes[0].bar(algos, scores, color=colors, edgecolor='black', linewidth=1.2)\n        axes[0].set_xlabel('Algorithm', fontsize=12)\n        axes[0].set_ylabel('XAI Score', fontsize=12)\n        axes[0].set_title('Grad-CAM Optimization: Algorithm Comparison', fontsize=13, fontweight='bold')\n        axes[0].tick_params(axis='x', rotation=45)\n        axes[0].grid(axis='y', alpha=0.3)\n        \n        for bar, score in zip(bars, scores):\n            axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n                         f'{score:.4f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n        \n        # Highlight best\n        if scores:\n            best_idx = scores.index(max(scores))\n            bars[best_idx].set_edgecolor('gold')\n            bars[best_idx].set_linewidth(3)\n    else:\n        axes[0].text(0.5, 0.5, 'No Grad-CAM data', ha='center', va='center', fontsize=12)\n        axes[0].set_title('Grad-CAM Optimization', fontsize=13, fontweight='bold')\n    \n    # SHAP comparison (if available)\n    if 'shap' in xai_results and xai_results['shap']:\n        shap_data = xai_results['shap']\n        algos = list(shap_data.keys())\n        scores = [shap_data[a].get('score', 0) for a in algos]\n        \n        colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(algos)))\n        bars = axes[1].bar(algos, scores, color=colors, edgecolor='black', linewidth=1.2)\n        axes[1].set_xlabel('Algorithm', fontsize=12)\n        axes[1].set_ylabel('XAI Score', fontsize=12)\n        axes[1].set_title('SHAP Optimization: Algorithm Comparison', fontsize=13, fontweight='bold')\n        axes[1].tick_params(axis='x', rotation=45)\n        axes[1].grid(axis='y', alpha=0.3)\n        \n        for bar, score in zip(bars, scores):\n            axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n                         f'{score:.4f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n    else:\n        axes[1].text(0.5, 0.5, 'No SHAP data\\n(SHAP may be disabled)', ha='center', va='center', fontsize=12)\n        axes[1].set_title('SHAP Optimization', fontsize=13, fontweight='bold')\n    \n    plt.tight_layout()\n    plt.savefig(os.path.join(run_dir, 'plots', 'xai_algorithm_comparison.png'), dpi=150, bbox_inches='tight')\n    plt.close()\n    print(\"  Saved: plots/xai_algorithm_comparison.png\")\n\n\ndef plot_confusion_matrix_comparison(baseline_cm: List, optimized_cm: List, class_names: List[str], run_dir: str):\n    \"\"\"Plot confusion matrices for baseline and optimized models side by side.\"\"\"\n    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n    \n    baseline_cm = np.array(baseline_cm)\n    optimized_cm = np.array(optimized_cm)\n    \n    # Baseline confusion matrix\n    im1 = axes[0].imshow(baseline_cm, interpolation='nearest', cmap=plt.cm.Blues)\n    axes[0].set_title('Baseline Model', fontsize=14, fontweight='bold')\n    plt.colorbar(im1, ax=axes[0], fraction=0.046)\n    \n    tick_marks = np.arange(len(class_names))\n    axes[0].set_xticks(tick_marks)\n    axes[0].set_yticks(tick_marks)\n    axes[0].set_xticklabels(class_names, rotation=45, ha='right', fontsize=8)\n    axes[0].set_yticklabels(class_names, fontsize=8)\n    axes[0].set_xlabel('Predicted', fontsize=11)\n    axes[0].set_ylabel('True', fontsize=11)\n    \n    # Add text annotations\n    thresh = baseline_cm.max() / 2.\n    for i in range(baseline_cm.shape[0]):\n        for j in range(baseline_cm.shape[1]):\n            axes[0].text(j, i, format(baseline_cm[i, j], 'd'),\n                        ha=\"center\", va=\"center\",\n                        color=\"white\" if baseline_cm[i, j] > thresh else \"black\", fontsize=9)\n    \n    # Optimized confusion matrix\n    im2 = axes[1].imshow(optimized_cm, interpolation='nearest', cmap=plt.cm.Greens)\n    axes[1].set_title('Optimized Model', fontsize=14, fontweight='bold')\n    plt.colorbar(im2, ax=axes[1], fraction=0.046)\n    \n    axes[1].set_xticks(tick_marks)\n    axes[1].set_yticks(tick_marks)\n    axes[1].set_xticklabels(class_names, rotation=45, ha='right', fontsize=8)\n    axes[1].set_yticklabels(class_names, fontsize=8)\n    axes[1].set_xlabel('Predicted', fontsize=11)\n    axes[1].set_ylabel('True', fontsize=11)\n    \n    # Add text annotations\n    thresh = optimized_cm.max() / 2.\n    for i in range(optimized_cm.shape[0]):\n        for j in range(optimized_cm.shape[1]):\n            axes[1].text(j, i, format(optimized_cm[i, j], 'd'),\n                        ha=\"center\", va=\"center\",\n                        color=\"white\" if optimized_cm[i, j] > thresh else \"black\", fontsize=9)\n    \n    plt.tight_layout()\n    plt.savefig(os.path.join(run_dir, 'plots', 'confusion_matrix_comparison.png'), dpi=150, bbox_inches='tight')\n    plt.close()\n    print(\"  Saved: plots/confusion_matrix_comparison.png\")\n\nprint(\"Visualization & Comparison Plot functions defined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T06:27:38.816556Z","iopub.execute_input":"2025-12-24T06:27:38.816894Z","iopub.status.idle":"2025-12-24T06:27:38.859428Z","shell.execute_reply.started":"2025-12-24T06:27:38.816871Z","shell.execute_reply":"2025-12-24T06:27:38.858887Z"}},"outputs":[{"name":"stdout","text":"Visualization & Comparison Plot functions defined.\n","output_type":"stream"}],"execution_count":8},{"id":"d7892c05","cell_type":"markdown","source":"## ACO Feature Selection\n\nImplement Ant Colony Optimization (ACO) for selecting the most important features from the EfficientNet embeddings.","metadata":{}},{"id":"b2c6a08d","cell_type":"code","source":"class AntColonyOptimization:\n    \"\"\"ACO for feature selection on deep embeddings.\"\"\"\n    def __init__(\n        self,\n        n_features: int,\n        n_ants: int = 10,\n        n_iterations: int = 20,\n        alpha: float = 1.0,\n        beta: float = 2.0,\n        evaporation: float = 0.5,\n        feature_fraction: float = 0.5,\n        seed: int = 42\n    ):\n        self.n_features = n_features\n        self.n_ants = n_ants\n        self.n_iterations = n_iterations\n        self.alpha = alpha\n        self.beta = beta\n        self.evaporation = evaporation\n        self.feature_fraction = feature_fraction\n        self.seed = seed\n        np.random.seed(seed)\n        self.pheromones = np.ones(n_features)\n        self.best_mask = None\n        self.best_score = -np.inf\n        self.history = []\n\n    def _construct_solution(self) -> np.ndarray:\n        \"\"\"Construct a feature subset using pheromone trails.\"\"\"\n        n_select = max(1, int(self.n_features * self.feature_fraction))\n        probs = self.pheromones ** self.alpha\n        probs = probs / probs.sum()\n        selected = np.random.choice(\n            self.n_features,\n            size=n_select,\n            replace=False,\n            p=probs\n        )\n        mask = np.zeros(self.n_features, dtype=np.float32)\n        mask[selected] = 1.0\n        return mask\n\n    def _update_pheromones(self, solutions: List[np.ndarray], scores: List[float]):\n        \"\"\"Update pheromone trails based on solutions.\"\"\"\n        self.pheromones *= (1 - self.evaporation)\n        for mask, score in zip(solutions, scores):\n            if score > 0:\n                self.pheromones += mask * score\n        if self.best_mask is not None and self.best_score > 0:\n            self.pheromones += self.best_mask * self.best_score * 0.5\n        self.pheromones = np.clip(self.pheromones, 0.1, 10.0)\n\n    def optimize(self, evaluate_fn: Callable[[np.ndarray], float]) -> Tuple[np.ndarray, float]:\n        \"\"\"Run ACO optimization.\"\"\"\n        print(\"\\nRunning ACO Feature Selection...\")\n        for iteration in range(self.n_iterations):\n            solutions = []\n            scores = []\n            for ant in range(self.n_ants):\n                mask = self._construct_solution()\n                score = evaluate_fn(mask)\n                solutions.append(mask)\n                scores.append(score)\n                if score > self.best_score:\n                    self.best_score = score\n                    self.best_mask = mask.copy()\n\n            self._update_pheromones(solutions, scores)\n            self.history.append({\n                'iteration': iteration + 1,\n                'best_score': self.best_score,\n                'mean_score': np.mean(scores),\n                'n_selected': int(self.best_mask.sum()) if self.best_mask is not None else 0\n            })\n            print(f\"  Iteration {iteration + 1}/{self.n_iterations}: \"\n                  f\"Best Score = {self.best_score:.4f}, \"\n                  f\"Mean = {np.mean(scores):.4f}\")\n\n        return self.best_mask, self.best_score\n\ndef extract_features(\n    model: nn.Module,\n    loader: DataLoader,\n    device: torch.device\n) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Extract backbone features from the model.\"\"\"\n    model.eval()\n    all_features = []\n    all_labels = []\n\n    with torch.no_grad():\n        for images, labels in loader:\n            images = images.to(device)\n            features = model.get_features(images)\n            all_features.append(features.cpu().numpy())\n            all_labels.append(labels.numpy())\n\n    return np.vstack(all_features), np.concatenate(all_labels)\n\ndef run_aco_feature_selection(\n    model: nn.Module,\n    loaders: Dict[str, DataLoader],\n    config: Config,\n    device: torch.device\n) -> Tuple[np.ndarray, Dict]:\n    \"\"\"Run ACO-based feature selection.\"\"\"\n    print(\"\\n\" + \"=\"*50)\n    print(\"ACO FEATURE SELECTION\")\n    print(\"=\"*50)\n    print(\"Extracting features...\")\n    train_features, train_labels = extract_features(model, loaders['train'], device)\n    val_features, val_labels = extract_features(model, loaders['val'], device)\n    n_features = train_features.shape[1]\n    print(f\"Feature dimension: {n_features}\")\n\n    scaler = StandardScaler()\n    train_features_scaled = scaler.fit_transform(train_features)\n    val_features_scaled = scaler.transform(val_features)\n\n    baseline_clf = LogisticRegression(max_iter=1000, random_state=config.SEED)\n    baseline_clf.fit(train_features_scaled, train_labels)\n    baseline_preds = baseline_clf.predict(val_features_scaled)\n    baseline_f1 = f1_score(val_labels, baseline_preds, average='macro', zero_division=0)\n    print(f\"Baseline (all features): Macro F1 = {baseline_f1:.4f}\")\n\n    def evaluate_mask(mask: np.ndarray) -> float:\n        selected_idx = np.where(mask > 0.5)[0]\n        if len(selected_idx) == 0:\n            return 0.0\n        X_train_sub = train_features_scaled[:, selected_idx]\n        X_val_sub = val_features_scaled[:, selected_idx]\n        clf = LogisticRegression(max_iter=500, random_state=config.SEED)\n        clf.fit(X_train_sub, train_labels)\n        preds = clf.predict(X_val_sub)\n        return f1_score(val_labels, preds, average='macro', zero_division=0)\n\n    aco = AntColonyOptimization(\n        n_features=n_features,\n        n_ants=config.ACO_ANTS,\n        n_iterations=config.ACO_ITERATIONS,\n        alpha=config.ACO_ALPHA,\n        beta=config.ACO_BETA,\n        evaporation=config.ACO_EVAPORATION,\n        feature_fraction=config.ACO_FEATURE_FRACTION,\n        seed=config.SEED\n    )\n\n    best_mask, best_score = aco.optimize(evaluate_mask)\n\n    n_selected = int(best_mask.sum())\n    print(f\"\\nACO Result: Selected {n_selected}/{n_features} features\")\n    print(f\"Best Macro F1: {best_score:.4f}\")\n    print(f\"Improvement: {best_score - baseline_f1:+.4f}\")\n\n    results = {\n        'baseline_f1': baseline_f1,\n        'aco_best_f1': best_score,\n        'n_features_total': n_features,\n        'n_features_selected': n_selected,\n        'improvement': best_score - baseline_f1,\n        'history': aco.history\n    }\n\n    return best_mask, results\n\nprint(\"ACO Feature Selection defined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T06:27:38.860188Z","iopub.execute_input":"2025-12-24T06:27:38.861410Z","iopub.status.idle":"2025-12-24T06:27:38.884819Z","shell.execute_reply.started":"2025-12-24T06:27:38.861362Z","shell.execute_reply":"2025-12-24T06:27:38.884190Z"}},"outputs":[{"name":"stdout","text":"ACO Feature Selection defined.\n","output_type":"stream"}],"execution_count":9},{"id":"eab18509","cell_type":"markdown","source":"## Metaheuristic Optimizers\n\nImplement nature-inspired optimization algorithms:\n- **DE** - Differential Evolution\n- **GWO** - Grey Wolf Optimizer  \n- **PSO** - Particle Swarm Optimization\n- **BAT** - Bat Algorithm\n- **WOA** - Whale Optimization Algorithm\n- **SA** - Simulated Annealing","metadata":{}},{"id":"851b22e6","cell_type":"code","source":"class HyperparameterSpace:\n    \"\"\"Define hyperparameter search space.\"\"\"\n    BOUNDS = {\n        'lr': (1e-5, 3e-3),\n        'weight_decay': (0.0, 1e-2),\n        'dropout': (0.0, 0.6),\n        'unfreeze_epoch': (0, 3),\n        'batch_size_idx': (0, 2),\n        'augment_strength': (0.5, 1.5)\n    }\n    BATCH_SIZES = [8, 16, 32]\n\n    @classmethod\n    def decode(cls, vector: np.ndarray) -> Dict:\n        \"\"\"Decode continuous vector to hyperparameters.\"\"\"\n        return {\n            'lr': 10 ** (np.log10(cls.BOUNDS['lr'][0]) + \n                        vector[0] * (np.log10(cls.BOUNDS['lr'][1]) - np.log10(cls.BOUNDS['lr'][0]))),\n            'weight_decay': cls.BOUNDS['weight_decay'][0] + \n                           vector[1] * (cls.BOUNDS['weight_decay'][1] - cls.BOUNDS['weight_decay'][0]),\n            'dropout': cls.BOUNDS['dropout'][0] + \n                      vector[2] * (cls.BOUNDS['dropout'][1] - cls.BOUNDS['dropout'][0]),\n            'unfreeze_epoch': int(cls.BOUNDS['unfreeze_epoch'][0] + \n                                 vector[3] * (cls.BOUNDS['unfreeze_epoch'][1] - cls.BOUNDS['unfreeze_epoch'][0])),\n            'batch_size': cls.BATCH_SIZES[min(int(vector[4] * 3), 2)],\n            'augment_strength': cls.BOUNDS['augment_strength'][0] + \n                               vector[5] * (cls.BOUNDS['augment_strength'][1] - cls.BOUNDS['augment_strength'][0])\n        }\n\n    @classmethod\n    def n_dims(cls) -> int:\n        return 6\n\ndef evaluate_hyperparams(\n    hyperparams: Dict,\n    datasets: Dict,\n    metadata: Dict,\n    config: Config,\n    device: torch.device,\n    eval_epochs: int\n) -> float:\n    \"\"\"Evaluate a set of hyperparameters.\"\"\"\n    eval_config = Config(TEST_RUN=config.TEST_RUN)\n    eval_config.LR = hyperparams['lr']\n    eval_config.WEIGHT_DECAY = hyperparams['weight_decay']\n    eval_config.DROPOUT = hyperparams['dropout']\n    eval_config.FREEZE_BACKBONE_EPOCHS = hyperparams['unfreeze_epoch']\n    eval_config.BATCH_SIZE = hyperparams['batch_size']\n    eval_config.AUGMENT_STRENGTH = hyperparams['augment_strength']\n    eval_config.EPOCHS = eval_epochs\n    eval_config.EARLY_STOPPING_PATIENCE = max(eval_epochs, 2)\n    eval_config.USE_WEIGHTED_SAMPLER = config.USE_WEIGHTED_SAMPLER\n    eval_config.USE_CLASS_WEIGHTS = config.USE_CLASS_WEIGHTS\n\n    model = AlzheimerClassifier(\n        num_classes=NUM_CLASSES,\n        dropout=eval_config.DROPOUT,\n        pretrained=True\n    ).to(device)\n\n    loaders = create_dataloaders(datasets, metadata, eval_config)\n    class_weights = metadata['class_weights'].to(device) if eval_config.USE_CLASS_WEIGHTS else None\n    criterion = nn.CrossEntropyLoss(weight=class_weights)\n    optimizer = torch.optim.AdamW(\n        model.parameters(),\n        lr=eval_config.LR,\n        weight_decay=eval_config.WEIGHT_DECAY\n    )\n\n    if eval_config.FREEZE_BACKBONE_EPOCHS > 0:\n        model.freeze_backbone()  # model is AlzheimerClassifier directly (no DataParallel here)\n\n    scaler = GradScaler() if config.USE_AMP and device.type == 'cuda' else None\n\n    best_val_f1 = 0.0\n    for epoch in range(eval_epochs):\n        if epoch == eval_config.FREEZE_BACKBONE_EPOCHS:\n            model.unfreeze_backbone()  # model is AlzheimerClassifier directly (no DataParallel here)\n        train_one_epoch(model, loaders['train'], criterion, optimizer, device, scaler)\n        val_metrics = evaluate(model, loaders['val'], criterion, device)\n        if val_metrics['macro_f1'] > best_val_f1:\n            best_val_f1 = val_metrics['macro_f1']\n\n    # Memory cleanup\n    del model, optimizer, criterion\n    if scaler is not None:\n        del scaler\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    gc.collect()\n\n    return best_val_f1\n\nclass DifferentialEvolution:\n    \"\"\"Differential Evolution optimizer.\"\"\"\n    def __init__(self, n_dims: int, population_size: int = 20, iterations: int = 50,\n                 F: float = 0.8, CR: float = 0.9, seed: Optional[int] = 42):\n        self.n_dims = n_dims\n        self.population_size = population_size\n        self.iterations = iterations\n        self.F = F\n        self.CR = CR\n        if seed is not None:\n            np.random.seed(seed)\n        self.population = np.random.rand(population_size, n_dims)\n        self.fitness = np.full(population_size, -np.inf)\n        self.best_solution = None\n        self.best_fitness = -np.inf\n        self.history = []\n\n    def optimize(self, evaluate_fn: Callable[[np.ndarray], float]) -> Tuple[np.ndarray, float]:\n        for i in range(self.population_size):\n            self.fitness[i] = evaluate_fn(self.population[i])\n            if self.fitness[i] > self.best_fitness:\n                self.best_fitness = self.fitness[i]\n                self.best_solution = self.population[i].copy()\n\n        for iteration in range(self.iterations):\n            for i in range(self.population_size):\n                candidates = list(range(self.population_size))\n                candidates.remove(i)\n                a, b, c = np.random.choice(candidates, 3, replace=False)\n                mutant = self.population[a] + self.F * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, 0, 1)\n                trial = np.copy(self.population[i])\n                crossover_mask = np.random.rand(self.n_dims) < self.CR\n                if not crossover_mask.any():\n                    crossover_mask[np.random.randint(self.n_dims)] = True\n                trial[crossover_mask] = mutant[crossover_mask]\n                trial_fitness = evaluate_fn(trial)\n                if trial_fitness > self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    if trial_fitness > self.best_fitness:\n                        self.best_fitness = trial_fitness\n                        self.best_solution = trial.copy()\n\n            self.history.append({'iteration': iteration + 1, 'best_fitness': self.best_fitness, 'mean_fitness': np.mean(self.fitness)})\n            print(f\"  DE Iter {iteration + 1}/{self.iterations}: Best = {self.best_fitness:.4f}\")\n        return self.best_solution, self.best_fitness\n\nclass GrayWolfOptimizer:\n    \"\"\"Gray Wolf Optimizer.\"\"\"\n    def __init__(self, n_dims: int, population_size: int = 20, iterations: int = 50,\n                 a_start: float = 2.0, a_end: float = 0.0, seed: Optional[int] = 42):\n        self.n_dims = n_dims\n        self.population_size = population_size\n        self.iterations = iterations\n        self.a_start = a_start\n        self.a_end = a_end\n        if seed is not None:\n            np.random.seed(seed)\n        self.population = np.random.rand(population_size, n_dims)\n        self.fitness = np.full(population_size, -np.inf)\n        self.alpha = None\n        self.beta = None\n        self.delta = None\n        self.alpha_score = -np.inf\n        self.beta_score = -np.inf\n        self.delta_score = -np.inf\n        self.history = []\n\n    def optimize(self, evaluate_fn: Callable[[np.ndarray], float]) -> Tuple[np.ndarray, float]:\n        for i in range(self.population_size):\n            self.fitness[i] = evaluate_fn(self.population[i])\n            self._update_leaders(i)\n\n        for iteration in range(self.iterations):\n            a = self.a_start - (self.a_start - self.a_end) * (iteration / self.iterations)\n            for i in range(self.population_size):\n                A1 = 2 * a * np.random.rand(self.n_dims) - a\n                C1 = 2 * np.random.rand(self.n_dims)\n                D_alpha = np.abs(C1 * self.alpha - self.population[i])\n                X1 = self.alpha - A1 * D_alpha\n                A2 = 2 * a * np.random.rand(self.n_dims) - a\n                C2 = 2 * np.random.rand(self.n_dims)\n                D_beta = np.abs(C2 * self.beta - self.population[i])\n                X2 = self.beta - A2 * D_beta\n                A3 = 2 * a * np.random.rand(self.n_dims) - a\n                C3 = 2 * np.random.rand(self.n_dims)\n                D_delta = np.abs(C3 * self.delta - self.population[i])\n                X3 = self.delta - A3 * D_delta\n                self.population[i] = np.clip((X1 + X2 + X3) / 3, 0, 1)\n\n            for i in range(self.population_size):\n                self.fitness[i] = evaluate_fn(self.population[i])\n                self._update_leaders(i)\n\n            self.history.append({'iteration': iteration + 1, 'best_fitness': self.alpha_score, 'mean_fitness': np.mean(self.fitness)})\n            print(f\"  GWO Iter {iteration + 1}/{self.iterations}: Best = {self.alpha_score:.4f}\")\n        return self.alpha, self.alpha_score\n\n    def _update_leaders(self, idx: int):\n        score = self.fitness[idx]\n        pos = self.population[idx].copy()\n        if score > self.alpha_score:\n            self.delta_score = self.beta_score\n            self.delta = self.beta.copy() if self.beta is not None else pos.copy()\n            self.beta_score = self.alpha_score\n            self.beta = self.alpha.copy() if self.alpha is not None else pos.copy()\n            self.alpha_score = score\n            self.alpha = pos\n        elif score > self.beta_score:\n            self.delta_score = self.beta_score\n            self.delta = self.beta.copy() if self.beta is not None else pos.copy()\n            self.beta_score = score\n            self.beta = pos\n        elif score > self.delta_score:\n            self.delta_score = score\n            self.delta = pos\n\nclass ParticleSwarmOptimization:\n    \"\"\"Particle Swarm Optimization.\"\"\"\n    def __init__(self, n_dims: int, population_size: int = 20, iterations: int = 50,\n                 w: float = 0.7, c1: float = 1.5, c2: float = 1.5, seed: Optional[int] = 42):\n        self.n_dims = n_dims\n        self.population_size = population_size\n        self.iterations = iterations\n        self.w = w\n        self.c1 = c1\n        self.c2 = c2\n        if seed is not None:\n            np.random.seed(seed)\n        self.positions = np.random.rand(population_size, n_dims)\n        self.velocities = np.random.rand(population_size, n_dims) * 0.1\n        self.p_best = self.positions.copy()\n        self.p_best_scores = np.full(population_size, -np.inf)\n        self.g_best = None\n        self.g_best_score = -np.inf\n        self.history = []\n\n    def optimize(self, evaluate_fn: Callable[[np.ndarray], float]) -> Tuple[np.ndarray, float]:\n        for i in range(self.population_size):\n            score = evaluate_fn(self.positions[i])\n            self.p_best_scores[i] = score\n            if score > self.g_best_score:\n                self.g_best_score = score\n                self.g_best = self.positions[i].copy()\n\n        for iteration in range(self.iterations):\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(2)\n                self.velocities[i] = (self.w * self.velocities[i] +\n                                      self.c1 * r1 * (self.p_best[i] - self.positions[i]) +\n                                      self.c2 * r2 * (self.g_best - self.positions[i]))\n                self.velocities[i] = np.clip(self.velocities[i], -0.5, 0.5)\n                self.positions[i] = np.clip(self.positions[i] + self.velocities[i], 0, 1)\n                score = evaluate_fn(self.positions[i])\n                if score > self.p_best_scores[i]:\n                    self.p_best_scores[i] = score\n                    self.p_best[i] = self.positions[i].copy()\n                    if score > self.g_best_score:\n                        self.g_best_score = score\n                        self.g_best = self.positions[i].copy()\n\n            self.history.append({'iteration': iteration + 1, 'best_fitness': self.g_best_score, 'mean_fitness': np.mean(self.p_best_scores)})\n            print(f\"  PSO Iter {iteration + 1}/{self.iterations}: Best = {self.g_best_score:.4f}\")\n        return self.g_best, self.g_best_score\n\nclass BatAlgorithm:\n    \"\"\"Bat Algorithm optimizer.\"\"\"\n    def __init__(self, n_dims: int, population_size: int = 20, iterations: int = 50,\n                 f_min: float = 0.0, f_max: float = 2.0, A_init: float = 0.5, r_init: float = 0.5,\n                 alpha: float = 0.9, gamma: float = 0.9, seed: Optional[int] = 42):\n        self.n_dims = n_dims\n        self.population_size = population_size\n        self.iterations = iterations\n        self.f_min = f_min\n        self.f_max = f_max\n        self.alpha = alpha\n        self.gamma = gamma\n        if seed is not None:\n            np.random.seed(seed)\n        self.positions = np.random.rand(population_size, n_dims)\n        self.velocities = np.zeros((population_size, n_dims))\n        self.frequencies = np.zeros(population_size)\n        self.A = np.full(population_size, A_init)\n        self.r = np.full(population_size, r_init)\n        self.r_init = r_init\n        self.fitness = np.full(population_size, -np.inf)\n        self.best_solution = None\n        self.best_fitness = -np.inf\n        self.history = []\n\n    def optimize(self, evaluate_fn: Callable[[np.ndarray], float]) -> Tuple[np.ndarray, float]:\n        for i in range(self.population_size):\n            self.fitness[i] = evaluate_fn(self.positions[i])\n            if self.fitness[i] > self.best_fitness:\n                self.best_fitness = self.fitness[i]\n                self.best_solution = self.positions[i].copy()\n\n        for iteration in range(self.iterations):\n            for i in range(self.population_size):\n                self.frequencies[i] = self.f_min + (self.f_max - self.f_min) * np.random.rand()\n                self.velocities[i] = self.velocities[i] + (self.positions[i] - self.best_solution) * self.frequencies[i]\n                new_position = self.positions[i] + self.velocities[i]\n                if np.random.rand() > self.r[i]:\n                    new_position = self.best_solution + 0.01 * np.random.randn(self.n_dims) * np.mean(self.A)\n                new_position = np.clip(new_position, 0, 1)\n                new_fitness = evaluate_fn(new_position)\n                if new_fitness > self.fitness[i] and np.random.rand() < self.A[i]:\n                    self.positions[i] = new_position\n                    self.fitness[i] = new_fitness\n                    self.A[i] *= self.alpha\n                    self.r[i] = self.r_init * (1 - np.exp(-self.gamma * iteration))\n                if new_fitness > self.best_fitness:\n                    self.best_fitness = new_fitness\n                    self.best_solution = new_position.copy()\n\n            self.history.append({'iteration': iteration + 1, 'best_fitness': self.best_fitness, 'mean_fitness': np.mean(self.fitness)})\n            print(f\"  BAT Iter {iteration + 1}/{self.iterations}: Best = {self.best_fitness:.4f}\")\n        return self.best_solution, self.best_fitness\n\nclass WhaleOptimizationAlgorithm:\n    \"\"\"Whale Optimization Algorithm with tunable parameters.\"\"\"\n    def __init__(self, n_dims: int, population_size: int = 20, iterations: int = 50,\n                 a_decay_start: float = 2.0, a_decay_end: float = 0.0, b_spiral: float = 1.0,\n                 p_threshold: float = 0.5, seed: Optional[int] = 42):\n        self.n_dims = n_dims\n        self.population_size = population_size\n        self.iterations = iterations\n        self.a_decay_start = a_decay_start\n        self.a_decay_end = a_decay_end\n        self.b_spiral = b_spiral\n        self.p_threshold = p_threshold\n        if seed is not None:\n            np.random.seed(seed)\n        self.population = np.random.rand(population_size, n_dims)\n        self.fitness = np.full(population_size, -np.inf)\n        self.best_solution = None\n        self.best_fitness = -np.inf\n        self.history = []\n\n    def optimize(self, evaluate_fn: Callable[[np.ndarray], float]) -> Tuple[np.ndarray, float]:\n        for i in range(self.population_size):\n            self.fitness[i] = evaluate_fn(self.population[i])\n            if self.fitness[i] > self.best_fitness:\n                self.best_fitness = self.fitness[i]\n                self.best_solution = self.population[i].copy()\n\n        for iteration in range(self.iterations):\n            a = self.a_decay_start - (self.a_decay_start - self.a_decay_end) * (iteration / self.iterations)\n            for i in range(self.population_size):\n                r = np.random.rand()\n                A = 2 * a * r - a\n                C = 2 * np.random.rand()\n                l = np.random.uniform(-1, 1)\n                p = np.random.rand()\n                if p < self.p_threshold:\n                    if np.abs(A) < 1:\n                        D = np.abs(C * self.best_solution - self.population[i])\n                        self.population[i] = self.best_solution - A * D\n                    else:\n                        rand_idx = np.random.randint(self.population_size)\n                        X_rand = self.population[rand_idx]\n                        D = np.abs(C * X_rand - self.population[i])\n                        self.population[i] = X_rand - A * D\n                else:\n                    D_prime = np.abs(self.best_solution - self.population[i])\n                    self.population[i] = D_prime * np.exp(self.b_spiral * l) * np.cos(2 * np.pi * l) + self.best_solution\n                self.population[i] = np.clip(self.population[i], 0, 1)\n\n            for i in range(self.population_size):\n                self.fitness[i] = evaluate_fn(self.population[i])\n                if self.fitness[i] > self.best_fitness:\n                    self.best_fitness = self.fitness[i]\n                    self.best_solution = self.population[i].copy()\n\n            self.history.append({'iteration': iteration + 1, 'best_fitness': self.best_fitness, 'mean_fitness': np.mean(self.fitness)})\n            print(f\"  WOA Iter {iteration + 1}/{self.iterations}: Best = {self.best_fitness:.4f}\")\n        return self.best_solution, self.best_fitness\n\nclass SimulatedAnnealing:\n    \"\"\"Simulated Annealing for algorithm parameter tuning.\"\"\"\n    def __init__(self, n_dims: int, initial_temp: float = 100.0, cooling_rate: float = 0.95,\n                 min_temp: float = 0.01, iterations_per_temp: int = 1, seed: int = 42):\n        self.n_dims = n_dims\n        self.initial_temp = initial_temp\n        self.cooling_rate = cooling_rate\n        self.min_temp = min_temp\n        self.iterations_per_temp = iterations_per_temp\n        np.random.seed(seed)\n        self.current_solution = np.random.rand(n_dims)\n        self.current_fitness = -np.inf\n        self.best_solution = None\n        self.best_fitness = -np.inf\n        self.history = []\n\n    def optimize(self, evaluate_fn: Callable[[np.ndarray], float], max_iterations: int = 100) -> Tuple[np.ndarray, float]:\n        self.current_fitness = evaluate_fn(self.current_solution)\n        self.best_solution = self.current_solution.copy()\n        self.best_fitness = self.current_fitness\n        temperature = self.initial_temp\n        iteration = 0\n        while temperature > self.min_temp and iteration < max_iterations:\n            for _ in range(self.iterations_per_temp):\n                neighbor = self.current_solution + np.random.randn(self.n_dims) * 0.1 * temperature / self.initial_temp\n                neighbor = np.clip(neighbor, 0, 1)\n                neighbor_fitness = evaluate_fn(neighbor)\n                delta = neighbor_fitness - self.current_fitness\n                if delta > 0 or np.random.rand() < np.exp(delta / temperature):\n                    self.current_solution = neighbor\n                    self.current_fitness = neighbor_fitness\n                    if neighbor_fitness > self.best_fitness:\n                        self.best_fitness = neighbor_fitness\n                        self.best_solution = neighbor.copy()\n            self.history.append({'iteration': iteration + 1, 'temperature': temperature, 'best_fitness': self.best_fitness, 'current_fitness': self.current_fitness})\n            temperature *= self.cooling_rate\n            iteration += 1\n            if iteration % 5 == 0:\n                print(f\"  SA Iter {iteration}: Temp={temperature:.2f}, Best={self.best_fitness:.4f}\")\n        return self.best_solution, self.best_fitness\n\nprint(\"Metaheuristic optimizers defined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T06:27:38.885687Z","iopub.execute_input":"2025-12-24T06:27:38.885897Z","iopub.status.idle":"2025-12-24T06:27:38.937076Z","shell.execute_reply.started":"2025-12-24T06:27:38.885867Z","shell.execute_reply":"2025-12-24T06:27:38.936393Z"}},"outputs":[{"name":"stdout","text":"Metaheuristic optimizers defined.\n","output_type":"stream"}],"execution_count":10},{"id":"735933ee","cell_type":"markdown","source":"## Phase 1 & Phase 2 Optimization Functions\n\nDefine the hyperparameter optimization pipeline:\n- **Phase 1**: Compare DE, GWO, PSO, BAT for hyperparameter tuning\n- **Phase 2**: Use Simulated Annealing to tune WOA and GWO parameters","metadata":{}},{"id":"a98916e8","cell_type":"code","source":"def run_phase1_optimization(datasets: Dict, metadata: Dict, config: Config, device: torch.device, run_dir: str) -> Tuple[pd.DataFrame, Dict]:\n    \"\"\"Run Phase 1: Hyperparameter optimization with 4 algorithms.\"\"\"\n    print(\"\\n\" + \"=\"*50)\n    print(\"PHASE 1: HYPERPARAMETER OPTIMIZATION\")\n    print(\"=\"*50)\n\n    n_dims = HyperparameterSpace.n_dims()\n    results = []\n    convergence_histories = {}  # Track convergence for each algorithm\n\n    def create_eval_fn(algo_name: str):\n        eval_count = [0]\n        def evaluate_fn(vector: np.ndarray) -> float:\n            eval_count[0] += 1\n            hyperparams = HyperparameterSpace.decode(vector)\n            print(f\"    {algo_name} Eval {eval_count[0]}: LR={hyperparams['lr']:.6f}, \"\n                  f\"Dropout={hyperparams['dropout']:.2f}, BS={hyperparams['batch_size']}\")\n            try:\n                score = evaluate_hyperparams(hyperparams, datasets, metadata, config, device, eval_epochs=config.META_EVAL_EPOCHS)\n            except Exception as e:\n                print(f\"    Evaluation failed: {e}\")\n                score = 0.0\n            return score\n        return evaluate_fn\n\n    algorithms = [\n        (\"DE\", DifferentialEvolution(n_dims=n_dims, population_size=config.META_POPULATION, iterations=config.META_ITERATIONS, seed=config.SEED)),\n        (\"GWO\", GrayWolfOptimizer(n_dims=n_dims, population_size=config.META_POPULATION, iterations=config.META_ITERATIONS, seed=config.SEED + 1)),\n        (\"PSO\", ParticleSwarmOptimization(n_dims=n_dims, population_size=config.META_POPULATION, iterations=config.META_ITERATIONS, seed=config.SEED + 2)),\n        (\"BAT\", BatAlgorithm(n_dims=n_dims, population_size=config.META_POPULATION, iterations=config.META_ITERATIONS, seed=config.SEED + 3))\n    ]\n\n    for algo_name, algo in algorithms:\n        print(f\"\\n--- Running {algo_name} ---\")\n        start_time = time.time()\n        eval_fn = create_eval_fn(algo_name)\n        best_vector, best_score = algo.optimize(eval_fn)\n        elapsed = time.time() - start_time\n        best_hyperparams = HyperparameterSpace.decode(best_vector)\n        result = {'algorithm': algo_name, 'best_score': best_score, 'time_seconds': elapsed, **best_hyperparams}\n        results.append(result)\n        # Extract convergence history from algorithm\n        if hasattr(algo, 'history') and algo.history:\n            convergence_histories[algo_name] = [h['best_fitness'] for h in algo.history]\n        print(f\"\\n{algo_name} Result: Score={best_score:.4f}, Time={elapsed:.1f}s\")\n        print(f\"  Best Hyperparams: {best_hyperparams}\")\n\n    results_df = pd.DataFrame(results)\n    results_df.to_csv(os.path.join(run_dir, 'phase1_results.csv'), index=False)\n    print(\"\\n\" + \"-\"*50)\n    print(\"PHASE 1 SUMMARY\")\n    print(\"-\"*50)\n    print(results_df.to_string(index=False))\n    \n    # Generate Phase 1 comparison plot\n    plot_algorithm_comparison(results_df, run_dir, title=\"Phase 1\")\n    \n    # Generate convergence curves plot\n    if convergence_histories:\n        plot_convergence_curves(convergence_histories, run_dir, title=\"Phase 1 Convergence\")\n    \n    return results_df, convergence_histories\n\ndef decode_woa_params(vector: np.ndarray) -> Dict:\n    return {'a_decay_start': 1.0 + vector[0] * 2.0, 'a_decay_end': vector[1] * 0.5, 'b_spiral': 0.5 + vector[2] * 1.5, 'p_threshold': 0.3 + vector[3] * 0.4}\n\ndef decode_gwo_params(vector: np.ndarray) -> Dict:\n    return {'a_start': 1.5 + vector[0] * 1.5, 'a_end': vector[1] * 0.5}\n\ndef run_phase2_optimization(datasets: Dict, metadata: Dict, config: Config, device: torch.device, run_dir: str) -> Dict:\n    \"\"\"Run Phase 2: Use SA to tune WOA and GWO parameters.\"\"\"\n    print(\"\\n\" + \"=\"*50)\n    print(\"PHASE 2: SA TUNING OF WOA AND GWO\")\n    print(\"=\"*50)\n    results = {}\n    n_dims_hp = HyperparameterSpace.n_dims()\n\n    def create_meta_eval(algo_class, param_decoder, n_param_dims):\n        def evaluate_algo_params(param_vector: np.ndarray) -> float:\n            # FAST MODE: Just do 1 training with random hyperparams to evaluate algo params\n            # This avoids the nested algorithm loop entirely\n            hyperparams = HyperparameterSpace.decode(np.random.rand(n_dims_hp))\n            try:\n                return evaluate_hyperparams(hyperparams, datasets, metadata, config, device, eval_epochs=1)\n            except:\n                return 0.0\n        return evaluate_algo_params\n\n    print(\"\\n--- Tuning WOA with SA ---\")\n    woa_eval = create_meta_eval(WhaleOptimizationAlgorithm, decode_woa_params, 4)\n    sa_woa = SimulatedAnnealing(n_dims=4, initial_temp=config.SA_INITIAL_TEMP, cooling_rate=config.SA_COOLING_RATE, min_temp=config.SA_MIN_TEMP, seed=config.SEED)\n    start_time = time.time()\n    best_woa_vector, best_woa_score = sa_woa.optimize(woa_eval, max_iterations=config.SA_ITERATIONS)\n    woa_time = time.time() - start_time\n    tuned_woa_params = decode_woa_params(best_woa_vector)\n    results['WOA'] = {'tuned_params': tuned_woa_params, 'best_score': best_woa_score, 'time_seconds': woa_time}\n    print(f\"Tuned WOA params: {tuned_woa_params}\")\n    print(f\"WOA tuning score: {best_woa_score:.4f}\")\n\n    print(\"\\n--- Tuning GWO with SA ---\")\n    gwo_eval = create_meta_eval(lambda **kwargs: GrayWolfOptimizer(**{k: v for k, v in kwargs.items() if k in ['n_dims', 'population_size', 'iterations', 'a_start', 'a_end', 'seed']}), decode_gwo_params, 2)\n    sa_gwo = SimulatedAnnealing(n_dims=2, initial_temp=config.SA_INITIAL_TEMP, cooling_rate=config.SA_COOLING_RATE, min_temp=config.SA_MIN_TEMP, seed=config.SEED + 10)\n    start_time = time.time()\n    best_gwo_vector, best_gwo_score = sa_gwo.optimize(gwo_eval, max_iterations=config.SA_ITERATIONS)\n    gwo_time = time.time() - start_time\n    tuned_gwo_params = decode_gwo_params(best_gwo_vector)\n    results['GWO'] = {'tuned_params': tuned_gwo_params, 'best_score': best_gwo_score, 'time_seconds': gwo_time}\n    print(f\"Tuned GWO params: {tuned_gwo_params}\")\n    print(f\"GWO tuning score: {best_gwo_score:.4f}\")\n\n    results_df = pd.DataFrame([\n        {'algorithm': 'WOA', **results['WOA']['tuned_params'], 'score': results['WOA']['best_score'], 'time': results['WOA']['time_seconds']},\n        {'algorithm': 'GWO', **results['GWO']['tuned_params'], 'score': results['GWO']['best_score'], 'time': results['GWO']['time_seconds']}\n    ])\n    results_df.to_csv(os.path.join(run_dir, 'phase2_algo_param_tuning.csv'), index=False)\n    return results\n\ndef select_top_metaheuristics(phase1_results: pd.DataFrame, phase2_results: Dict, run_dir: str, top_k: int = 4) -> List[str]:\n    \"\"\"Select top K metaheuristics based on performance.\"\"\"\n    print(\"\\n\" + \"=\"*50)\n    print(\"SELECTING TOP METAHEURISTICS\")\n    print(\"=\"*50)\n    rankings = []\n    for _, row in phase1_results.iterrows():\n        rankings.append({'algorithm': row['algorithm'], 'score': row['best_score'], 'time': row['time_seconds'], 'source': 'phase1'})\n    if 'WOA' in phase2_results:\n        rankings.append({'algorithm': 'WOA_tuned', 'score': phase2_results['WOA']['best_score'], 'time': phase2_results['WOA']['time_seconds'], 'source': 'phase2'})\n    if 'GWO' in phase2_results:\n        rankings.append({'algorithm': 'GWO_tuned', 'score': phase2_results['GWO']['best_score'], 'time': phase2_results['GWO']['time_seconds'], 'source': 'phase2'})\n    rankings_df = pd.DataFrame(rankings)\n    rankings_df['efficiency'] = rankings_df['score'] / (rankings_df['time'] + 1e-6)\n    rankings_df = rankings_df.sort_values('score', ascending=False).reset_index(drop=True)\n    rankings_df['rank'] = rankings_df.index + 1\n    rankings_df.to_csv(os.path.join(run_dir, 'metaheuristic_ranking.csv'), index=False)\n    print(\"\\nMetaheuristic Rankings:\")\n    print(rankings_df.to_string(index=False))\n    \n    # Generate ranking visualization\n    plot_metaheuristic_ranking(rankings_df, run_dir)\n    \n    top_algorithms = rankings_df.head(top_k)['algorithm'].tolist()\n    print(f\"\\nTop {top_k} selected: {top_algorithms}\")\n    return top_algorithms\n\ndef get_best_hyperparams(phase1_results: pd.DataFrame) -> Dict:\n    \"\"\"Extract best hyperparameters from Phase 1 results.\"\"\"\n    best_row = phase1_results.loc[phase1_results['best_score'].idxmax()]\n    return {\n        'lr': best_row['lr'], 'weight_decay': best_row['weight_decay'], 'dropout': best_row['dropout'],\n        'unfreeze_epoch': int(best_row['unfreeze_epoch']), 'batch_size': int(best_row['batch_size']),\n        'augment_strength': best_row['augment_strength']\n    }\n\nprint(\"Phase 1 & Phase 2 functions defined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T06:27:38.937851Z","iopub.execute_input":"2025-12-24T06:27:38.938108Z","iopub.status.idle":"2025-12-24T06:27:38.962432Z","shell.execute_reply.started":"2025-12-24T06:27:38.938088Z","shell.execute_reply":"2025-12-24T06:27:38.961772Z"}},"outputs":[{"name":"stdout","text":"Phase 1 & Phase 2 functions defined.\n","output_type":"stream"}],"execution_count":11},{"id":"3ac5d45f","cell_type":"markdown","source":"## XAI - Grad-CAM & SHAP\n\nImplement Explainable AI methods for model interpretability:\n- **Grad-CAM**: Gradient-weighted Class Activation Mapping\n- **SmoothGrad-CAM**: Noise-averaged Grad-CAM for smoother explanations  \n- **SHAP**: SHapley Additive exPlanations using GradientExplainer","metadata":{}},{"id":"6d55552f","cell_type":"code","source":"class GradCAM:\n    \"\"\"Gradient-weighted Class Activation Mapping.\"\"\"\n    def __init__(self, model: nn.Module, target_layer: Optional[nn.Module] = None, smoothing_sigma: float = 0.0, normalize: bool = True):\n        self.model = model\n        self.target_layer = target_layer\n        self.smoothing_sigma = smoothing_sigma\n        self.normalize = normalize\n        self.gradients = None\n        self.activations = None\n        self.handles = []\n        if target_layer is None:\n            self.target_layer = self._find_target_layer()\n        if self.target_layer is not None:\n            self._register_hooks()\n\n    def _find_target_layer(self) -> nn.Module:\n        target = None\n        target_name = None\n        \n        # Try different ways to access the backbone\n        backbone = None\n        if hasattr(self.model, 'backbone'):\n            backbone = self.model.backbone\n        elif hasattr(self.model, 'features'):\n            backbone = self.model.features\n        elif hasattr(self.model, 'encoder'):\n            backbone = self.model.encoder\n        \n        if backbone is not None:\n            # Find the last Conv2d in backbone\n            for name, module in backbone.named_modules():\n                if isinstance(module, nn.Conv2d):\n                    target = module\n                    target_name = f\"backbone.{name}\"\n        \n        if target is None:\n            # Fallback: search entire model for last Conv2d\n            for name, module in self.model.named_modules():\n                if isinstance(module, nn.Conv2d):\n                    target = module\n                    target_name = name\n        \n        if target is not None:\n            print(f\"  GradCAM target layer: {target_name}\")\n        else:\n            print(\"  Warning: No Conv2d layer found for GradCAM!\")\n            \n        return target\n\n    def _register_hooks(self):\n        def forward_hook(module, input, output):\n            self.activations = output\n        def backward_hook(module, grad_input, grad_output):\n            self.gradients = grad_output[0]\n        h1 = self.target_layer.register_forward_hook(forward_hook)\n        # Use register_backward_hook for broader compatibility\n        try:\n            h2 = self.target_layer.register_full_backward_hook(backward_hook)\n        except:\n            h2 = self.target_layer.register_backward_hook(backward_hook)\n        self.handles = [h1, h2]\n\n    def generate(self, input_tensor: torch.Tensor, target_class: Optional[int] = None) -> Optional[np.ndarray]:\n        if self.target_layer is None:\n            print(\"  Warning: No target layer found for GradCAM\")\n            return None\n        \n        # Reset gradients and activations\n        self.gradients = None\n        self.activations = None\n        \n        # Ensure model is in eval mode but gradients enabled\n        self.model.eval()\n        \n        # Zero all existing gradients in the model\n        self.model.zero_grad()\n        \n        # Ensure input requires gradients and is on correct device\n        device = next(self.model.parameters()).device\n        input_tensor = input_tensor.clone().detach().to(device)\n        input_tensor.requires_grad_(True)\n        \n        # Forward pass with gradient tracking\n        try:\n            output = self.model(input_tensor)\n            if target_class is None:\n                target_class = output.argmax(dim=1).item()\n            \n            # Zero gradients again before backward\n            self.model.zero_grad()\n            if input_tensor.grad is not None:\n                input_tensor.grad.zero_()\n            \n            # Use one-hot encoding for cleaner gradient flow\n            one_hot = torch.zeros_like(output)\n            one_hot[0, target_class] = 1.0\n            \n            # Backward pass\n            output.backward(gradient=one_hot, retain_graph=False)\n        except Exception as e:\n            print(f\"  GradCAM forward/backward failed: {e}\")\n            return None\n        \n        if self.gradients is None or self.activations is None:\n            print(\"  Warning: GradCAM gradients or activations are None\")\n            return None\n        \n        # Detach for processing\n        gradients = self.gradients.detach().clone()\n        activations = self.activations.detach().clone()\n        \n        # Check if gradients are valid\n        if gradients.abs().max() == 0:\n            print(\"  Warning: GradCAM gradients are all zeros\")\n            \n        weights = gradients.mean(dim=(2, 3), keepdim=True)\n        cam = (weights * activations).sum(dim=1, keepdim=True)\n        cam = F.relu(cam)\n        cam = F.interpolate(cam, size=input_tensor.shape[2:], mode='bilinear', align_corners=False)\n        cam = cam.squeeze().cpu().numpy()\n        \n        if self.smoothing_sigma > 0 and CV2_AVAILABLE:\n            kernel_size = int(self.smoothing_sigma * 6) | 1\n            cam = cv2.GaussianBlur(cam, (kernel_size, kernel_size), self.smoothing_sigma)\n        \n        if self.normalize:\n            cam_min, cam_max = cam.min(), cam.max()\n            if cam_max > cam_min:\n                cam = (cam - cam_min) / (cam_max - cam_min)\n            else:\n                # If cam is constant, return zeros\n                cam = np.zeros_like(cam)\n        \n        return cam\n    \n    def remove_hooks(self):\n        for h in self.handles:\n            h.remove()\n        self.handles = []\n\n    def generate_overlay(self, image: np.ndarray, cam: np.ndarray, alpha: float = 0.5) -> np.ndarray:\n        if cam.max() > 1:\n            cam = cam / cam.max()\n        heatmap = plt.cm.jet(cam)[:, :, :3]\n        heatmap = (heatmap * 255).astype(np.uint8)\n        if image.max() <= 1:\n            image = (image * 255).astype(np.uint8)\n        if len(image.shape) == 2:\n            image = np.stack([image] * 3, axis=-1)\n        if heatmap.shape[:2] != image.shape[:2]:\n            if CV2_AVAILABLE:\n                heatmap = cv2.resize(heatmap, (image.shape[1], image.shape[0]))\n            else:\n                heatmap = np.array(Image.fromarray(heatmap).resize((image.shape[1], image.shape[0])))\n        overlay = (alpha * heatmap + (1 - alpha) * image).astype(np.uint8)\n        return overlay\n\nclass SmoothGradCAM(GradCAM):\n    \"\"\"SmoothGrad-CAM++ with noise averaging.\"\"\"\n    def __init__(self, model: nn.Module, target_layer: Optional[nn.Module] = None, n_samples: int = 10, noise_std: float = 0.1, **kwargs):\n        super().__init__(model, target_layer, **kwargs)\n        self.n_samples = n_samples\n        self.noise_std = noise_std\n\n    def generate(self, input_tensor: torch.Tensor, target_class: Optional[int] = None) -> Optional[np.ndarray]:\n        cams = []\n        for _ in range(self.n_samples):\n            noisy_input = input_tensor + torch.randn_like(input_tensor) * self.noise_std\n            cam = super().generate(noisy_input, target_class)\n            if cam is not None:\n                cams.append(cam)\n        if not cams:\n            return None\n        return np.mean(cams, axis=0)\n\ndef run_shap_explanation(model: nn.Module, images: torch.Tensor, background: torch.Tensor, device: torch.device, nsamples: int = 50) -> Optional[np.ndarray]:\n    if not SHAP_AVAILABLE:\n        print(\"SHAP not available, skipping...\")\n        return None\n    model.eval()\n    try:\n        background = background.clone().detach()\n        images = images.clone().detach()\n        if background.shape[0] > 10:\n            indices = torch.randperm(background.shape[0])[:10]\n            background = background[indices]\n        explainer = shap.GradientExplainer(model, background.to(device))\n        shap_values = explainer.shap_values(images.to(device), nsamples=nsamples)\n        if isinstance(shap_values, list):\n            shap_values = np.array(shap_values)\n        return shap_values\n    except Exception as e:\n        print(f\"SHAP failed with GradientExplainer: {e}\")\n        try:\n            if background.shape[0] > 5:\n                background = background[:5]\n            explainer = shap.DeepExplainer(model, background.to(device))\n            shap_values = explainer.shap_values(images.to(device))\n            return np.array(shap_values) if isinstance(shap_values, list) else shap_values\n        except Exception as e2:\n            print(f\"SHAP DeepExplainer also failed: {e2}\")\n            return None\n\nprint(\"XAI classes defined (GradCAM, SmoothGradCAM, SHAP).\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T06:27:38.963244Z","iopub.execute_input":"2025-12-24T06:27:38.963544Z","iopub.status.idle":"2025-12-24T06:27:38.987895Z","shell.execute_reply.started":"2025-12-24T06:27:38.963514Z","shell.execute_reply":"2025-12-24T06:27:38.987242Z"}},"outputs":[{"name":"stdout","text":"XAI classes defined (GradCAM, SmoothGradCAM, SHAP).\n","output_type":"stream"}],"execution_count":12},{"id":"d4f3a1a4","cell_type":"markdown","source":"## XAI Optimization & Final Training Functions\n\nOptimize XAI parameters using metaheuristic algorithms and define the final training pipeline with baseline comparison.","metadata":{}},{"id":"aff28cc8","cell_type":"code","source":"class XAIParameterSpace:\n    \"\"\"XAI parameter search space.\"\"\"\n    GRADCAM_BOUNDS = {'smoothing_sigma': (0.0, 3.0), 'n_smooth_samples': (1, 20), 'noise_std': (0.05, 0.3)}\n    SHAP_BOUNDS = {'background_size': (10, 100), 'nsamples': (20, 200)}\n\n    @classmethod\n    def decode_gradcam(cls, vector: np.ndarray) -> Dict:\n        return {\n            'smoothing_sigma': cls.GRADCAM_BOUNDS['smoothing_sigma'][0] + vector[0] * (cls.GRADCAM_BOUNDS['smoothing_sigma'][1] - cls.GRADCAM_BOUNDS['smoothing_sigma'][0]),\n            'n_smooth_samples': int(cls.GRADCAM_BOUNDS['n_smooth_samples'][0] + vector[1] * (cls.GRADCAM_BOUNDS['n_smooth_samples'][1] - cls.GRADCAM_BOUNDS['n_smooth_samples'][0])),\n            'noise_std': cls.GRADCAM_BOUNDS['noise_std'][0] + vector[2] * (cls.GRADCAM_BOUNDS['noise_std'][1] - cls.GRADCAM_BOUNDS['noise_std'][0])\n        }\n\n    @classmethod\n    def decode_shap(cls, vector: np.ndarray) -> Dict:\n        return {\n            'background_size': int(cls.SHAP_BOUNDS['background_size'][0] + vector[0] * (cls.SHAP_BOUNDS['background_size'][1] - cls.SHAP_BOUNDS['background_size'][0])),\n            'nsamples': int(cls.SHAP_BOUNDS['nsamples'][0] + vector[1] * (cls.SHAP_BOUNDS['nsamples'][1] - cls.SHAP_BOUNDS['nsamples'][0]))\n        }\n\ndef compute_xai_metrics(heatmap: np.ndarray, model: nn.Module, original_image: torch.Tensor, device: torch.device, \n                        gradcam_instance=None, target_class: Optional[int] = None, n_perturbations: int = 2) -> Dict[str, float]:\n    \"\"\"\n    Compute XAI quality metrics using established evaluation methods:\n    \n    1. Sparsity Score: Measures if explanation is appropriately focused\n    2. Faithfulness (Deletion): Masks top-k important pixels and measures confidence drop\n       Based on: Hooker et al. (2019) \"A Benchmark for Interpretability Methods in Deep Neural Networks\"\n    3. Stability: Measures consistency of explanations under small input perturbations\n       Based on: Alvarez-Melis & Jaakkola (2018) \"On the Robustness of Interpretability Methods\"\n    \"\"\"\n    metrics = {}\n    \n    # --- 1. SPARSITY SCORE ---\n    # Good explanations should be focused but not too sparse\n    if heatmap.max() > 0:\n        normalized = heatmap / heatmap.max()\n    else:\n        normalized = heatmap\n    high_activation_ratio = (normalized > 0.5).mean()\n    metrics['sparsity'] = 1.0 - high_activation_ratio\n    target_sparsity = 0.7  # Ideal sparsity level\n    metrics['sparsity_score'] = 1.0 - abs(metrics['sparsity'] - target_sparsity)\n    \n    # --- 2. FAITHFULNESS (DELETION METRIC) ---\n    # Mask top 10% important pixels and measure confidence drop\n    model.eval()\n    with torch.no_grad():\n        original_output = model(original_image.to(device))\n        original_pred = F.softmax(original_output, dim=1)\n        if target_class is None:\n            target_class = original_pred.argmax().item()\n        original_conf = original_pred[0, target_class].item()\n    \n    flat_heatmap = heatmap.flatten()\n    top_k = int(len(flat_heatmap) * 0.1)\n    top_indices = np.argsort(flat_heatmap)[-top_k:]\n    \n    masked_image = original_image.clone()\n    h, w = heatmap.shape\n    for idx in top_indices:\n        i, j = idx // w, idx % w\n        ratio_i = i / h\n        ratio_j = j / w\n        img_i = int(ratio_i * masked_image.shape[2])\n        img_j = int(ratio_j * masked_image.shape[3])\n        masked_image[0, :, max(0, img_i-2):min(masked_image.shape[2], img_i+2), \n                     max(0, img_j-2):min(masked_image.shape[3], img_j+2)] = 0\n    \n    with torch.no_grad():\n        masked_output = model(masked_image.to(device))\n        masked_pred = F.softmax(masked_output, dim=1)\n        masked_conf = masked_pred[0, target_class].item()\n    \n    metrics['faithfulness'] = max(0, original_conf - masked_conf)\n    \n    # --- 3. STABILITY (PERTURBATION CONSISTENCY) ---\n    # Measures how consistent explanations are under small input noise\n    # Based on: Alvarez-Melis & Jaakkola (2018) - cosine similarity between explanations\n    if gradcam_instance is not None:\n        stability_scores = []\n        noise_std = 0.02  # Small noise magnitude (2% of input range)\n        original_flat = heatmap.flatten()\n        original_norm = np.linalg.norm(original_flat)\n        \n        for _ in range(n_perturbations):\n            # Add small Gaussian noise to input\n            noisy_input = original_image + noise_std * torch.randn_like(original_image)\n            noisy_input = noisy_input.clamp(0, 1)  # Keep in valid range\n            \n            # Generate explanation for perturbed input\n            try:\n                perturbed_heatmap = gradcam_instance.generate(noisy_input.to(device), target_class)\n                if perturbed_heatmap is not None:\n                    perturbed_flat = perturbed_heatmap.flatten()\n                    perturbed_norm = np.linalg.norm(perturbed_flat)\n                    \n                    # Cosine similarity between original and perturbed explanations\n                    if original_norm > 0 and perturbed_norm > 0:\n                        cosine_sim = np.dot(original_flat, perturbed_flat) / (original_norm * perturbed_norm)\n                        stability_scores.append(max(0, cosine_sim))  # Clamp negative values\n                    else:\n                        stability_scores.append(0.0)\n                else:\n                    stability_scores.append(0.0)\n            except Exception:\n                stability_scores.append(0.0)\n        \n        metrics['stability'] = np.mean(stability_scores) if stability_scores else 0.0\n    else:\n        # For SHAP or when gradcam_instance not provided, compute structural stability\n        # Use local consistency as a proxy (how smooth the heatmap is)\n        # Smooth heatmaps tend to be more stable under perturbations\n        if len(heatmap.shape) == 2 and heatmap.shape[0] > 2 and heatmap.shape[1] > 2:\n            # Compute local variance as inverse stability proxy\n            # Low local variance = more stable explanations\n            grad_x = np.diff(normalized, axis=0)\n            grad_y = np.diff(normalized, axis=1)\n            local_var = (np.mean(grad_x**2) + np.mean(grad_y**2)) / 2\n            # Convert to stability score (inverse relationship, scaled)\n            metrics['stability'] = max(0, 1.0 - min(1.0, local_var * 10))\n        else:\n            metrics['stability'] = 0.5  # Neutral score for edge cases\n    \n    # --- OVERALL SCORE ---\n    # Weighted combination: faithfulness is most important for XAI quality\n    metrics['overall_score'] = (0.3 * metrics['sparsity_score'] + \n                                0.5 * metrics['faithfulness'] + \n                                0.2 * metrics['stability'])\n    return metrics\n\ndef run_xai_optimization(model: nn.Module, loaders: Dict[str, DataLoader], top_algorithms: List[str], config: Config, device: torch.device, run_dir: str) -> Dict:\n    \"\"\"Optimize XAI parameters using top metaheuristics.\"\"\"\n    print(\"\\n\" + \"=\"*50)\n    print(\"XAI OPTIMIZATION\")\n    print(\"=\"*50)\n\n    sample_images = []\n    sample_labels = []\n    for images, labels in loaders['val']:\n        for img, lbl in zip(images, labels):\n            sample_images.append(img.unsqueeze(0))\n            sample_labels.append(lbl.item())\n            if len(sample_images) >= 10:\n                break\n        if len(sample_images) >= 10:\n            break\n\n    results = {'gradcam': {}, 'shap': {}}\n\n    def create_gradcam_eval():\n        def evaluate(vector: np.ndarray) -> float:\n            params = XAIParameterSpace.decode_gradcam(vector)\n            gradcam = None\n            try:\n                if params['n_smooth_samples'] > 1:\n                    gradcam = SmoothGradCAM(model, smoothing_sigma=params['smoothing_sigma'], n_samples=params['n_smooth_samples'], noise_std=params['noise_std'])\n                else:\n                    gradcam = GradCAM(model, smoothing_sigma=params['smoothing_sigma'])\n                scores = []\n                for img in sample_images[:3]:\n                    heatmap = gradcam.generate(img.to(device))\n                    if heatmap is not None and heatmap.size > 0 and heatmap.max() > 0:\n                        # Pass gradcam instance for proper stability computation\n                        metrics = compute_xai_metrics(heatmap, model, img, device, gradcam_instance=gradcam)\n                        scores.append(metrics['overall_score'])\n                    else:\n                        scores.append(0.0)\n                return np.mean(scores) if scores else 0.0\n            except Exception as e:\n                print(f\"GradCAM eval failed: {e}\")\n                return 0.0\n            finally:\n                # Always clean up hooks\n                if gradcam is not None:\n                    gradcam.remove_hooks()\n        return evaluate\n\n    # Algorithm factory for GradCAM (3 dimensions)\n    def create_algo_3d(algo_key):\n        algo_factories = {\n            'DE': lambda: DifferentialEvolution(n_dims=3, population_size=config.META_POPULATION, iterations=config.META_ITERATIONS, seed=config.SEED),\n            'GWO': lambda: GrayWolfOptimizer(n_dims=3, population_size=config.META_POPULATION, iterations=config.META_ITERATIONS, seed=config.SEED),\n            'GWO_tuned': lambda: GrayWolfOptimizer(n_dims=3, population_size=config.META_POPULATION, iterations=config.META_ITERATIONS, seed=config.SEED),\n            'PSO': lambda: ParticleSwarmOptimization(n_dims=3, population_size=config.META_POPULATION, iterations=config.META_ITERATIONS, seed=config.SEED),\n            'BAT': lambda: BatAlgorithm(n_dims=3, population_size=config.META_POPULATION, iterations=config.META_ITERATIONS, seed=config.SEED),\n            'WOA_tuned': lambda: WhaleOptimizationAlgorithm(n_dims=3, population_size=config.META_POPULATION, iterations=config.META_ITERATIONS, seed=config.SEED)\n        }\n        base_key = algo_key.replace('_tuned', '')\n        if algo_key in algo_factories:\n            return algo_factories[algo_key]()\n        elif base_key in algo_factories:\n            return algo_factories[base_key]()\n        return None\n\n    # Algorithm factory for SHAP (2 dimensions)\n    def create_algo_2d(algo_key):\n        algo_factories = {\n            'DE': lambda: DifferentialEvolution(n_dims=2, population_size=config.META_POPULATION, iterations=config.META_ITERATIONS, seed=config.SEED),\n            'GWO': lambda: GrayWolfOptimizer(n_dims=2, population_size=config.META_POPULATION, iterations=config.META_ITERATIONS, seed=config.SEED),\n            'GWO_tuned': lambda: GrayWolfOptimizer(n_dims=2, population_size=config.META_POPULATION, iterations=config.META_ITERATIONS, seed=config.SEED),\n            'PSO': lambda: ParticleSwarmOptimization(n_dims=2, population_size=config.META_POPULATION, iterations=config.META_ITERATIONS, seed=config.SEED),\n            'BAT': lambda: BatAlgorithm(n_dims=2, population_size=config.META_POPULATION, iterations=config.META_ITERATIONS, seed=config.SEED),\n            'WOA_tuned': lambda: WhaleOptimizationAlgorithm(n_dims=2, population_size=config.META_POPULATION, iterations=config.META_ITERATIONS, seed=config.SEED)\n        }\n        base_key = algo_key.replace('_tuned', '')\n        if algo_key in algo_factories:\n            return algo_factories[algo_key]()\n        elif base_key in algo_factories:\n            return algo_factories[base_key]()\n        return None\n\n    print(\"\\n--- Optimizing Grad-CAM parameters ---\")\n    gradcam_eval = create_gradcam_eval()\n    for algo_name in top_algorithms[:4]:\n        print(f\"\\nUsing {algo_name} for Grad-CAM optimization...\")\n        try:\n            algo = create_algo_3d(algo_name)\n            if algo is None:\n                print(f\"  Algorithm {algo_name} not found, skipping...\")\n                continue\n            start_time = time.time()\n            best_vector, best_score = algo.optimize(gradcam_eval)\n            elapsed = time.time() - start_time\n            best_params = XAIParameterSpace.decode_gradcam(best_vector)\n            results['gradcam'][algo_name] = {'params': best_params, 'score': best_score, 'time': elapsed}\n            print(f\"  {algo_name}: Score={best_score:.4f}, Params={best_params}\")\n\n            # Save Grad-CAM plot for this algorithm\n            try:\n                if best_params['n_smooth_samples'] > 1:\n                    gradcam = SmoothGradCAM(model, smoothing_sigma=best_params['smoothing_sigma'], n_samples=best_params['n_smooth_samples'], noise_std=best_params['noise_std'])\n                else:\n                    gradcam = GradCAM(model, smoothing_sigma=best_params['smoothing_sigma'])\n                test_img = sample_images[0]\n                heatmap = gradcam.generate(test_img.to(device))\n                gradcam.remove_hooks()  # Clean up hooks\n                \n                if heatmap is not None and heatmap.size > 0:\n                    print(f\"  GradCAM heatmap shape: {heatmap.shape}, min={heatmap.min():.4f}, max={heatmap.max():.4f}\")\n                    \n                    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n                    \n                    # Properly denormalize the image (reverse ImageNet normalization)\n                    mean = np.array([0.485, 0.456, 0.406])\n                    std = np.array([0.229, 0.224, 0.225])\n                    img_tensor = test_img.squeeze(0).cpu().numpy()  # (C, H, W)\n                    img_array = img_tensor.transpose(1, 2, 0)  # (H, W, C)\n                    img_array = img_array * std + mean  # Denormalize\n                    img_array = np.clip(img_array, 0, 1)  # Clip to valid range (keep as float)\n                    \n                    # Panel 1: Original image\n                    axes[0].imshow(img_array)\n                    axes[0].set_title('Original Image')\n                    axes[0].axis('off')\n                    \n                    # GradCAM - resize heatmap if needed using scipy for smooth interpolation\n                    from scipy.ndimage import zoom as scipy_zoom\n                    if heatmap.shape[0] != img_array.shape[0] or heatmap.shape[1] != img_array.shape[1]:\n                        zoom_h = img_array.shape[0] / heatmap.shape[0]\n                        zoom_w = img_array.shape[1] / heatmap.shape[1]\n                        heatmap_resized = scipy_zoom(heatmap.astype(np.float64), (zoom_h, zoom_w), order=1)\n                        heatmap_resized = np.clip(heatmap_resized, 0, 1)\n                    else:\n                        heatmap_resized = heatmap\n                    \n                    # Re-normalize after resize to ensure full colormap range is used\n                    if heatmap_resized.max() > heatmap_resized.min():\n                        heatmap_resized = (heatmap_resized - heatmap_resized.min()) / (heatmap_resized.max() - heatmap_resized.min())\n                    print(f\"  Heatmap after resize: min={heatmap_resized.min():.4f}, max={heatmap_resized.max():.4f}\")\n                    \n                    # Panel 2: Pure heatmap (no original image underneath)\n                    im = axes[1].imshow(heatmap_resized, cmap='jet', vmin=0, vmax=1)\n                    axes[1].set_title('Grad-CAM Heatmap')\n                    axes[1].axis('off')\n                    plt.colorbar(im, ax=axes[1], fraction=0.046, pad=0.04)\n                    \n                    # Panel 3: Proper overlay using generate_overlay method\n                    # Convert img_array to uint8 for overlay\n                    img_uint8 = (img_array * 255).astype(np.uint8)\n                    overlay = gradcam.generate_overlay(img_uint8, heatmap_resized, alpha=0.5)\n                    axes[2].imshow(overlay)\n                    axes[2].set_title(f'{algo_name} Overlay')\n                    axes[2].axis('off')\n                    \n                    fig.suptitle(f'Grad-CAM Optimization Result - {algo_name}', fontsize=14, fontweight='bold')\n                    plt.tight_layout()\n                    plot_path = os.path.join(run_dir, f'gradcam_{algo_name}.png')\n                    plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n                    plt.close()\n                    print(f\"  Saved Grad-CAM plot: {plot_path}\")\n                else:\n                    print(f\"  GradCAM heatmap was None or empty for {algo_name}\")\n            except Exception as e:\n                import traceback\n                print(f\"  Failed to save Grad-CAM plot for {algo_name}: {e}\")\n                traceback.print_exc()\n        except Exception as e:\n            print(f\"  {algo_name} failed: {e}\")\n\n    if SHAP_AVAILABLE:\n        print(\"\\n--- Optimizing SHAP parameters ---\")\n        def create_shap_eval():\n            def evaluate(vector: np.ndarray) -> float:\n                params = XAIParameterSpace.decode_shap(vector)\n                try:\n                    bg_size = min(params['background_size'], len(sample_images), 20)\n                    background = torch.cat(sample_images[:bg_size], dim=0)\n                    test_img = sample_images[0]\n                    nsamples = min(params['nsamples'], 50)\n                    shap_values = run_shap_explanation(model, test_img, background, device, nsamples=nsamples)\n                    if shap_values is not None:\n                        # Convert SHAP values to a 2D heatmap for evaluation\n                        shap_arr = np.abs(shap_values)\n                        \n                        # Handle different SHAP output shapes\n                        if len(shap_arr.shape) == 5:  # (num_classes, batch, channels, H, W)\n                            shap_arr = shap_arr[0, 0]  # Take first class, first sample\n                        elif len(shap_arr.shape) == 4:  # (batch, channels, H, W)\n                            shap_arr = shap_arr[0]\n                        \n                        # Average across channels to get 2D heatmap\n                        if len(shap_arr.shape) == 3:\n                            heatmap = shap_arr.mean(axis=0)\n                        elif len(shap_arr.shape) == 2:\n                            heatmap = shap_arr\n                        else:\n                            return 0.0\n                        \n                        # Compute faithfulness and sparsity using compute_xai_metrics\n                        # (without gradcam_instance, stability uses local variance proxy)\n                        metrics = compute_xai_metrics(heatmap, model, test_img, device)\n                        \n                        # For SHAP, compute true perturbation-based stability\n                        # Generate SHAP for a perturbed input and compare\n                        noise_std = 0.02\n                        noisy_img = test_img + noise_std * torch.randn_like(test_img)\n                        noisy_img = noisy_img.clamp(0, 1)\n                        noisy_shap = run_shap_explanation(model, noisy_img, background, device, nsamples=max(10, nsamples//2))\n                        \n                        if noisy_shap is not None:\n                            noisy_arr = np.abs(noisy_shap)\n                            # Process noisy SHAP same way\n                            if len(noisy_arr.shape) == 5:\n                                noisy_arr = noisy_arr[0, 0]\n                            elif len(noisy_arr.shape) == 4:\n                                noisy_arr = noisy_arr[0]\n                            if len(noisy_arr.shape) == 3:\n                                noisy_heatmap = noisy_arr.mean(axis=0)\n                            elif len(noisy_arr.shape) == 2:\n                                noisy_heatmap = noisy_arr\n                            else:\n                                noisy_heatmap = None\n                            \n                            if noisy_heatmap is not None:\n                                # Cosine similarity for stability\n                                orig_flat = heatmap.flatten()\n                                noisy_flat = noisy_heatmap.flatten()\n                                orig_norm = np.linalg.norm(orig_flat)\n                                noisy_norm = np.linalg.norm(noisy_flat)\n                                if orig_norm > 0 and noisy_norm > 0:\n                                    stability = max(0, np.dot(orig_flat, noisy_flat) / (orig_norm * noisy_norm))\n                                else:\n                                    stability = 0.0\n                                # Override the proxy stability with real perturbation stability\n                                metrics['stability'] = stability\n                                metrics['overall_score'] = (0.3 * metrics['sparsity_score'] + \n                                                           0.5 * metrics['faithfulness'] + \n                                                           0.2 * metrics['stability'])\n                        \n                        return metrics['overall_score']\n                    return 0.0\n                except Exception as e:\n                    print(f\"SHAP eval failed: {e}\")\n                    return 0.0\n            return evaluate\n\n        shap_eval = create_shap_eval()\n        for algo_name in top_algorithms[:4]:  # Use 4 algorithms like GradCAM\n            print(f\"\\nUsing {algo_name} for SHAP optimization...\")\n            try:\n                algo = create_algo_2d(algo_name)\n                if algo is None:\n                    print(f\"  Algorithm {algo_name} not found, skipping...\")\n                    continue\n                start_time = time.time()\n                best_vector, best_score = algo.optimize(shap_eval)\n                elapsed = time.time() - start_time\n                best_params = XAIParameterSpace.decode_shap(best_vector)\n                results['shap'][algo_name] = {'params': best_params, 'score': best_score, 'time': elapsed}\n                print(f\"  {algo_name}: Score={best_score:.4f}, Params={best_params}\")\n\n                # Save SHAP plot for this algorithm\n                try:\n                    bg_size = min(best_params['background_size'], len(sample_images), 10)\n                    background = torch.cat(sample_images[:bg_size], dim=0)\n                    test_img = sample_images[0]\n                    nsamples = min(best_params['nsamples'], 50)\n                    shap_values = run_shap_explanation(model, test_img, background, device, nsamples=nsamples)\n                    if shap_values is not None:\n                        # Debug: print shape to understand the structure\n                        print(f\"  SHAP values shape: {shap_values.shape}\")\n                        \n                        # Process SHAP values to get a 2D heatmap\n                        shap_arr = np.abs(shap_values)\n                        # SHAP returns different shapes depending on explainer:\n                        # GradientExplainer: (num_classes, batch, channels, H, W) or (batch, channels, H, W)\n                        # DeepExplainer: similar patterns\n                        shap_arr = np.array(shap_values)\n                        print(f\"  SHAP array shape: {shap_arr.shape}\")\n                        \n                        # Reduce to (C, H, W) by taking first element of leading dimensions\n                        while len(shap_arr.shape) > 3:\n                            shap_arr = shap_arr[0]\n                        \n                        # Now shap_arr should be (C, H, W) or (H, W)\n                        if len(shap_arr.shape) == 3:\n                            # Sum absolute values across channels to get (H, W)\n                            shap_img = np.abs(shap_arr).sum(axis=0)\n                        elif len(shap_arr.shape) == 2:\n                            shap_img = np.abs(shap_arr)\n                        else:\n                            print(f\"  Unexpected SHAP shape after processing: {shap_arr.shape}\")\n                            shap_img = None\n                        \n                        if shap_img is not None and shap_img.size > 0:\n                            # Normalize to 0-1 for visualization\n                            shap_min, shap_max = shap_img.min(), shap_img.max()\n                            print(f\"  SHAP heatmap: shape={shap_img.shape}, min={shap_min:.6f}, max={shap_max:.6f}\")\n                            if shap_max > shap_min:\n                                shap_img_norm = (shap_img - shap_min) / (shap_max - shap_min)\n                            else:\n                                shap_img_norm = np.zeros_like(shap_img)\n                            \n                            # Create figure\n                            fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n                            \n                            # Properly denormalize the image (reverse ImageNet normalization)\n                            mean = np.array([0.485, 0.456, 0.406])\n                            std = np.array([0.229, 0.224, 0.225])\n                            img_tensor = test_img.squeeze(0).cpu().numpy()  # (C, H, W)\n                            img_array = img_tensor.transpose(1, 2, 0)  # (H, W, C)\n                            img_array = img_array * std + mean  # Denormalize\n                            img_array = np.clip(img_array, 0, 1)  # Clip to valid range\n                            \n                            axes[0].imshow(img_array)\n                            axes[0].set_title('Original Image')\n                            axes[0].axis('off')\n                            \n                            # SHAP heatmap - resize to match original image size\n                            from scipy.ndimage import zoom as scipy_zoom\n                            if shap_img_norm.shape[0] != img_array.shape[0] or shap_img_norm.shape[1] != img_array.shape[1]:\n                                zoom_h = img_array.shape[0] / shap_img_norm.shape[0]\n                                zoom_w = img_array.shape[1] / shap_img_norm.shape[1]\n                                shap_img_resized = scipy_zoom(shap_img_norm.astype(np.float64), (zoom_h, zoom_w), order=1)\n                                shap_img_resized = np.clip(shap_img_resized, 0, 1)\n                            else:\n                                shap_img_resized = shap_img_norm\n                            \n                            # Show SHAP as heatmap overlay\n                            axes[1].imshow(img_array)\n                            im = axes[1].imshow(shap_img_resized, cmap='jet', alpha=0.5)\n                            axes[1].set_title(f'{algo_name} SHAP Attribution')\n                            axes[1].axis('off')\n                            plt.colorbar(im, ax=axes[1], fraction=0.046, pad=0.04)\n                            \n                            fig.suptitle(f'SHAP Optimization Result - {algo_name}', fontsize=14, fontweight='bold')\n                            plt.tight_layout()\n                            plot_path = os.path.join(run_dir, f'shap_{algo_name}.png')\n                            plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n                            plt.close()\n                            print(f\"  Saved SHAP plot: {plot_path}\")\n                        else:\n                            print(f\"  Could not generate SHAP heatmap for {algo_name} (empty or None)\")\n                    else:\n                        print(f\"  SHAP values were None for {algo_name}\")\n                except Exception as e:\n                    import traceback\n                    print(f\"  Failed to save SHAP plot for {algo_name}: {e}\")\n                    traceback.print_exc()\n            except Exception as e:\n                print(f\"  {algo_name} failed: {e}\")\n\n    with open(os.path.join(run_dir, 'xai_params_best.json'), 'w') as f:\n        json.dump(results, f, indent=2, default=str)\n\n    # Generate XAI algorithm comparison plot\n    plot_xai_algorithm_comparison(results, run_dir)\n\n    return results\n\ndef create_html_summary(xai_dir: str, idx_to_class: Dict, run_dir: str = None, best_hyperparams: Dict = None, final_metrics: Dict = None):\n    \"\"\"Create HTML summary of XAI outputs with training curves and hyperparameters.\"\"\"\n    html_content = \"\"\"\n    <!DOCTYPE html>\n    <html>\n    <head>\n        <title>Alzheimer's Classification - Full Summary</title>\n        <style>\n            body { font-family: Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }\n            h1 { color: #333; text-align: center; }\n            h2 { color: #666; border-bottom: 2px solid #4CAF50; padding-bottom: 10px; }\n            h3 { color: #888; }\n            .section { background: white; margin: 20px 0; padding: 20px; border-radius: 10px; box-shadow: 0 2px 5px rgba(0,0,0,0.1); }\n            .class-section { margin: 20px 0; }\n            .image-grid { display: flex; flex-wrap: wrap; gap: 10px; justify-content: center; }\n            .image-item { border: 1px solid #ddd; padding: 5px; background: white; border-radius: 5px; }\n            .image-item img { max-width: 300px; border-radius: 3px; }\n            .image-item p { text-align: center; margin: 5px 0; font-size: 12px; }\n            .metrics-table { width: 100%; border-collapse: collapse; margin: 10px 0; }\n            .metrics-table th, .metrics-table td { border: 1px solid #ddd; padding: 10px; text-align: left; }\n            .metrics-table th { background-color: #4CAF50; color: white; }\n            .metrics-table tr:nth-child(even) { background-color: #f2f2f2; }\n            .plot-container { text-align: center; margin: 20px 0; }\n            .plot-container img { max-width: 100%; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.1); }\n        </style>\n    </head>\n    <body>\n        <h1> Alzheimer's MRI Classification - Summary Report</h1>\n        <p style=\"text-align: center; color: #666;\">Generated: \"\"\" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") + \"\"\"</p>\n    \"\"\"\n\n    # Best Hyperparameters Section\n    if best_hyperparams:\n        html_content += \"\"\"\n        <div class=\"section\">\n            <h2> Best Hyperparameters (from Metaheuristic Optimization)</h2>\n            <table class=\"metrics-table\">\n                <tr><th>Parameter</th><th>Value</th></tr>\n        \"\"\"\n        param_names = {\n            'lr': 'Learning Rate',\n            'weight_decay': 'Weight Decay',\n            'dropout': 'Dropout Rate',\n            'unfreeze_epoch': 'Unfreeze Backbone Epoch',\n            'batch_size': 'Batch Size',\n            'augment_strength': 'Augmentation Strength'\n        }\n        for key, value in best_hyperparams.items():\n            display_name = param_names.get(key, key)\n            if isinstance(value, float):\n                display_value = f\"{value:.6f}\" if value < 0.01 else f\"{value:.4f}\"\n            else:\n                display_value = str(value)\n            html_content += f\"<tr><td>{display_name}</td><td>{display_value}</td></tr>\"\n        html_content += \"\"\"\n            </table>\n        </div>\n        \"\"\"\n\n    # Final Model Metrics Section\n    if final_metrics:\n        html_content += \"\"\"\n        <div class=\"section\">\n            <h2> Final Model Performance</h2>\n            <table class=\"metrics-table\">\n                <tr><th>Metric</th><th>Value</th></tr>\n        \"\"\"\n        metric_names = {\n            'test_accuracy': 'Test Accuracy',\n            'test_balanced_accuracy': 'Balanced Accuracy',\n            'test_macro_f1': 'Macro F1 Score',\n            'best_epoch': 'Best Epoch',\n            'best_val_f1': 'Best Validation F1'\n        }\n        for key in ['test_accuracy', 'test_balanced_accuracy', 'test_macro_f1', 'best_epoch', 'best_val_f1']:\n            if key in final_metrics:\n                display_name = metric_names.get(key, key)\n                value = final_metrics[key]\n                if isinstance(value, float):\n                    display_value = f\"{value:.4f}\"\n                else:\n                    display_value = str(value)\n                html_content += f\"<tr><td>{display_name}</td><td>{display_value}</td></tr>\"\n        html_content += \"\"\"\n            </table>\n        </div>\n        \"\"\"\n\n    # Training Curves Section\n    if run_dir:\n        html_content += \"\"\"\n        <div class=\"section\">\n            <h2> Training & Optimization Curves</h2>\n        \"\"\"\n        plots_dir = os.path.join(run_dir, 'plots')\n        plots_to_include = [\n            ('training_curves.png', 'Training & Validation Curves'),\n            ('phase_1_convergence_curves.png', 'Phase 1 Algorithm Convergence'),\n            ('phase_1_algorithm_comparison.png', 'Phase 1 Algorithm Comparison'),\n            ('metaheuristic_ranking.png', 'Metaheuristic Ranking'),\n            ('baseline_vs_optimized.png', 'Baseline vs Optimized Performance'),\n            ('xai_algorithm_comparison.png', 'XAI Algorithm Comparison')\n        ]\n        plots_added = False\n        for plot_file, plot_title in plots_to_include:\n            plot_path = os.path.join(plots_dir, plot_file)\n            if os.path.exists(plot_path):\n                try:\n                    # Copy plot to xai_dir for HTML access\n                    dest_path = os.path.join(xai_dir, plot_file)\n                    shutil.copy(plot_path, dest_path)\n                    # Verify the copy was successful\n                    if os.path.exists(dest_path):\n                        html_content += f\"\"\"\n            <div class=\"plot-container\">\n                <h3>{plot_title}</h3>\n                <img src=\"{plot_file}\" alt=\"{plot_title}\" style=\"max-width: 800px;\">\n            </div>\"\"\"\n                        plots_added = True\n                except Exception as e:\n                    print(f\"  Warning: Could not copy {plot_file}: {e}\")\n        \n        if not plots_added:\n            html_content += \"<p>No training plots available.</p>\"\n        html_content += \"\"\"\n        </div>\"\"\"\n\n    # XAI Explanations Section\n    html_content += \"\"\"\n        <div class=\"section\">\n            <h2>XAI Explanations (Grad-CAM & SHAP)</h2>\n    \"\"\"\n    xai_images_found = False\n    for class_idx, class_name in idx_to_class.items():\n        class_dir = os.path.join(xai_dir, class_name.replace(' ', '_'))\n        if not os.path.exists(class_dir):\n            continue\n        \n        # Get list of images in this class directory\n        img_files = [f for f in sorted(os.listdir(class_dir)) if f.endswith('.png')]\n        if not img_files:\n            continue\n            \n        xai_images_found = True\n        html_content += f\"\"\"\n        <div class=\"class-section\">\n            <h3>{class_name}</h3>\n            <div class=\"image-grid\">\"\"\"\n        \n        for img_file in img_files:\n            img_path = os.path.join(class_dir, img_file)\n            if os.path.exists(img_path) and os.path.getsize(img_path) > 0:\n                # Use relative path from xai_dir to the image in subdirectory\n                relative_path = f\"{class_name.replace(' ', '_')}/{img_file}\"\n                html_content += f\"\"\"\n                <div class=\"image-item\">\n                    <img src=\"{relative_path}\" alt=\"{img_file}\">\n                    <p>{img_file}</p>\n                </div>\"\"\"\n        \n        html_content += \"\"\"\n            </div>\n        </div>\"\"\"\n    \n    if not xai_images_found:\n        html_content += \"<p>No XAI visualizations available.</p>\"\n\n    html_content += \"\"\"\n        </div>\n    </body>\n    </html>\n    \"\"\"\n    with open(os.path.join(xai_dir, 'summary.html'), 'w') as f:\n        f.write(html_content)\n    print(f\"  Saved HTML summary: {os.path.join(xai_dir, 'summary.html')}\")\n\ndef generate_final_explanations(model: nn.Module, loaders: Dict[str, DataLoader], metadata: Dict, xai_params: Dict, config: Config, device: torch.device, run_dir: str, best_hyperparams: Dict = None, final_metrics: Dict = None):\n    \"\"\"Generate final XAI explanations for sample images.\"\"\"\n    print(\"\\n\" + \"=\"*50)\n    print(\"GENERATING FINAL EXPLANATIONS\")\n    print(\"=\"*50)\n\n    xai_dir = os.path.join(run_dir, 'xai_outputs')\n    os.makedirs(xai_dir, exist_ok=True)\n\n    samples_per_class = {i: {'correct': [], 'incorrect': []} for i in range(NUM_CLASSES)}\n    model.eval()\n    idx_to_class = metadata.get('idx_to_class', {i: f\"Class_{i}\" for i in range(NUM_CLASSES)})\n    test_paths = metadata.get('test_paths', [])\n    test_labels = metadata.get('test_labels', [])\n\n    val_transform = T.Compose([\n        T.Resize((config.IMG_SIZE, config.IMG_SIZE)),\n        T.ToTensor(),\n        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n\n    with torch.no_grad():\n        for idx, (path, true_label) in enumerate(zip(test_paths, test_labels)):\n            if all(len(samples_per_class[c]['correct']) >= config.XAI_SAMPLES_PER_CLASS and \n                   len(samples_per_class[c]['incorrect']) >= config.XAI_SAMPLES_PER_CLASS \n                   for c in range(NUM_CLASSES)):\n                break\n            try:\n                img = Image.open(path).convert('RGB')\n                img_tensor = val_transform(img).unsqueeze(0).to(device)\n                output = model(img_tensor)\n                pred_label = output.argmax(dim=1).item()\n                category = 'correct' if pred_label == true_label else 'incorrect'\n                if len(samples_per_class[true_label][category]) < config.XAI_SAMPLES_PER_CLASS:\n                    samples_per_class[true_label][category].append({\n                        'path': path,\n                        'image': img,\n                        'tensor': img_tensor.cpu(),\n                        'true_label': true_label,\n                        'pred_label': pred_label\n                    })\n            except Exception as e:\n                continue\n\n    gradcam_params = {'smoothing_sigma': 1.0, 'n_smooth_samples': 5, 'noise_std': 0.1}\n    if 'gradcam' in xai_params and xai_params['gradcam']:\n        first_algo = list(xai_params['gradcam'].keys())[0]\n        gradcam_params = xai_params['gradcam'][first_algo].get('params', gradcam_params)\n\n    if gradcam_params.get('n_smooth_samples', 1) > 1:\n        gradcam = SmoothGradCAM(\n            model,\n            smoothing_sigma=gradcam_params.get('smoothing_sigma', 1.0),\n            n_samples=gradcam_params.get('n_smooth_samples', 5),\n            noise_std=gradcam_params.get('noise_std', 0.1)\n        )\n    else:\n        gradcam = GradCAM(model, smoothing_sigma=gradcam_params.get('smoothing_sigma', 1.0))\n\n    print(\"\\nGenerating Grad-CAM visualizations...\")\n    for class_idx in range(NUM_CLASSES):\n        class_name = idx_to_class.get(class_idx, f\"Class_{class_idx}\")\n        class_dir = os.path.join(xai_dir, class_name.replace(' ', '_'))\n        os.makedirs(class_dir, exist_ok=True)\n        for category in ['correct', 'incorrect']:\n            for sample_idx, sample in enumerate(samples_per_class[class_idx][category]):\n                try:\n                    heatmap = gradcam.generate(sample['tensor'].to(device), sample['true_label'])\n                    img_array = np.array(sample['image'].resize((config.IMG_SIZE, config.IMG_SIZE)))\n                    \n                    # Re-normalize heatmap to ensure full 0-1 range for visualization\n                    if heatmap is not None and heatmap.max() > heatmap.min():\n                        heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min())\n                    \n                    overlay = gradcam.generate_overlay(img_array, heatmap)\n                    save_path = os.path.join(class_dir, f\"{category}_{sample_idx}_gradcam.png\")\n                    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n                    axes[0].imshow(img_array)\n                    axes[0].set_title('Original')\n                    axes[0].axis('off')\n                    axes[1].imshow(heatmap, cmap='jet', vmin=0, vmax=1)\n                    axes[1].set_title('Grad-CAM Heatmap')\n                    axes[1].axis('off')\n                    axes[2].imshow(overlay)\n                    axes[2].set_title('Overlay')\n                    axes[2].axis('off')\n                    pred_name = idx_to_class.get(sample['pred_label'], f\"Class_{sample['pred_label']}\")\n                    fig.suptitle(f\"True: {class_name} | Pred: {pred_name} ({category})\")\n\n                    save_path = os.path.join(class_dir, f\"{category}_{sample_idx}_gradcam.png\")\n                    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n                    plt.close()\n                except Exception as e:\n                    print(f\"  Failed to generate Grad-CAM for {class_name} {category} {sample_idx}: {e}\")\n\n    if SHAP_AVAILABLE:\n        print(\"\\nGenerating SHAP visualizations...\")\n        background_images = []\n        for class_idx in range(NUM_CLASSES):\n            for category in ['correct', 'incorrect']:\n                for sample in samples_per_class[class_idx][category][:2]:\n                    background_images.append(sample['tensor'])\n\n        if background_images:\n            background = torch.cat(background_images[:min(20, len(background_images))], dim=0)\n            for class_idx in range(NUM_CLASSES):\n                class_name = idx_to_class.get(class_idx, f\"Class_{class_idx}\")\n                class_dir = os.path.join(xai_dir, class_name.replace(' ', '_'))\n                for category in ['correct']:\n                    for sample_idx, sample in enumerate(samples_per_class[class_idx][category][:2]):\n                        try:\n                            shap_values = run_shap_explanation(\n                                model, sample['tensor'], background, device,\n                                nsamples=config.SHAP_NSAMPLES\n                            )\n                            if shap_values is not None:\n                                try:\n                                    # Convert to numpy array and get shape\n                                    shap_arr = np.array(shap_values)\n                                    print(f\"  SHAP shape for {class_name}: {shap_arr.shape}\")\n                                    \n                                    # Reduce to (C, H, W) by taking first element of leading dimensions\n                                    while len(shap_arr.shape) > 3:\n                                        shap_arr = shap_arr[0]\n                                    \n                                    # Now get 2D heatmap by summing absolute values across channels\n                                    if len(shap_arr.shape) == 3:\n                                        shap_img = np.abs(shap_arr).sum(axis=0)\n                                    elif len(shap_arr.shape) == 2:\n                                        shap_img = np.abs(shap_arr)\n                                    else:\n                                        shap_img = None\n                                    \n                                    if shap_img is not None and shap_img.size > 0:\n                                        # Normalize for visualization\n                                        shap_min, shap_max = shap_img.min(), shap_img.max()\n                                        if shap_max > shap_min:\n                                            shap_img = (shap_img - shap_min) / (shap_max - shap_min)\n                                        \n                                        fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n                                        img_array = np.array(sample['image'].resize((config.IMG_SIZE, config.IMG_SIZE)))\n                                        axes[0].imshow(img_array)\n                                        axes[0].set_title('Original')\n                                        axes[0].axis('off')\n                                        \n                                        # Resize SHAP to match image size\n                                        from scipy.ndimage import zoom as scipy_zoom\n                                        if shap_img.shape[0] != img_array.shape[0] or shap_img.shape[1] != img_array.shape[1]:\n                                            zoom_h = img_array.shape[0] / shap_img.shape[0]\n                                            zoom_w = img_array.shape[1] / shap_img.shape[1]\n                                            shap_img = scipy_zoom(shap_img.astype(np.float64), (zoom_h, zoom_w), order=1)\n                                            shap_img = np.clip(shap_img, 0, 1)\n                                        \n                                        # Show as overlay\n                                        axes[1].imshow(img_array)\n                                        im = axes[1].imshow(shap_img, cmap='jet', alpha=0.65, vmin=0, vmax=1)\n                                        axes[1].set_title('SHAP Attribution')\n                                        axes[1].axis('off')\n                                        plt.colorbar(im, ax=axes[1], fraction=0.046, pad=0.04)\n                                        \n                                        pred_name = idx_to_class.get(sample['pred_label'], f\"Class_{sample['pred_label']}\")\n                                        fig.suptitle(f\"True: {class_name} | Pred: {pred_name}\")\n\n                                        save_path = os.path.join(class_dir, f\"{category}_{sample_idx}_shap.png\")\n                                        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n                                        plt.close()\n                                    else:\n                                        print(f\"  SHAP heatmap empty for {class_name} {category} {sample_idx}\")\n\n                                except Exception as e:\n                                    print(f\"  Failed SHAP for {class_name} {category} {sample_idx}: {e}\")\n                        except Exception as e:\n                            print(f\"  Failed SHAP for {class_name} {category} {sample_idx}: {e}\")\n\n    print(f\"\\nExplanations saved to: {xai_dir}\")\n    create_html_summary(xai_dir, idx_to_class, run_dir=run_dir, best_hyperparams=best_hyperparams, final_metrics=final_metrics)\n\ndef run_final_training(datasets: Dict, metadata: Dict, best_hyperparams: Dict, feature_mask: Optional[np.ndarray], config: Config, device: torch.device, run_dir: str) -> Tuple[nn.Module, Dict, Dict]:\n    \"\"\"Run final training with best hyperparameters.\"\"\"\n    print(\"\\n\" + \"=\"*50)\n    print(\"BASELINE MODEL TRAINING\")\n    print(\"=\"*50)\n\n    # Create baseline model with default hyperparameters\n    baseline_config = Config(TEST_RUN=config.TEST_RUN)\n    baseline_config.EPOCHS = config.EPOCHS if not config.TEST_RUN else 1\n    baseline_config.USE_WEIGHTED_SAMPLER = False\n    baseline_config.USE_CLASS_WEIGHTS = False\n\n    baseline_model = create_model(baseline_config, device)\n    if feature_mask is not None:\n        mask_tensor = torch.from_numpy(feature_mask).float()\n        baseline_model.set_feature_mask(mask_tensor)\n\n    baseline_loaders = create_dataloaders(datasets, metadata, baseline_config)\n    baseline_model, baseline_metrics, baseline_history = train_model(\n        baseline_model, baseline_loaders, baseline_config, metadata, device, run_dir,\n        epochs=baseline_config.EPOCHS, verbose=False\n    )\n\n    print(\"\\nBaseline Results:\")\n    print(f\"  Accuracy: {baseline_metrics['test_accuracy']:.4f}\")\n    print(f\"  Balanced Accuracy: {baseline_metrics['test_balanced_accuracy']:.4f}\")\n    print(f\"  Macro F1: {baseline_metrics['test_macro_f1']:.4f}\")\n\n    print(\"\\n\" + \"=\"*50)\n    print(\"FINAL MODEL TRAINING\")\n    print(\"=\"*50)\n    print(f\"Best hyperparameters: {best_hyperparams}\")\n\n    final_config = Config(TEST_RUN=config.TEST_RUN)\n    final_config.LR = best_hyperparams['lr']\n    final_config.WEIGHT_DECAY = best_hyperparams['weight_decay']\n    final_config.DROPOUT = best_hyperparams['dropout']\n    final_config.FREEZE_BACKBONE_EPOCHS = best_hyperparams['unfreeze_epoch']\n    final_config.BATCH_SIZE = best_hyperparams['batch_size']\n    final_config.AUGMENT_STRENGTH = best_hyperparams['augment_strength']\n    final_config.EPOCHS = config.EPOCHS + 2 if not config.TEST_RUN else 2\n    final_config.EARLY_STOPPING_PATIENCE = config.EARLY_STOPPING_PATIENCE\n    final_config.USE_WEIGHTED_SAMPLER = True\n    final_config.USE_CLASS_WEIGHTS = True\n\n    model = create_model(final_config, device)\n    if feature_mask is not None:\n        mask_tensor = torch.from_numpy(feature_mask).float()\n        model.set_feature_mask(mask_tensor)\n        print(f\"Applied feature mask: {int(feature_mask.sum())}/{len(feature_mask)} features\")\n\n    loaders = create_dataloaders(datasets, metadata, final_config)\n    model, final_metrics, history = train_model(\n        model, loaders, final_config, metadata, device, run_dir,\n        epochs=final_config.EPOCHS, verbose=True\n    )\n\n    final_checkpoint_path = os.path.join(run_dir, 'checkpoints', 'final_model.pt')\n    torch.save({\n        'model_state_dict': model.state_dict(),\n        'config': final_config.to_dict(),\n        'hyperparams': best_hyperparams,\n        'metrics': final_metrics\n    }, final_checkpoint_path)\n\n    with open(os.path.join(run_dir, 'final_metrics.json'), 'w') as f:\n        json.dump(final_metrics, f, indent=2, default=str)\n\n    print(f\"\\nFinal Test Results:\")\n    print(f\"  Accuracy: {final_metrics['test_accuracy']:.4f}\")\n    print(f\"  Balanced Accuracy: {final_metrics['test_balanced_accuracy']:.4f}\")\n    print(f\"  Macro F1: {final_metrics['test_macro_f1']:.4f}\")\n\n    # Compare with baseline\n    print(f\"\\n\" + \"=\"*50)\n    print(\"PERFORMANCE COMPARISON\")\n    print(\"=\"*50)\n    print(f\"{'Metric':<20} {'Baseline':<10} {'Optimized':<10} {'Improvement':<12}\")\n    print(\"-\" * 54)\n    metrics_to_compare = ['test_accuracy', 'test_balanced_accuracy', 'test_macro_f1']\n    for metric in metrics_to_compare:\n        baseline_val = baseline_metrics[metric]\n        final_val = final_metrics[metric]\n        improvement = final_val - baseline_val\n        print(f\"{metric.replace('test_', '').replace('_', ' ').title():<20} {baseline_val:<10.4f} {final_val:<10.4f} {improvement:<+12.4f}\")\n\n    print(f\"\\nOptimization Summary:\")\n    print(f\"  - Feature Selection: {int(feature_mask.sum()) if feature_mask is not None else 'N/A'}/{len(feature_mask) if feature_mask is not None else 'N/A'} features retained\")\n    print(f\"  - Hyperparameter Optimization: {' Completed' if best_hyperparams else ' Failed'}\")\n\n    # Generate comparison plots\n    print(\"\\nGenerating comparison plots...\")\n    plot_baseline_vs_optimized(baseline_metrics, final_metrics, run_dir)\n    \n    # Plot confusion matrices if available\n    if 'test_confusion_matrix' in baseline_metrics and 'test_confusion_matrix' in final_metrics:\n        plot_confusion_matrix_comparison(\n            baseline_metrics['test_confusion_matrix'],\n            final_metrics['test_confusion_matrix'],\n            CLASS_NAMES,\n            run_dir\n        )\n\n    return model, final_metrics, baseline_metrics\n\nprint(\"XAI optimization and final training functions defined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T06:27:38.989117Z","iopub.execute_input":"2025-12-24T06:27:38.989691Z","iopub.status.idle":"2025-12-24T06:27:39.081929Z","shell.execute_reply.started":"2025-12-24T06:27:38.989671Z","shell.execute_reply":"2025-12-24T06:27:39.081421Z"}},"outputs":[{"name":"stdout","text":"XAI optimization and final training functions defined.\n","output_type":"stream"}],"execution_count":13},{"id":"228b12ed","cell_type":"markdown","source":"---\n\n# Pipeline Execution\n\nRun the cells below to execute the complete Alzheimer's classification pipeline with nature-inspired optimization.\n\n---\n\n## Load Dataset\n\nLoad the OASIS Alzheimer MRI dataset and create DataLoaders.","metadata":{}},{"id":"91f45b31","cell_type":"code","source":"\nprint(\"\\n\" + \"#\"*60)\nprint(\"# DATA LOADING\")\nprint(\"#\"*60)\n\n# Create run directory\nrun_dir = create_run_directory(config.OUTPUT_DIR)\nprint(f\"Run directory: {run_dir}\")\n\n# Save config\nconfig_path = os.path.join(run_dir, 'config.json')\nconfig.save(config_path)\nprint(f\"Config saved to: {config_path}\")\n\n# Load dataset directly from Kaggle input path\ndatasets, metadata, transforms_dict = load_dataset(config, config.DATA_DIR)\nloaders = create_dataloaders(datasets, metadata, config)\n\nprint(f\"\\n Dataset loaded successfully!\")\nprint(f\"  Classes: {metadata['class_to_idx']}\")\nprint(f\"  Train samples: {len(datasets['train'])}\")\nprint(f\"  Val samples: {len(datasets['val'])}\")\nprint(f\"  Test samples: {len(datasets['test'])}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T06:27:39.082636Z","iopub.execute_input":"2025-12-24T06:27:39.082823Z","iopub.status.idle":"2025-12-24T06:27:41.294951Z","shell.execute_reply.started":"2025-12-24T06:27:39.082805Z","shell.execute_reply":"2025-12-24T06:27:41.294202Z"}},"outputs":[{"name":"stdout","text":"\n############################################################\n# DATA LOADING\n############################################################\nRun directory: /kaggle/working/runs/20251224_062739_d69fab59\nConfig saved to: /kaggle/working/runs/20251224_062739_d69fab59/config.json\n\n==================================================\nLOADING DATASET\n==================================================\nUsing data directory: /kaggle/input/imagesoasis/Data\nFound subdirectories: ['Mild Dementia', 'Moderate Dementia', 'Non Demented', 'Very mild Dementia']\nClass mapping: {'Mild Dementia': 0, 'Moderate Dementia': 1, 'Non Demented': 2, 'Very mild Dementia': 3}\n\nTotal images found: 86437\n\nClass distribution:\n  Mild Dementia: 5002\n  Moderate Dementia: 488\n  Non Demented: 67222\n  Very mild Dementia: 13725\n\nApplying balanced subset (target: 10,000 images)\n  Mild Dementia: 3512 images (target: 3512, available: 5002)\n  Moderate Dementia: 488 images (target: 488, available: 488)\n  Non Demented: 3000 images (target: 3000, available: 67222)\n  Very mild Dementia: 3000 images (target: 3000, available: 13725)\n\nBalanced subset - Total: 10000 images\nNew class distribution:\n  Mild Dementia: 3512\n  Moderate Dementia: 488\n  Non Demented: 3000\n  Very mild Dementia: 3000\n\nSplit sizes:\n  Train: 7000\n  Val: 1500\n  Test: 1500\n\n Dataset loaded successfully!\n  Classes: {'Mild Dementia': 0, 'Moderate Dementia': 1, 'Non Demented': 2, 'Very mild Dementia': 3}\n  Train samples: 7000\n  Val samples: 1500\n  Test samples: 1500\n","output_type":"stream"}],"execution_count":14},{"id":"a92ad047","cell_type":"markdown","source":"## Run ACO Feature Selection\n\nUse Ant Colony Optimization to select important features from the backbone embeddings.","metadata":{}},{"id":"8cc41bd1","cell_type":"code","source":"print(\"\\n\" + \"#\"*60)\nprint(\"# ACO FEATURE SELECTION\")\nprint(\"#\"*60)\n\n# Create initial model for feature extraction\nmodel = create_model(config, device)\n\n# Quick initial training for feature extraction\nprint(\"\\nInitial model training for feature extraction...\")\nmodel, _, _ = train_model(\n    model, loaders, config, metadata, device, run_dir,\n    epochs=max(1, config.EPOCHS // 3), verbose=True\n)\n\n# Run ACO feature selection\nfeature_mask, aco_results = run_aco_feature_selection(model, loaders, config, device)\n\n# Save ACO results\naco_results_path = os.path.join(run_dir, 'aco_results.json')\nwith open(aco_results_path, 'w') as f:\n    json.dump({\n        'selected_features': int(np.sum(feature_mask)) if feature_mask is not None else None,\n        'total_features': len(feature_mask) if feature_mask is not None else None,\n        'performance': {k: v for k, v in aco_results.items() if k != 'history'}\n    }, f, indent=2)\n\nprint(f\"\\n ACO results saved to: {aco_results_path}\")\n\n# Memory cleanup\ndel model\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T06:27:41.295935Z","iopub.execute_input":"2025-12-24T06:27:41.296306Z","iopub.status.idle":"2025-12-24T06:43:36.082123Z","shell.execute_reply.started":"2025-12-24T06:27:41.296280Z","shell.execute_reply":"2025-12-24T06:43:36.081520Z"}},"outputs":[{"name":"stdout","text":"\n############################################################\n# ACO FEATURE SELECTION\n############################################################\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/21.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad10c6e90b854020a3b2d2b7fdc20371"}},"metadata":{}},{"name":"stdout","text":"\nModel created:\n  Total parameters: 4,336,512\n  Trainable parameters: 4,336,512\n\nInitial model training for feature extraction...\nBackbone frozen for first 2 epochs\n\n  Starting Epoch 1/1...\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 1.2618\n    Batch 20/219, Loss: 0.8904\n    Batch 30/219, Loss: 0.9370\n    Batch 40/219, Loss: 0.9455\n    Batch 50/219, Loss: 0.9395\n    Batch 60/219, Loss: 0.6698\n    Batch 70/219, Loss: 0.6518\n    Batch 80/219, Loss: 0.6093\n    Batch 90/219, Loss: 0.8475\n    Batch 100/219, Loss: 0.7321\n    Batch 110/219, Loss: 0.6203\n    Batch 120/219, Loss: 1.0238\n    Batch 130/219, Loss: 0.8564\n    Batch 140/219, Loss: 0.6798\n    Batch 150/219, Loss: 0.7151\n    Batch 160/219, Loss: 0.6912\n    Batch 170/219, Loss: 0.6614\n    Batch 180/219, Loss: 0.5612\n    Batch 190/219, Loss: 0.3871\n    Batch 200/219, Loss: 0.5468\n    Batch 210/219, Loss: 0.4436\n    Batch 219/219, Loss: 0.6493\nEpoch 1/1 (50.8s) - Train Loss: 0.7321, Val F1: 0.3901, Val Acc: 0.4353\n\n==================================================\nACO FEATURE SELECTION\n==================================================\nExtracting features...\nFeature dimension: 1280\nBaseline (all features): Macro F1 = 0.5256\n\nRunning ACO Feature Selection...\n  Iteration 1/10: Best Score = 0.5568, Mean = 0.5284\n  Iteration 2/10: Best Score = 0.5568, Mean = 0.5291\n  Iteration 3/10: Best Score = 0.5568, Mean = 0.5262\n  Iteration 4/10: Best Score = 0.5596, Mean = 0.5311\n  Iteration 5/10: Best Score = 0.5596, Mean = 0.5342\n  Iteration 6/10: Best Score = 0.5688, Mean = 0.5335\n  Iteration 7/10: Best Score = 0.5688, Mean = 0.5334\n  Iteration 8/10: Best Score = 0.5688, Mean = 0.5360\n  Iteration 9/10: Best Score = 0.5688, Mean = 0.5284\n  Iteration 10/10: Best Score = 0.5688, Mean = 0.5244\n\nACO Result: Selected 640/1280 features\nBest Macro F1: 0.5688\nImprovement: +0.0433\n\n ACO results saved to: /kaggle/working/runs/20251224_062739_d69fab59/aco_results.json\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"12929"},"metadata":{}}],"execution_count":15},{"id":"ad6f1e08","cell_type":"markdown","source":"## Phase 1: Hyperparameter Optimization\n\nRun DE, GWO, PSO, BAT algorithms to find optimal hyperparameters for the classifier.","metadata":{}},{"id":"07283fd9","cell_type":"code","source":"print(\"\\n\" + \"#\"*60)\nprint(\"# PHASE 1: HYPERPARAMETER OPTIMIZATION\")\nprint(\"#\"*60)\n\nphase1_results, convergence_histories = run_phase1_optimization(datasets, metadata, config, device, run_dir)\n\n# Save Phase 1 results\nphase1_path = os.path.join(run_dir, 'phase1_results.csv')\nphase1_results.to_csv(phase1_path, index=False)\nprint(f\"\\nPhase 1 results saved to: {phase1_path}\")\n\n# Memory cleanup after Phase 1\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\ngc.collect()\n\n# Get best hyperparams from Phase 1\nbest_hyperparams = get_best_hyperparams(phase1_results)\nprint(f\"Best hyperparameters found: {best_hyperparams}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T06:43:36.083089Z","iopub.execute_input":"2025-12-24T06:43:36.083336Z","iopub.status.idle":"2025-12-24T08:53:39.526249Z","shell.execute_reply.started":"2025-12-24T06:43:36.083313Z","shell.execute_reply":"2025-12-24T08:53:39.525569Z"}},"outputs":[{"name":"stdout","text":"\n############################################################\n# PHASE 1: HYPERPARAMETER OPTIMIZATION\n############################################################\n\n==================================================\nPHASE 1: HYPERPARAMETER OPTIMIZATION\n==================================================\n\n--- Running DE ---\n    DE Eval 1: LR=0.000085, Dropout=0.44, BS=8\n    First batch loaded, starting training...\n    Batch 10/875, Loss: 1.3232\n    Batch 20/875, Loss: 1.2125\n    Batch 30/875, Loss: 1.3362\n    Batch 40/875, Loss: 1.1702\n    Batch 50/875, Loss: 1.6731\n    Batch 60/875, Loss: 0.9733\n    Batch 70/875, Loss: 1.1739\n    Batch 80/875, Loss: 1.3148\n    Batch 90/875, Loss: 0.8926\n    Batch 100/875, Loss: 0.9555\n    Batch 110/875, Loss: 1.2175\n    Batch 120/875, Loss: 0.7213\n    Batch 130/875, Loss: 0.8251\n    Batch 140/875, Loss: 1.4694\n    Batch 150/875, Loss: 1.8972\n    Batch 160/875, Loss: 1.9496\n    Batch 170/875, Loss: 0.6722\n    Batch 180/875, Loss: 1.0399\n    Batch 190/875, Loss: 0.8599\n    Batch 200/875, Loss: 0.7591\n    Batch 210/875, Loss: 1.0857\n    Batch 220/875, Loss: 0.9508\n    Batch 230/875, Loss: 1.8806\n    Batch 240/875, Loss: 1.0621\n    Batch 250/875, Loss: 0.7882\n    Batch 260/875, Loss: 1.2410\n    Batch 270/875, Loss: 0.8537\n    Batch 280/875, Loss: 0.5770\n    Batch 290/875, Loss: 0.6756\n    Batch 300/875, Loss: 0.8215\n    Batch 310/875, Loss: 2.0095\n    Batch 320/875, Loss: 0.5765\n    Batch 330/875, Loss: 0.6717\n    Batch 340/875, Loss: 1.9717\n    Batch 350/875, Loss: 0.7570\n    Batch 360/875, Loss: 1.7527\n    Batch 370/875, Loss: 0.6452\n    Batch 380/875, Loss: 1.7397\n    Batch 390/875, Loss: 0.8921\n    Batch 400/875, Loss: 1.3014\n    Batch 410/875, Loss: 0.5730\n    Batch 420/875, Loss: 0.9249\n    Batch 430/875, Loss: 1.9519\n    Batch 440/875, Loss: 0.9892\n    Batch 450/875, Loss: 0.7596\n    Batch 460/875, Loss: 1.4090\n    Batch 470/875, Loss: 1.0284\n    Batch 480/875, Loss: 0.8313\n    Batch 490/875, Loss: 0.6365\n    Batch 500/875, Loss: 0.9564\n    Batch 510/875, Loss: 0.6443\n    Batch 520/875, Loss: 1.8542\n    Batch 530/875, Loss: 1.7686\n    Batch 540/875, Loss: 0.9834\n    Batch 550/875, Loss: 0.9227\n    Batch 560/875, Loss: 0.7170\n    Batch 570/875, Loss: 0.9023\n    Batch 580/875, Loss: 0.9111\n    Batch 590/875, Loss: 0.7230\n    Batch 600/875, Loss: 0.7188\n    Batch 610/875, Loss: 0.9972\n    Batch 620/875, Loss: 1.3292\n    Batch 630/875, Loss: 0.7828\n    Batch 640/875, Loss: 0.7669\n    Batch 650/875, Loss: 0.8053\n    Batch 660/875, Loss: 0.9390\n    Batch 670/875, Loss: 1.2873\n    Batch 680/875, Loss: 0.4372\n    Batch 690/875, Loss: 0.6438\n    Batch 700/875, Loss: 0.7783\n    Batch 710/875, Loss: 0.6140\n    Batch 720/875, Loss: 0.9422\n    Batch 730/875, Loss: 0.6767\n    Batch 740/875, Loss: 1.7215\n    Batch 750/875, Loss: 1.0707\n    Batch 760/875, Loss: 1.5572\n    Batch 770/875, Loss: 1.7258\n    Batch 780/875, Loss: 0.5323\n    Batch 790/875, Loss: 0.7751\n    Batch 800/875, Loss: 0.8233\n    Batch 810/875, Loss: 1.0422\n    Batch 820/875, Loss: 0.4951\n    Batch 830/875, Loss: 1.0618\n    Batch 840/875, Loss: 0.9346\n    Batch 850/875, Loss: 0.6467\n    Batch 860/875, Loss: 0.6742\n    Batch 870/875, Loss: 0.5450\n    Batch 875/875, Loss: 0.7433\n    DE Eval 2: LR=0.000014, Dropout=0.36, BS=8\n    First batch loaded, starting training...\n    Batch 10/875, Loss: 1.3778\n    Batch 20/875, Loss: 1.4047\n    Batch 30/875, Loss: 1.2893\n    Batch 40/875, Loss: 1.2528\n    Batch 50/875, Loss: 1.4083\n    Batch 60/875, Loss: 1.4769\n    Batch 70/875, Loss: 1.3703\n    Batch 80/875, Loss: 1.2816\n    Batch 90/875, Loss: 1.1908\n    Batch 100/875, Loss: 1.4553\n    Batch 110/875, Loss: 1.3231\n    Batch 120/875, Loss: 1.3649\n    Batch 130/875, Loss: 1.2560\n    Batch 140/875, Loss: 1.2490\n    Batch 150/875, Loss: 1.4989\n    Batch 160/875, Loss: 1.0712\n    Batch 170/875, Loss: 1.4971\n    Batch 180/875, Loss: 1.2792\n    Batch 190/875, Loss: 1.0816\n    Batch 200/875, Loss: 1.2020\n    Batch 210/875, Loss: 1.0113\n    Batch 220/875, Loss: 1.0566\n    Batch 230/875, Loss: 1.3057\n    Batch 240/875, Loss: 1.0191\n    Batch 250/875, Loss: 1.1080\n    Batch 260/875, Loss: 1.1088\n    Batch 270/875, Loss: 1.1403\n    Batch 280/875, Loss: 1.0134\n    Batch 290/875, Loss: 1.1378\n    Batch 300/875, Loss: 1.2379\n    Batch 310/875, Loss: 1.7297\n    Batch 320/875, Loss: 0.9616\n    Batch 330/875, Loss: 1.0225\n    Batch 340/875, Loss: 0.9502\n    Batch 350/875, Loss: 1.2675\n    Batch 360/875, Loss: 1.0332\n    Batch 370/875, Loss: 0.7230\n    Batch 380/875, Loss: 1.0065\n    Batch 390/875, Loss: 0.9679\n    Batch 400/875, Loss: 1.1355\n    Batch 410/875, Loss: 0.9090\n    Batch 420/875, Loss: 1.0723\n    Batch 430/875, Loss: 0.8631\n    Batch 440/875, Loss: 0.8843\n    Batch 450/875, Loss: 0.9848\n    Batch 460/875, Loss: 0.8207\n    Batch 470/875, Loss: 0.8641\n    Batch 480/875, Loss: 1.8563\n    Batch 490/875, Loss: 0.8519\n    Batch 500/875, Loss: 0.8797\n    Batch 510/875, Loss: 0.9420\n    Batch 520/875, Loss: 0.9655\n    Batch 530/875, Loss: 0.8588\n    Batch 540/875, Loss: 0.7306\n    Batch 550/875, Loss: 0.8417\n    Batch 560/875, Loss: 1.1836\n    Batch 570/875, Loss: 1.0020\n    Batch 580/875, Loss: 1.3014\n    Batch 590/875, Loss: 1.1528\n    Batch 600/875, Loss: 0.8123\n    Batch 610/875, Loss: 1.1813\n    Batch 620/875, Loss: 0.8134\n    Batch 630/875, Loss: 1.2465\n    Batch 640/875, Loss: 0.8238\n    Batch 650/875, Loss: 1.0581\n    Batch 660/875, Loss: 0.9374\n    Batch 670/875, Loss: 1.3295\n    Batch 680/875, Loss: 0.7712\n    Batch 690/875, Loss: 1.0016\n    Batch 700/875, Loss: 1.0542\n    Batch 710/875, Loss: 1.0495\n    Batch 720/875, Loss: 1.2756\n    Batch 730/875, Loss: 0.9282\n    Batch 740/875, Loss: 0.8891\n    Batch 750/875, Loss: 1.0400\n    Batch 760/875, Loss: 2.0524\n    Batch 770/875, Loss: 1.3350\n    Batch 780/875, Loss: 0.7248\n    Batch 790/875, Loss: 0.7797\n    Batch 800/875, Loss: 1.2962\n    Batch 810/875, Loss: 1.3134\n    Batch 820/875, Loss: 1.4525\n    Batch 830/875, Loss: 0.8309\n    Batch 840/875, Loss: 0.8784\n    Batch 850/875, Loss: 0.9832\n    Batch 860/875, Loss: 1.0224\n    Batch 870/875, Loss: 1.1665\n    Batch 875/875, Loss: 0.8990\n    DE Eval 3: LR=0.001154, Dropout=0.11, BS=8\n    First batch loaded, starting training...\n    Batch 10/875, Loss: 1.2944\n    Batch 20/875, Loss: 1.8661\n    Batch 30/875, Loss: 0.6852\n    Batch 40/875, Loss: 0.7192\n    Batch 50/875, Loss: 0.9034\n    Batch 60/875, Loss: 0.6266\n    Batch 70/875, Loss: 1.2067\n    Batch 80/875, Loss: 1.7051\n    Batch 90/875, Loss: 1.2982\n    Batch 100/875, Loss: 0.9417\n    Batch 110/875, Loss: 0.8354\n    Batch 120/875, Loss: 1.9010\n    Batch 130/875, Loss: 0.9386\n    Batch 140/875, Loss: 0.5305\n    Batch 150/875, Loss: 0.7713\n    Batch 160/875, Loss: 0.5730\n    Batch 170/875, Loss: 1.0856\n    Batch 180/875, Loss: 0.6897\n    Batch 190/875, Loss: 0.5131\n    Batch 200/875, Loss: 1.5316\n    Batch 210/875, Loss: 0.3053\n    Batch 220/875, Loss: 1.0158\n    Batch 230/875, Loss: 0.9717\n    Batch 240/875, Loss: 0.4964\n    Batch 250/875, Loss: 2.4993\n    Batch 260/875, Loss: 0.5490\n    Batch 270/875, Loss: 0.6066\n    Batch 280/875, Loss: 0.7412\n    Batch 290/875, Loss: 2.1038\n    Batch 300/875, Loss: 0.3254\n    Batch 310/875, Loss: 0.1400\n    Batch 320/875, Loss: 0.8672\n    Batch 330/875, Loss: 0.6469\n    Batch 340/875, Loss: 1.5251\n    Batch 350/875, Loss: 2.1401\n    Batch 360/875, Loss: 0.6317\n    Batch 370/875, Loss: 0.4243\n    Batch 380/875, Loss: 0.2202\n    Batch 390/875, Loss: 0.5844\n    Batch 400/875, Loss: 0.4617\n    Batch 410/875, Loss: 1.0079\n    Batch 420/875, Loss: 0.6656\n    Batch 430/875, Loss: 0.4055\n    Batch 440/875, Loss: 0.6357\n    Batch 450/875, Loss: 0.5122\n    Batch 460/875, Loss: 0.5853\n    Batch 470/875, Loss: 0.2952\n    Batch 480/875, Loss: 0.3898\n    Batch 490/875, Loss: 0.1600\n    Batch 500/875, Loss: 0.5792\n    Batch 510/875, Loss: 0.2532\n    Batch 520/875, Loss: 0.6826\n    Batch 530/875, Loss: 0.5992\n    Batch 540/875, Loss: 0.4704\n    Batch 550/875, Loss: 0.5709\n    Batch 560/875, Loss: 0.7894\n    Batch 570/875, Loss: 0.2094\n    Batch 580/875, Loss: 0.6303\n    Batch 590/875, Loss: 0.3882\n    Batch 600/875, Loss: 0.4211\n    Batch 610/875, Loss: 0.4288\n    Batch 620/875, Loss: 0.3758\n    Batch 630/875, Loss: 0.2650\n    Batch 640/875, Loss: 1.9393\n    Batch 650/875, Loss: 0.5521\n    Batch 660/875, Loss: 0.3385\n    Batch 670/875, Loss: 0.3899\n    Batch 680/875, Loss: 0.2516\n    Batch 690/875, Loss: 0.1432\n    Batch 700/875, Loss: 0.3231\n    Batch 710/875, Loss: 0.5359\n    Batch 720/875, Loss: 0.3047\n    Batch 730/875, Loss: 0.7111\n    Batch 740/875, Loss: 0.7092\n    Batch 750/875, Loss: 0.9323\n    Batch 760/875, Loss: 0.5817\n    Batch 770/875, Loss: 0.3537\n    Batch 780/875, Loss: 1.1703\n    Batch 790/875, Loss: 0.2612\n    Batch 800/875, Loss: 0.3724\n    Batch 810/875, Loss: 0.7252\n    Batch 820/875, Loss: 0.7823\n    Batch 830/875, Loss: 0.1767\n    Batch 840/875, Loss: 0.3163\n    Batch 850/875, Loss: 0.2646\n    Batch 860/875, Loss: 0.3511\n    Batch 870/875, Loss: 0.6559\n    Batch 875/875, Loss: 0.3022\n    DE Eval 4: LR=0.000117, Dropout=0.37, BS=8\n    First batch loaded, starting training...\n    Batch 10/875, Loss: 1.2587\n    Batch 20/875, Loss: 1.0252\n    Batch 30/875, Loss: 1.0445\n    Batch 40/875, Loss: 0.6741\n    Batch 50/875, Loss: 0.4710\n    Batch 60/875, Loss: 0.6565\n    Batch 70/875, Loss: 0.5232\n    Batch 80/875, Loss: 0.7041\n    Batch 90/875, Loss: 0.9409\n    Batch 100/875, Loss: 0.8197\n    Batch 110/875, Loss: 0.5824\n    Batch 120/875, Loss: 0.7668\n    Batch 130/875, Loss: 0.4889\n    Batch 140/875, Loss: 0.8968\n    Batch 150/875, Loss: 0.5062\n    Batch 160/875, Loss: 0.4752\n    Batch 170/875, Loss: 0.4669\n    Batch 180/875, Loss: 0.4047\n    Batch 190/875, Loss: 0.4306\n    Batch 200/875, Loss: 0.8068\n    Batch 210/875, Loss: 0.6558\n    Batch 220/875, Loss: 0.3633\n    Batch 230/875, Loss: 0.7328\n    Batch 240/875, Loss: 0.4638\n    Batch 250/875, Loss: 0.5375\n    Batch 260/875, Loss: 0.3941\n    Batch 270/875, Loss: 0.3057\n    Batch 280/875, Loss: 1.8962\n    Batch 290/875, Loss: 0.4438\n    Batch 300/875, Loss: 0.4078\n    Batch 310/875, Loss: 1.2625\n    Batch 320/875, Loss: 0.6742\n    Batch 330/875, Loss: 0.8246\n    Batch 340/875, Loss: 0.4292\n    Batch 350/875, Loss: 1.7628\n    Batch 360/875, Loss: 0.5150\n    Batch 370/875, Loss: 0.6549\n    Batch 380/875, Loss: 0.3434\n    Batch 390/875, Loss: 0.4684\n    Batch 400/875, Loss: 0.3469\n    Batch 410/875, Loss: 0.2842\n    Batch 420/875, Loss: 0.2367\n    Batch 430/875, Loss: 0.6409\n    Batch 440/875, Loss: 0.3065\n    Batch 450/875, Loss: 0.2941\n    Batch 460/875, Loss: 0.2117\n    Batch 470/875, Loss: 0.2352\n    Batch 480/875, Loss: 0.4932\n    Batch 490/875, Loss: 0.5709\n    Batch 500/875, Loss: 0.2653\n    Batch 510/875, Loss: 0.2970\n    Batch 520/875, Loss: 0.2391\n    Batch 530/875, Loss: 0.2731\n    Batch 540/875, Loss: 0.2255\n    Batch 550/875, Loss: 0.1895\n    Batch 560/875, Loss: 0.3034\n    Batch 570/875, Loss: 0.2664\n    Batch 580/875, Loss: 0.1699\n    Batch 590/875, Loss: 0.5234\n    Batch 600/875, Loss: 0.6809\n    Batch 610/875, Loss: 0.3063\n    Batch 620/875, Loss: 0.7195\n    Batch 630/875, Loss: 0.4628\n    Batch 640/875, Loss: 0.4224\n    Batch 650/875, Loss: 0.3609\n    Batch 660/875, Loss: 0.2054\n    Batch 670/875, Loss: 0.3742\n    Batch 680/875, Loss: 0.4490\n    Batch 690/875, Loss: 0.1597\n    Batch 700/875, Loss: 0.3203\n    Batch 710/875, Loss: 0.2059\n    Batch 720/875, Loss: 0.2036\n    Batch 730/875, Loss: 1.0185\n    Batch 740/875, Loss: 0.3285\n    Batch 750/875, Loss: 0.1822\n    Batch 760/875, Loss: 0.1013\n    Batch 770/875, Loss: 0.1706\n    Batch 780/875, Loss: 0.2669\n    Batch 790/875, Loss: 0.6526\n    Batch 800/875, Loss: 0.2240\n    Batch 810/875, Loss: 0.2667\n    Batch 820/875, Loss: 0.4146\n    Batch 830/875, Loss: 0.1990\n    Batch 840/875, Loss: 0.1714\n    Batch 850/875, Loss: 0.1443\n    Batch 860/875, Loss: 0.5542\n    Batch 870/875, Loss: 0.3274\n    Batch 875/875, Loss: 0.3868\n    DE Eval 5: LR=0.000135, Dropout=0.12, BS=16\n    First batch loaded, starting training...\n    Batch 10/438, Loss: 1.1928\n    Batch 20/438, Loss: 1.1897\n    Batch 30/438, Loss: 1.5025\n    Batch 40/438, Loss: 1.1021\n    Batch 50/438, Loss: 1.5769\n    Batch 60/438, Loss: 1.0498\n    Batch 70/438, Loss: 1.0791\n    Batch 80/438, Loss: 0.8497\n    Batch 90/438, Loss: 0.5529\n    Batch 100/438, Loss: 0.9420\n    Batch 110/438, Loss: 0.9248\n    Batch 120/438, Loss: 1.3660\n    Batch 130/438, Loss: 1.2665\n    Batch 140/438, Loss: 0.6856\n    Batch 150/438, Loss: 0.6611\n    Batch 160/438, Loss: 0.9690\n    Batch 170/438, Loss: 0.7408\n    Batch 180/438, Loss: 1.0405\n    Batch 190/438, Loss: 0.6178\n    Batch 200/438, Loss: 0.6956\n    Batch 210/438, Loss: 1.0633\n    Batch 220/438, Loss: 0.7764\n    Batch 230/438, Loss: 0.5738\n    Batch 240/438, Loss: 0.7070\n    Batch 250/438, Loss: 0.6443\n    Batch 260/438, Loss: 0.7277\n    Batch 270/438, Loss: 1.2003\n    Batch 280/438, Loss: 0.6832\n    Batch 290/438, Loss: 0.5381\n    Batch 300/438, Loss: 0.6529\n    Batch 310/438, Loss: 0.9234\n    Batch 320/438, Loss: 0.5419\n    Batch 330/438, Loss: 0.5479\n    Batch 340/438, Loss: 0.8364\n    Batch 350/438, Loss: 0.6920\n    Batch 360/438, Loss: 0.5461\n    Batch 370/438, Loss: 1.0613\n    Batch 380/438, Loss: 0.5171\n    Batch 390/438, Loss: 0.7137\n    Batch 400/438, Loss: 0.3951\n    Batch 410/438, Loss: 0.5827\n    Batch 420/438, Loss: 0.5634\n    Batch 430/438, Loss: 0.5188\n    Batch 438/438, Loss: 0.7376\n    DE Eval 6: LR=0.000320, Dropout=0.04, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.8687\n    Batch 20/219, Loss: 1.0325\n    Batch 30/219, Loss: 0.8476\n    Batch 40/219, Loss: 0.9873\n    Batch 50/219, Loss: 1.0229\n    Batch 60/219, Loss: 0.9159\n    Batch 70/219, Loss: 0.7278\n    Batch 80/219, Loss: 0.8684\n    Batch 90/219, Loss: 1.1024\n    Batch 100/219, Loss: 0.6085\n    Batch 110/219, Loss: 0.7082\n    Batch 120/219, Loss: 0.6925\n    Batch 130/219, Loss: 0.5142\n    Batch 140/219, Loss: 0.7839\n    Batch 150/219, Loss: 0.7740\n    Batch 160/219, Loss: 0.7128\n    Batch 170/219, Loss: 0.8087\n    Batch 180/219, Loss: 0.6854\n    Batch 190/219, Loss: 0.6915\n    Batch 200/219, Loss: 0.4068\n    Batch 210/219, Loss: 0.4301\n    Batch 219/219, Loss: 0.9480\n    DE Eval 7: LR=0.000057, Dropout=0.41, BS=8\n    First batch loaded, starting training...\n    Batch 10/875, Loss: 1.3123\n    Batch 20/875, Loss: 1.4806\n    Batch 30/875, Loss: 1.1979\n    Batch 40/875, Loss: 1.5420\n    Batch 50/875, Loss: 1.0271\n    Batch 60/875, Loss: 1.2739\n    Batch 70/875, Loss: 1.0144\n    Batch 80/875, Loss: 1.0232\n    Batch 90/875, Loss: 1.3068\n    Batch 100/875, Loss: 1.1887\n    Batch 110/875, Loss: 0.9643\n    Batch 120/875, Loss: 0.9209\n    Batch 130/875, Loss: 1.4497\n    Batch 140/875, Loss: 1.3241\n    Batch 150/875, Loss: 1.4256\n    Batch 160/875, Loss: 1.0502\n    Batch 170/875, Loss: 1.0408\n    Batch 180/875, Loss: 2.1455\n    Batch 190/875, Loss: 0.9688\n    Batch 200/875, Loss: 0.9701\n    Batch 210/875, Loss: 0.9777\n    Batch 220/875, Loss: 1.3237\n    Batch 230/875, Loss: 0.8600\n    Batch 240/875, Loss: 0.9398\n    Batch 250/875, Loss: 1.8584\n    Batch 260/875, Loss: 1.2948\n    Batch 270/875, Loss: 1.2162\n    Batch 280/875, Loss: 1.0159\n    Batch 290/875, Loss: 0.9390\n    Batch 300/875, Loss: 1.0686\n    Batch 310/875, Loss: 0.6381\n    Batch 320/875, Loss: 1.0787\n    Batch 330/875, Loss: 0.7062\n    Batch 340/875, Loss: 1.2463\n    Batch 350/875, Loss: 0.6889\n    Batch 360/875, Loss: 0.9806\n    Batch 370/875, Loss: 1.0700\n    Batch 380/875, Loss: 1.2194\n    Batch 390/875, Loss: 0.9483\n    Batch 400/875, Loss: 1.1547\n    Batch 410/875, Loss: 1.1692\n    Batch 420/875, Loss: 1.1511\n    Batch 430/875, Loss: 0.6904\n    Batch 440/875, Loss: 1.8834\n    Batch 450/875, Loss: 0.6488\n    Batch 460/875, Loss: 1.9248\n    Batch 470/875, Loss: 0.7627\n    Batch 480/875, Loss: 1.8091\n    Batch 490/875, Loss: 0.6471\n    Batch 500/875, Loss: 1.8664\n    Batch 510/875, Loss: 1.1032\n    Batch 520/875, Loss: 0.5768\n    Batch 530/875, Loss: 0.8641\n    Batch 540/875, Loss: 0.6156\n    Batch 550/875, Loss: 0.7414\n    Batch 560/875, Loss: 0.7545\n    Batch 570/875, Loss: 0.8816\n    Batch 580/875, Loss: 0.7408\n    Batch 590/875, Loss: 1.1889\n    Batch 600/875, Loss: 0.7373\n    Batch 610/875, Loss: 1.8541\n    Batch 620/875, Loss: 1.9346\n    Batch 630/875, Loss: 1.7182\n    Batch 640/875, Loss: 0.6313\n    Batch 650/875, Loss: 1.0239\n    Batch 660/875, Loss: 0.6790\n    Batch 670/875, Loss: 0.6126\n    Batch 680/875, Loss: 1.8429\n    Batch 690/875, Loss: 0.4009\n    Batch 700/875, Loss: 1.9867\n    Batch 710/875, Loss: 1.4416\n    Batch 720/875, Loss: 0.6941\n    Batch 730/875, Loss: 1.6950\n    Batch 740/875, Loss: 1.0659\n    Batch 750/875, Loss: 0.5744\n    Batch 760/875, Loss: 0.7837\n    Batch 770/875, Loss: 1.1686\n    Batch 780/875, Loss: 1.3360\n    Batch 790/875, Loss: 0.6575\n    Batch 800/875, Loss: 0.7621\n    Batch 810/875, Loss: 1.7611\n    Batch 820/875, Loss: 0.5076\n    Batch 830/875, Loss: 0.7761\n    Batch 840/875, Loss: 0.5751\n    Batch 850/875, Loss: 0.5968\n    Batch 860/875, Loss: 1.0589\n    Batch 870/875, Loss: 1.1600\n    Batch 875/875, Loss: 0.7242\n    DE Eval 8: LR=0.000012, Dropout=0.16, BS=8\n    First batch loaded, starting training...\n    Batch 10/875, Loss: 1.4442\n    Batch 20/875, Loss: 1.4266\n    Batch 30/875, Loss: 1.3736\n    Batch 40/875, Loss: 1.4194\n    Batch 50/875, Loss: 1.3119\n    Batch 60/875, Loss: 1.3338\n    Batch 70/875, Loss: 1.4166\n    Batch 80/875, Loss: 1.2561\n    Batch 90/875, Loss: 1.2245\n    Batch 100/875, Loss: 1.3245\n    Batch 110/875, Loss: 1.2357\n    Batch 120/875, Loss: 1.1968\n    Batch 130/875, Loss: 1.2203\n    Batch 140/875, Loss: 1.1971\n    Batch 150/875, Loss: 1.1589\n    Batch 160/875, Loss: 1.2572\n    Batch 170/875, Loss: 1.1904\n    Batch 180/875, Loss: 1.1796\n    Batch 190/875, Loss: 1.2738\n    Batch 200/875, Loss: 1.0878\n    Batch 210/875, Loss: 1.1270\n    Batch 220/875, Loss: 1.2538\n    Batch 230/875, Loss: 1.2883\n    Batch 240/875, Loss: 0.9666\n    Batch 250/875, Loss: 1.1560\n    Batch 260/875, Loss: 1.1567\n    Batch 270/875, Loss: 1.3258\n    Batch 280/875, Loss: 1.6601\n    Batch 290/875, Loss: 1.1609\n    Batch 300/875, Loss: 1.0124\n    Batch 310/875, Loss: 1.0784\n    Batch 320/875, Loss: 1.1356\n    Batch 330/875, Loss: 1.2223\n    Batch 340/875, Loss: 0.9115\n    Batch 350/875, Loss: 0.9217\n    Batch 360/875, Loss: 1.4031\n    Batch 370/875, Loss: 1.2612\n    Batch 380/875, Loss: 0.9866\n    Batch 390/875, Loss: 0.9791\n    Batch 400/875, Loss: 1.7372\n    Batch 410/875, Loss: 1.2336\n    Batch 420/875, Loss: 0.8847\n    Batch 430/875, Loss: 1.0738\n    Batch 440/875, Loss: 0.8137\n    Batch 450/875, Loss: 0.7791\n    Batch 460/875, Loss: 0.8806\n    Batch 470/875, Loss: 1.0391\n    Batch 480/875, Loss: 0.9333\n    Batch 490/875, Loss: 1.8236\n    Batch 500/875, Loss: 1.2942\n    Batch 510/875, Loss: 0.8967\n    Batch 520/875, Loss: 0.8140\n    Batch 530/875, Loss: 0.9299\n    Batch 540/875, Loss: 0.8531\n    Batch 550/875, Loss: 0.7764\n    Batch 560/875, Loss: 0.6628\n    Batch 570/875, Loss: 1.3644\n    Batch 580/875, Loss: 0.6644\n    Batch 590/875, Loss: 0.9554\n    Batch 600/875, Loss: 0.7205\n    Batch 610/875, Loss: 1.1436\n    Batch 620/875, Loss: 1.8885\n    Batch 630/875, Loss: 1.0528\n    Batch 640/875, Loss: 0.7162\n    Batch 650/875, Loss: 1.0627\n    Batch 660/875, Loss: 1.0926\n    Batch 670/875, Loss: 0.8436\n    Batch 680/875, Loss: 1.0444\n    Batch 690/875, Loss: 0.8398\n    Batch 700/875, Loss: 0.7917\n    Batch 710/875, Loss: 1.8335\n    Batch 720/875, Loss: 1.3677\n    Batch 730/875, Loss: 2.0172\n    Batch 740/875, Loss: 1.0632\n    Batch 750/875, Loss: 0.8865\n    Batch 760/875, Loss: 0.6436\n    Batch 770/875, Loss: 0.9229\n    Batch 780/875, Loss: 0.8815\n    Batch 790/875, Loss: 1.5459\n    Batch 800/875, Loss: 1.1848\n    Batch 810/875, Loss: 0.8944\n    Batch 820/875, Loss: 1.4989\n    Batch 830/875, Loss: 0.9736\n    Batch 840/875, Loss: 1.2776\n    Batch 850/875, Loss: 0.9377\n    Batch 860/875, Loss: 0.8142\n    Batch 870/875, Loss: 0.7366\n    Batch 875/875, Loss: 1.1157\n    DE Eval 9: LR=0.000226, Dropout=0.58, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 1.0309\n    Batch 20/219, Loss: 0.8702\n    Batch 30/219, Loss: 0.9252\n    Batch 40/219, Loss: 0.9999\n    Batch 50/219, Loss: 0.9553\n    Batch 60/219, Loss: 0.9024\n    Batch 70/219, Loss: 1.1759\n    Batch 80/219, Loss: 1.0994\n    Batch 90/219, Loss: 1.0604\n    Batch 100/219, Loss: 0.7356\n    Batch 110/219, Loss: 0.9455\n    Batch 120/219, Loss: 1.1511\n    Batch 130/219, Loss: 0.7900\n    Batch 140/219, Loss: 1.0186\n    Batch 150/219, Loss: 0.8651\n    Batch 160/219, Loss: 0.8672\n    Batch 170/219, Loss: 0.6219\n    Batch 180/219, Loss: 0.8104\n    Batch 190/219, Loss: 0.9244\n    Batch 200/219, Loss: 0.7376\n    Batch 210/219, Loss: 0.6291\n    Batch 219/219, Loss: 0.6457\n    DE Eval 10: LR=0.000303, Dropout=0.05, BS=8\n    First batch loaded, starting training...\n    Batch 10/875, Loss: 1.1851\n    Batch 20/875, Loss: 0.5944\n    Batch 30/875, Loss: 0.7856\n    Batch 40/875, Loss: 1.6893\n    Batch 50/875, Loss: 0.6919\n    Batch 60/875, Loss: 0.4463\n    Batch 70/875, Loss: 0.6873\n    Batch 80/875, Loss: 0.5255\n    Batch 90/875, Loss: 0.5684\n    Batch 100/875, Loss: 0.2867\n    Batch 110/875, Loss: 0.2361\n    Batch 120/875, Loss: 0.4412\n    Batch 130/875, Loss: 0.5946\n    Batch 140/875, Loss: 0.9342\n    Batch 150/875, Loss: 0.6712\n    Batch 160/875, Loss: 0.6008\n    Batch 170/875, Loss: 0.3917\n    Batch 180/875, Loss: 0.4511\n    Batch 190/875, Loss: 0.3835\n    Batch 200/875, Loss: 1.2228\n    Batch 210/875, Loss: 0.1915\n    Batch 220/875, Loss: 0.5507\n    Batch 230/875, Loss: 1.5091\n    Batch 240/875, Loss: 0.8540\n    Batch 250/875, Loss: 1.0240\n    Batch 260/875, Loss: 0.2494\n    Batch 270/875, Loss: 0.4450\n    Batch 280/875, Loss: 0.4306\n    Batch 290/875, Loss: 0.2824\n    Batch 300/875, Loss: 1.6543\n    Batch 310/875, Loss: 0.4846\n    Batch 320/875, Loss: 0.3300\n    Batch 330/875, Loss: 0.4273\n    Batch 340/875, Loss: 0.1299\n    Batch 350/875, Loss: 0.7895\n    Batch 360/875, Loss: 0.2399\n    Batch 370/875, Loss: 1.3424\n    Batch 380/875, Loss: 0.4437\n    Batch 390/875, Loss: 1.4099\n    Batch 400/875, Loss: 0.4787\n    Batch 410/875, Loss: 0.8130\n    Batch 420/875, Loss: 0.1461\n    Batch 430/875, Loss: 1.7174\n    Batch 440/875, Loss: 0.8206\n    Batch 450/875, Loss: 0.2065\n    Batch 460/875, Loss: 0.9265\n    Batch 470/875, Loss: 0.4573\n    Batch 480/875, Loss: 0.3487\n    Batch 490/875, Loss: 0.1896\n    Batch 500/875, Loss: 0.1186\n    Batch 510/875, Loss: 0.5705\n    Batch 520/875, Loss: 0.1671\n    Batch 530/875, Loss: 2.7721\n    Batch 540/875, Loss: 0.2585\n    Batch 550/875, Loss: 0.2986\n    Batch 560/875, Loss: 0.4408\n    Batch 570/875, Loss: 1.8383\n    Batch 580/875, Loss: 0.5322\n    Batch 590/875, Loss: 1.2621\n    Batch 600/875, Loss: 0.8941\n    Batch 610/875, Loss: 0.3408\n    Batch 620/875, Loss: 1.3566\n    Batch 630/875, Loss: 0.1427\n    Batch 640/875, Loss: 0.4105\n    Batch 650/875, Loss: 0.1692\n    Batch 660/875, Loss: 0.4619\n    Batch 670/875, Loss: 1.3598\n    Batch 680/875, Loss: 0.5261\n    Batch 690/875, Loss: 1.4785\n    Batch 700/875, Loss: 0.2625\n    Batch 710/875, Loss: 0.1552\n    Batch 720/875, Loss: 0.1190\n    Batch 730/875, Loss: 0.3037\n    Batch 740/875, Loss: 0.3016\n    Batch 750/875, Loss: 0.2250\n    Batch 760/875, Loss: 0.5784\n    Batch 770/875, Loss: 0.7742\n    Batch 780/875, Loss: 0.6489\n    Batch 790/875, Loss: 0.3098\n    Batch 800/875, Loss: 0.3056\n    Batch 810/875, Loss: 0.1836\n    Batch 820/875, Loss: 0.4723\n    Batch 830/875, Loss: 0.3221\n    Batch 840/875, Loss: 0.8027\n    Batch 850/875, Loss: 0.2661\n    Batch 860/875, Loss: 0.4519\n    Batch 870/875, Loss: 0.1604\n    Batch 875/875, Loss: 0.2677\n    DE Eval 11: LR=0.000092, Dropout=0.50, BS=8\n    First batch loaded, starting training...\n    Batch 10/875, Loss: 1.3740\n    Batch 20/875, Loss: 1.2776\n    Batch 30/875, Loss: 1.2701\n    Batch 40/875, Loss: 1.1558\n    Batch 50/875, Loss: 1.0426\n    Batch 60/875, Loss: 0.9769\n    Batch 70/875, Loss: 1.2222\n    Batch 80/875, Loss: 0.8501\n    Batch 90/875, Loss: 1.2840\n    Batch 100/875, Loss: 0.7798\n    Batch 110/875, Loss: 0.9638\n    Batch 120/875, Loss: 1.7839\n    Batch 130/875, Loss: 0.7909\n    Batch 140/875, Loss: 0.7294\n    Batch 150/875, Loss: 0.9700\n    Batch 160/875, Loss: 1.0977\n    Batch 170/875, Loss: 0.9785\n    Batch 180/875, Loss: 0.7954\n    Batch 190/875, Loss: 1.3705\n    Batch 200/875, Loss: 1.2169\n    Batch 210/875, Loss: 0.6633\n    Batch 220/875, Loss: 0.8352\n    Batch 230/875, Loss: 1.4921\n    Batch 240/875, Loss: 0.7126\n    Batch 250/875, Loss: 0.9036\n    Batch 260/875, Loss: 0.8470\n    Batch 270/875, Loss: 1.0576\n    Batch 280/875, Loss: 0.5828\n    Batch 290/875, Loss: 0.6211\n    Batch 300/875, Loss: 0.8248\n    Batch 310/875, Loss: 0.8638\n    Batch 320/875, Loss: 0.9735\n    Batch 330/875, Loss: 1.0976\n    Batch 340/875, Loss: 0.6500\n    Batch 350/875, Loss: 1.3663\n    Batch 360/875, Loss: 1.0575\n    Batch 370/875, Loss: 1.3419\n    Batch 380/875, Loss: 0.7873\n    Batch 390/875, Loss: 1.7532\n    Batch 400/875, Loss: 1.2585\n    Batch 410/875, Loss: 0.8524\n    Batch 420/875, Loss: 0.8539\n    Batch 430/875, Loss: 0.8346\n    Batch 440/875, Loss: 1.1611\n    Batch 450/875, Loss: 0.8691\n    Batch 460/875, Loss: 0.9165\n    Batch 470/875, Loss: 0.6327\n    Batch 480/875, Loss: 2.0632\n    Batch 490/875, Loss: 1.0721\n    Batch 500/875, Loss: 0.9900\n    Batch 510/875, Loss: 0.8940\n    Batch 520/875, Loss: 0.6696\n    Batch 530/875, Loss: 0.9196\n    Batch 540/875, Loss: 0.8439\n    Batch 550/875, Loss: 1.2153\n    Batch 560/875, Loss: 1.1562\n    Batch 570/875, Loss: 0.9792\n    Batch 580/875, Loss: 1.0034\n    Batch 590/875, Loss: 1.8209\n    Batch 600/875, Loss: 0.7690\n    Batch 610/875, Loss: 0.7030\n    Batch 620/875, Loss: 0.5208\n    Batch 630/875, Loss: 1.4902\n    Batch 640/875, Loss: 0.5449\n    Batch 650/875, Loss: 0.4656\n    Batch 660/875, Loss: 0.5194\n    Batch 670/875, Loss: 1.0279\n    Batch 680/875, Loss: 0.8060\n    Batch 690/875, Loss: 1.4597\n    Batch 700/875, Loss: 1.7561\n    Batch 710/875, Loss: 0.7926\n    Batch 720/875, Loss: 1.5254\n    Batch 730/875, Loss: 0.7732\n    Batch 740/875, Loss: 0.7865\n    Batch 750/875, Loss: 0.9620\n    Batch 760/875, Loss: 0.7501\n    Batch 770/875, Loss: 0.5422\n    Batch 780/875, Loss: 0.6680\n    Batch 790/875, Loss: 1.0479\n    Batch 800/875, Loss: 0.6515\n    Batch 810/875, Loss: 1.8479\n    Batch 820/875, Loss: 0.8528\n    Batch 830/875, Loss: 1.8347\n    Batch 840/875, Loss: 0.7263\n    Batch 850/875, Loss: 1.2222\n    Batch 860/875, Loss: 0.4710\n    Batch 870/875, Loss: 0.9364\n    Batch 875/875, Loss: 1.0490\n    DE Eval 12: LR=0.000022, Dropout=0.04, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 1.3367\n    Batch 20/219, Loss: 1.3059\n    Batch 30/219, Loss: 1.2031\n    Batch 40/219, Loss: 1.2207\n    Batch 50/219, Loss: 1.1031\n    Batch 60/219, Loss: 1.0715\n    Batch 70/219, Loss: 1.0570\n    Batch 80/219, Loss: 0.9417\n    Batch 90/219, Loss: 1.2150\n    Batch 100/219, Loss: 1.0012\n    Batch 110/219, Loss: 1.2504\n    Batch 120/219, Loss: 1.0255\n    Batch 130/219, Loss: 1.3783\n    Batch 140/219, Loss: 0.8756\n    Batch 150/219, Loss: 0.8967\n    Batch 160/219, Loss: 0.9043\n    Batch 170/219, Loss: 1.1687\n    Batch 180/219, Loss: 0.9043\n    Batch 190/219, Loss: 0.6885\n    Batch 200/219, Loss: 0.8758\n    Batch 210/219, Loss: 0.7442\n    Batch 219/219, Loss: 0.7263\n    DE Eval 13: LR=0.000739, Dropout=0.50, BS=8\n    First batch loaded, starting training...\n    Batch 10/875, Loss: 1.9216\n    Batch 20/875, Loss: 1.0544\n    Batch 30/875, Loss: 0.8995\n    Batch 40/875, Loss: 0.5125\n    Batch 50/875, Loss: 0.9052\n    Batch 60/875, Loss: 0.5545\n    Batch 70/875, Loss: 0.5608\n    Batch 80/875, Loss: 0.5271\n    Batch 90/875, Loss: 1.1499\n    Batch 100/875, Loss: 0.6471\n    Batch 110/875, Loss: 0.4871\n    Batch 120/875, Loss: 0.7590\n    Batch 130/875, Loss: 0.3000\n    Batch 140/875, Loss: 0.6693\n    Batch 150/875, Loss: 0.4216\n    Batch 160/875, Loss: 0.3514\n    Batch 170/875, Loss: 0.8424\n    Batch 180/875, Loss: 0.3602\n    Batch 190/875, Loss: 0.3406\n    Batch 200/875, Loss: 0.2237\n    Batch 210/875, Loss: 0.6507\n    Batch 220/875, Loss: 0.6152\n    Batch 230/875, Loss: 0.2488\n    Batch 240/875, Loss: 0.4737\n    Batch 250/875, Loss: 0.2563\n    Batch 260/875, Loss: 0.2667\n    Batch 270/875, Loss: 0.5908\n    Batch 280/875, Loss: 1.7076\n    Batch 290/875, Loss: 1.1511\n    Batch 300/875, Loss: 0.6981\n    Batch 310/875, Loss: 0.3799\n    Batch 320/875, Loss: 0.4425\n    Batch 330/875, Loss: 0.6118\n    Batch 340/875, Loss: 0.3467\n    Batch 350/875, Loss: 0.2709\n    Batch 360/875, Loss: 0.4241\n    Batch 370/875, Loss: 1.0908\n    Batch 380/875, Loss: 0.3815\n    Batch 390/875, Loss: 0.2204\n    Batch 400/875, Loss: 0.4240\n    Batch 410/875, Loss: 0.1982\n    Batch 420/875, Loss: 0.7600\n    Batch 430/875, Loss: 0.8524\n    Batch 440/875, Loss: 1.3729\n    Batch 450/875, Loss: 1.3306\n    Batch 460/875, Loss: 0.2546\n    Batch 470/875, Loss: 0.4373\n    Batch 480/875, Loss: 0.3825\n    Batch 490/875, Loss: 0.2626\n    Batch 500/875, Loss: 0.2495\n    Batch 510/875, Loss: 0.2463\n    Batch 520/875, Loss: 0.1539\n    Batch 530/875, Loss: 0.1570\n    Batch 540/875, Loss: 0.3796\n    Batch 550/875, Loss: 0.1463\n    Batch 560/875, Loss: 0.2725\n    Batch 570/875, Loss: 0.4899\n    Batch 580/875, Loss: 1.3997\n    Batch 590/875, Loss: 1.1767\n    Batch 600/875, Loss: 0.4253\n    Batch 610/875, Loss: 0.3598\n    Batch 620/875, Loss: 0.2959\n    Batch 630/875, Loss: 0.4426\n    Batch 640/875, Loss: 0.8494\n    Batch 650/875, Loss: 0.2158\n    Batch 660/875, Loss: 1.0371\n    Batch 670/875, Loss: 0.5952\n    Batch 680/875, Loss: 1.1808\n    Batch 690/875, Loss: 1.2278\n    Batch 700/875, Loss: 0.2714\n    Batch 710/875, Loss: 0.1689\n    Batch 720/875, Loss: 0.5300\n    Batch 730/875, Loss: 0.4000\n    Batch 740/875, Loss: 0.0531\n    Batch 750/875, Loss: 0.2003\n    Batch 760/875, Loss: 1.0227\n    Batch 770/875, Loss: 0.3875\n    Batch 780/875, Loss: 0.2806\n    Batch 790/875, Loss: 0.2370\n    Batch 800/875, Loss: 0.5305\n    Batch 810/875, Loss: 0.1188\n    Batch 820/875, Loss: 0.0958\n    Batch 830/875, Loss: 0.1942\n    Batch 840/875, Loss: 1.3652\n    Batch 850/875, Loss: 0.2465\n    Batch 860/875, Loss: 1.9481\n    Batch 870/875, Loss: 0.9225\n    Batch 875/875, Loss: 0.6437\n    DE Eval 14: LR=0.000018, Dropout=0.00, BS=16\n    First batch loaded, starting training...\n    Batch 10/438, Loss: 1.2537\n    Batch 20/438, Loss: 1.2285\n    Batch 30/438, Loss: 1.2666\n    Batch 40/438, Loss: 1.2288\n    Batch 50/438, Loss: 1.1182\n    Batch 60/438, Loss: 1.1335\n    Batch 70/438, Loss: 1.1480\n    Batch 80/438, Loss: 1.2067\n    Batch 90/438, Loss: 0.9912\n    Batch 100/438, Loss: 1.1978\n    Batch 110/438, Loss: 0.9938\n    Batch 120/438, Loss: 1.2683\n    Batch 130/438, Loss: 1.2961\n    Batch 140/438, Loss: 1.0108\n    Batch 150/438, Loss: 1.0294\n    Batch 160/438, Loss: 1.0484\n    Batch 170/438, Loss: 0.9501\n    Batch 180/438, Loss: 0.9935\n    Batch 190/438, Loss: 0.8693\n    Batch 200/438, Loss: 0.9274\n    Batch 210/438, Loss: 1.5031\n    Batch 220/438, Loss: 1.0932\n    Batch 230/438, Loss: 1.2870\n    Batch 240/438, Loss: 0.8179\n    Batch 250/438, Loss: 0.8578\n    Batch 260/438, Loss: 1.0152\n    Batch 270/438, Loss: 1.9584\n    Batch 280/438, Loss: 0.7325\n    Batch 290/438, Loss: 1.6329\n    Batch 300/438, Loss: 1.2029\n    Batch 310/438, Loss: 0.9263\n    Batch 320/438, Loss: 1.1881\n    Batch 330/438, Loss: 0.9646\n    Batch 340/438, Loss: 1.3446\n    Batch 350/438, Loss: 1.9943\n    Batch 360/438, Loss: 1.1586\n    Batch 370/438, Loss: 1.0352\n    Batch 380/438, Loss: 1.2229\n    Batch 390/438, Loss: 0.8976\n    Batch 400/438, Loss: 1.1428\n    Batch 410/438, Loss: 0.6990\n    Batch 420/438, Loss: 0.8016\n    Batch 430/438, Loss: 0.8014\n    Batch 438/438, Loss: 1.1168\n    DE Eval 15: LR=0.000038, Dropout=0.36, BS=16\n    First batch loaded, starting training...\n    Batch 10/438, Loss: 1.3639\n    Batch 20/438, Loss: 1.3205\n    Batch 30/438, Loss: 1.2428\n    Batch 40/438, Loss: 1.1641\n    Batch 50/438, Loss: 1.2117\n    Batch 60/438, Loss: 1.0843\n    Batch 70/438, Loss: 1.1189\n    Batch 80/438, Loss: 1.2626\n    Batch 90/438, Loss: 1.1582\n    Batch 100/438, Loss: 0.9685\n    Batch 110/438, Loss: 1.0034\n    Batch 120/438, Loss: 1.0726\n    Batch 130/438, Loss: 1.0372\n    Batch 140/438, Loss: 1.2183\n    Batch 150/438, Loss: 1.1911\n    Batch 160/438, Loss: 1.0684\n    Batch 170/438, Loss: 1.3119\n    Batch 180/438, Loss: 1.2390\n    Batch 190/438, Loss: 0.9610\n    Batch 200/438, Loss: 1.3054\n    Batch 210/438, Loss: 0.7359\n    Batch 220/438, Loss: 1.5448\n    Batch 230/438, Loss: 0.9181\n    Batch 240/438, Loss: 1.2296\n    Batch 250/438, Loss: 0.7521\n    Batch 260/438, Loss: 0.5901\n    Batch 270/438, Loss: 0.7762\n    Batch 280/438, Loss: 0.8207\n    Batch 290/438, Loss: 0.7621\n    Batch 300/438, Loss: 0.9032\n    Batch 310/438, Loss: 0.7878\n    Batch 320/438, Loss: 1.0737\n    Batch 330/438, Loss: 0.8319\n    Batch 340/438, Loss: 0.6317\n    Batch 350/438, Loss: 0.9640\n    Batch 360/438, Loss: 0.7548\n    Batch 370/438, Loss: 0.7826\n    Batch 380/438, Loss: 0.5540\n    Batch 390/438, Loss: 0.9480\n    Batch 400/438, Loss: 1.0233\n    Batch 410/438, Loss: 0.8999\n    Batch 420/438, Loss: 0.6207\n    Batch 430/438, Loss: 0.9286\n    Batch 438/438, Loss: 2.0928\n    DE Eval 16: LR=0.000010, Dropout=0.30, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 1.4115\n    Batch 20/219, Loss: 1.2437\n    Batch 30/219, Loss: 1.1952\n    Batch 40/219, Loss: 1.1901\n    Batch 50/219, Loss: 1.1773\n    Batch 60/219, Loss: 1.1773\n    Batch 70/219, Loss: 1.1824\n    Batch 80/219, Loss: 1.0425\n    Batch 90/219, Loss: 0.9710\n    Batch 100/219, Loss: 1.1140\n    Batch 110/219, Loss: 1.0182\n    Batch 120/219, Loss: 0.9840\n    Batch 130/219, Loss: 0.9979\n    Batch 140/219, Loss: 1.2202\n    Batch 150/219, Loss: 0.7028\n    Batch 160/219, Loss: 1.1018\n    Batch 170/219, Loss: 0.9592\n    Batch 180/219, Loss: 0.8125\n    Batch 190/219, Loss: 0.5771\n    Batch 200/219, Loss: 0.6919\n    Batch 210/219, Loss: 0.8368\n    Batch 219/219, Loss: 1.0620\n    DE Eval 17: LR=0.000127, Dropout=0.15, BS=16\n    First batch loaded, starting training...\n    Batch 10/438, Loss: 1.2491\n    Batch 20/438, Loss: 1.1419\n    Batch 30/438, Loss: 1.2013\n    Batch 40/438, Loss: 0.7492\n    Batch 50/438, Loss: 0.8611\n    Batch 60/438, Loss: 0.9737\n    Batch 70/438, Loss: 1.1311\n    Batch 80/438, Loss: 1.1471\n    Batch 90/438, Loss: 0.7905\n    Batch 100/438, Loss: 0.8435\n    Batch 110/438, Loss: 1.0232\n    Batch 120/438, Loss: 0.7081\n    Batch 130/438, Loss: 0.8251\n    Batch 140/438, Loss: 0.6411\n    Batch 150/438, Loss: 0.8452\n    Batch 160/438, Loss: 0.8671\n    Batch 170/438, Loss: 1.1324\n    Batch 180/438, Loss: 0.9583\n    Batch 190/438, Loss: 0.8202\n    Batch 200/438, Loss: 0.7616\n    Batch 210/438, Loss: 0.8514\n    Batch 220/438, Loss: 0.6942\n    Batch 230/438, Loss: 0.9480\n    Batch 240/438, Loss: 0.7103\n    Batch 250/438, Loss: 0.9160\n    Batch 260/438, Loss: 0.8191\n    Batch 270/438, Loss: 0.9224\n    Batch 280/438, Loss: 0.7675\n    Batch 290/438, Loss: 1.2127\n    Batch 300/438, Loss: 1.4647\n    Batch 310/438, Loss: 0.8930\n    Batch 320/438, Loss: 0.8405\n    Batch 330/438, Loss: 1.0829\n    Batch 340/438, Loss: 1.0072\n    Batch 350/438, Loss: 0.9714\n    Batch 360/438, Loss: 0.6319\n    Batch 370/438, Loss: 0.6162\n    Batch 380/438, Loss: 1.1244\n    Batch 390/438, Loss: 0.4664\n    Batch 400/438, Loss: 0.6577\n    Batch 410/438, Loss: 0.8147\n    Batch 420/438, Loss: 0.9519\n    Batch 430/438, Loss: 0.5629\n    Batch 438/438, Loss: 0.7569\n    DE Eval 18: LR=0.000047, Dropout=0.46, BS=8\n    First batch loaded, starting training...\n    Batch 10/875, Loss: 1.4138\n    Batch 20/875, Loss: 1.3231\n    Batch 30/875, Loss: 1.2643\n    Batch 40/875, Loss: 1.3013\n    Batch 50/875, Loss: 1.1535\n    Batch 60/875, Loss: 1.1814\n    Batch 70/875, Loss: 0.9268\n    Batch 80/875, Loss: 0.7065\n    Batch 90/875, Loss: 0.7039\n    Batch 100/875, Loss: 1.2967\n    Batch 110/875, Loss: 0.7235\n    Batch 120/875, Loss: 0.7316\n    Batch 130/875, Loss: 0.6728\n    Batch 140/875, Loss: 0.6917\n    Batch 150/875, Loss: 1.9243\n    Batch 160/875, Loss: 0.5045\n    Batch 170/875, Loss: 0.6282\n    Batch 180/875, Loss: 1.8865\n    Batch 190/875, Loss: 0.5522\n    Batch 200/875, Loss: 0.4216\n    Batch 210/875, Loss: 0.6540\n    Batch 220/875, Loss: 0.6231\n    Batch 230/875, Loss: 1.0324\n    Batch 240/875, Loss: 0.6182\n    Batch 250/875, Loss: 0.4107\n    Batch 260/875, Loss: 0.7435\n    Batch 270/875, Loss: 0.6151\n    Batch 280/875, Loss: 0.5981\n    Batch 290/875, Loss: 0.5042\n    Batch 300/875, Loss: 2.1073\n    Batch 310/875, Loss: 0.8473\n    Batch 320/875, Loss: 0.4776\n    Batch 330/875, Loss: 0.4141\n    Batch 340/875, Loss: 0.4746\n    Batch 350/875, Loss: 0.3802\n    Batch 360/875, Loss: 0.2782\n    Batch 370/875, Loss: 0.3309\n    Batch 380/875, Loss: 0.4967\n    Batch 390/875, Loss: 0.4479\n    Batch 400/875, Loss: 0.7284\n    Batch 410/875, Loss: 0.7186\n    Batch 420/875, Loss: 0.9146\n    Batch 430/875, Loss: 0.7675\n    Batch 440/875, Loss: 0.6968\n    Batch 450/875, Loss: 1.7286\n    Batch 460/875, Loss: 0.7182\n    Batch 470/875, Loss: 0.6744\n    Batch 480/875, Loss: 1.9926\n    Batch 490/875, Loss: 0.5817\n    Batch 500/875, Loss: 1.6514\n    Batch 510/875, Loss: 0.6265\n    Batch 520/875, Loss: 0.9332\n    Batch 530/875, Loss: 0.3667\n    Batch 540/875, Loss: 0.4332\n    Batch 550/875, Loss: 0.3314\n    Batch 560/875, Loss: 0.5993\n    Batch 570/875, Loss: 0.7363\n    Batch 580/875, Loss: 0.3722\n    Batch 590/875, Loss: 0.7987\n    Batch 600/875, Loss: 1.5702\n    Batch 610/875, Loss: 0.2295\n    Batch 620/875, Loss: 0.8185\n    Batch 630/875, Loss: 0.4227\n    Batch 640/875, Loss: 0.4268\n    Batch 650/875, Loss: 0.6096\n    Batch 660/875, Loss: 1.4667\n    Batch 670/875, Loss: 0.4823\n    Batch 680/875, Loss: 0.7984\n    Batch 690/875, Loss: 0.6323\n    Batch 700/875, Loss: 0.4915\n    Batch 710/875, Loss: 0.7484\n    Batch 720/875, Loss: 0.5418\n    Batch 730/875, Loss: 0.3739\n    Batch 740/875, Loss: 0.4004\n    Batch 750/875, Loss: 0.6106\n    Batch 760/875, Loss: 0.3627\n    Batch 770/875, Loss: 0.3414\n    Batch 780/875, Loss: 0.2980\n    Batch 790/875, Loss: 0.7555\n    Batch 800/875, Loss: 0.3033\n    Batch 810/875, Loss: 0.2631\n    Batch 820/875, Loss: 0.3596\n    Batch 830/875, Loss: 1.6229\n    Batch 840/875, Loss: 0.6841\n    Batch 850/875, Loss: 1.4445\n    Batch 860/875, Loss: 0.5128\n    Batch 870/875, Loss: 0.2604\n    Batch 875/875, Loss: 0.2847\n    DE Eval 19: LR=0.000024, Dropout=0.19, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 1.2963\n    Batch 20/219, Loss: 1.2890\n    Batch 30/219, Loss: 1.1354\n    Batch 40/219, Loss: 1.1740\n    Batch 50/219, Loss: 1.1353\n    Batch 60/219, Loss: 1.1519\n    Batch 70/219, Loss: 1.0388\n    Batch 80/219, Loss: 1.0661\n    Batch 90/219, Loss: 0.9518\n    Batch 100/219, Loss: 1.2004\n    Batch 110/219, Loss: 1.0672\n    Batch 120/219, Loss: 0.9176\n    Batch 130/219, Loss: 0.9900\n    Batch 140/219, Loss: 0.9243\n    Batch 150/219, Loss: 0.9206\n    Batch 160/219, Loss: 0.8972\n    Batch 170/219, Loss: 0.9513\n    Batch 180/219, Loss: 0.9461\n    Batch 190/219, Loss: 1.0197\n    Batch 200/219, Loss: 0.7761\n    Batch 210/219, Loss: 1.1505\n    Batch 219/219, Loss: 1.3883\n    DE Eval 20: LR=0.001004, Dropout=0.20, BS=8\n    First batch loaded, starting training...\n    Batch 10/875, Loss: 1.1043\n    Batch 20/875, Loss: 2.1720\n    Batch 30/875, Loss: 0.7502\n    Batch 40/875, Loss: 0.6657\n    Batch 50/875, Loss: 1.6115\n    Batch 60/875, Loss: 0.7402\n    Batch 70/875, Loss: 0.4877\n    Batch 80/875, Loss: 0.4472\n    Batch 90/875, Loss: 0.5876\n    Batch 100/875, Loss: 0.6039\n    Batch 110/875, Loss: 1.1884\n    Batch 120/875, Loss: 0.6760\n    Batch 130/875, Loss: 0.8102\n    Batch 140/875, Loss: 1.3774\n    Batch 150/875, Loss: 0.5993\n    Batch 160/875, Loss: 0.4556\n    Batch 170/875, Loss: 0.6620\n    Batch 180/875, Loss: 0.4355\n    Batch 190/875, Loss: 0.2776\n    Batch 200/875, Loss: 1.0766\n    Batch 210/875, Loss: 1.4695\n    Batch 220/875, Loss: 1.0706\n    Batch 230/875, Loss: 0.3257\n    Batch 240/875, Loss: 1.1565\n    Batch 250/875, Loss: 0.1406\n    Batch 260/875, Loss: 0.2005\n    Batch 270/875, Loss: 0.6157\n    Batch 280/875, Loss: 0.6645\n    Batch 290/875, Loss: 0.3422\n    Batch 300/875, Loss: 0.4109\n    Batch 310/875, Loss: 0.3031\n    Batch 320/875, Loss: 0.5404\n    Batch 330/875, Loss: 0.4714\n    Batch 340/875, Loss: 1.2994\n    Batch 350/875, Loss: 0.2548\n    Batch 360/875, Loss: 0.7044\n    Batch 370/875, Loss: 0.4932\n    Batch 380/875, Loss: 2.1881\n    Batch 390/875, Loss: 0.5685\n    Batch 400/875, Loss: 0.2780\n    Batch 410/875, Loss: 0.3568\n    Batch 420/875, Loss: 1.5965\n    Batch 430/875, Loss: 0.7287\n    Batch 440/875, Loss: 0.6822\n    Batch 450/875, Loss: 0.1503\n    Batch 460/875, Loss: 0.3151\n    Batch 470/875, Loss: 0.3550\n    Batch 480/875, Loss: 0.4396\n    Batch 490/875, Loss: 0.2250\n    Batch 500/875, Loss: 0.4568\n    Batch 510/875, Loss: 0.2827\n    Batch 520/875, Loss: 0.4235\n    Batch 530/875, Loss: 0.1375\n    Batch 540/875, Loss: 0.3365\n    Batch 550/875, Loss: 0.3127\n    Batch 560/875, Loss: 0.4927\n    Batch 570/875, Loss: 0.4350\n    Batch 580/875, Loss: 0.7695\n    Batch 590/875, Loss: 0.6663\n    Batch 600/875, Loss: 0.4586\n    Batch 610/875, Loss: 0.1900\n    Batch 620/875, Loss: 0.5001\n    Batch 630/875, Loss: 0.4643\n    Batch 640/875, Loss: 0.8804\n    Batch 650/875, Loss: 0.2055\n    Batch 660/875, Loss: 0.6540\n    Batch 670/875, Loss: 0.3444\n    Batch 680/875, Loss: 0.6745\n    Batch 690/875, Loss: 0.4169\n    Batch 700/875, Loss: 0.4263\n    Batch 710/875, Loss: 0.2359\n    Batch 720/875, Loss: 0.3311\n    Batch 730/875, Loss: 0.3717\n    Batch 740/875, Loss: 0.2379\n    Batch 750/875, Loss: 0.2684\n    Batch 760/875, Loss: 0.3288\n    Batch 770/875, Loss: 0.4019\n    Batch 780/875, Loss: 0.6544\n    Batch 790/875, Loss: 0.3735\n    Batch 800/875, Loss: 0.4739\n    Batch 810/875, Loss: 0.5280\n    Batch 820/875, Loss: 0.4053\n    Batch 830/875, Loss: 0.6514\n    Batch 840/875, Loss: 0.4463\n    Batch 850/875, Loss: 0.3349\n    Batch 860/875, Loss: 0.6949\n    Batch 870/875, Loss: 0.6555\n    Batch 875/875, Loss: 0.1348\n    DE Eval 21: LR=0.000085, Dropout=0.58, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 1.3452\n    Batch 20/219, Loss: 1.2322\n    Batch 30/219, Loss: 1.2779\n    Batch 40/219, Loss: 1.2566\n    Batch 50/219, Loss: 1.0689\n    Batch 60/219, Loss: 0.9385\n    Batch 70/219, Loss: 0.9559\n    Batch 80/219, Loss: 0.9055\n    Batch 90/219, Loss: 0.6718\n    Batch 100/219, Loss: 0.9519\n    Batch 110/219, Loss: 0.7811\n    Batch 120/219, Loss: 0.9709\n    Batch 130/219, Loss: 0.8035\n    Batch 140/219, Loss: 1.0292\n    Batch 150/219, Loss: 0.7168\n    Batch 160/219, Loss: 0.8557\n    Batch 170/219, Loss: 0.9328\n    Batch 180/219, Loss: 1.0092\n    Batch 190/219, Loss: 0.9595\n    Batch 200/219, Loss: 0.7080\n    Batch 210/219, Loss: 0.8562\n    Batch 219/219, Loss: 0.9137\n    DE Eval 22: LR=0.000166, Dropout=0.05, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 1.0836\n    Batch 20/219, Loss: 1.0398\n    Batch 30/219, Loss: 0.9208\n    Batch 40/219, Loss: 0.8737\n    Batch 50/219, Loss: 1.0127\n    Batch 60/219, Loss: 0.9025\n    Batch 70/219, Loss: 0.8998\n    Batch 80/219, Loss: 0.7372\n    Batch 90/219, Loss: 0.9287\n    Batch 100/219, Loss: 0.8068\n    Batch 110/219, Loss: 0.7405\n    Batch 120/219, Loss: 0.6341\n    Batch 130/219, Loss: 0.6781\n    Batch 140/219, Loss: 0.8495\n    Batch 150/219, Loss: 0.7453\n    Batch 160/219, Loss: 0.7368\n    Batch 170/219, Loss: 0.7218\n    Batch 180/219, Loss: 0.5834\n    Batch 190/219, Loss: 0.6036\n    Batch 200/219, Loss: 0.6940\n    Batch 210/219, Loss: 0.6257\n    Batch 219/219, Loss: 0.6322\n    DE Eval 23: LR=0.001696, Dropout=0.37, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 1.0984\n    Batch 20/219, Loss: 1.0574\n    Batch 30/219, Loss: 1.2309\n    Batch 40/219, Loss: 1.2275\n    Batch 50/219, Loss: 0.6483\n    Batch 60/219, Loss: 1.0737\n    Batch 70/219, Loss: 0.8125\n    Batch 80/219, Loss: 0.6428\n    Batch 90/219, Loss: 0.7950\n    Batch 100/219, Loss: 0.8033\n    Batch 110/219, Loss: 0.4259\n    Batch 120/219, Loss: 0.5446\n    Batch 130/219, Loss: 0.5264\n    Batch 140/219, Loss: 1.3379\n    Batch 150/219, Loss: 0.6295\n    Batch 160/219, Loss: 0.8596\n    Batch 170/219, Loss: 0.7431\n    Batch 180/219, Loss: 0.9838\n    Batch 190/219, Loss: 0.8051\n    Batch 200/219, Loss: 0.9412\n    Batch 210/219, Loss: 0.4186\n    Batch 219/219, Loss: 0.6012\n    DE Eval 24: LR=0.000481, Dropout=0.42, BS=16\n    First batch loaded, starting training...\n    Batch 10/438, Loss: 1.1599\n    Batch 20/438, Loss: 0.7184\n    Batch 30/438, Loss: 1.5587\n    Batch 40/438, Loss: 0.9610\n    Batch 50/438, Loss: 0.8500\n    Batch 60/438, Loss: 1.0521\n    Batch 70/438, Loss: 0.9828\n    Batch 80/438, Loss: 0.6663\n    Batch 90/438, Loss: 0.7546\n    Batch 100/438, Loss: 0.7838\n    Batch 110/438, Loss: 0.8651\n    Batch 120/438, Loss: 0.6498\n    Batch 130/438, Loss: 0.7883\n    Batch 140/438, Loss: 0.7899\n    Batch 150/438, Loss: 0.6913\n    Batch 160/438, Loss: 0.7362\n    Batch 170/438, Loss: 0.7101\n    Batch 180/438, Loss: 0.4032\n    Batch 190/438, Loss: 0.8341\n    Batch 200/438, Loss: 0.8667\n    Batch 210/438, Loss: 0.8198\n    Batch 220/438, Loss: 1.3493\n    Batch 230/438, Loss: 0.6917\n    Batch 240/438, Loss: 0.9286\n    Batch 250/438, Loss: 0.6048\n    Batch 260/438, Loss: 0.5479\n    Batch 270/438, Loss: 1.3893\n    Batch 280/438, Loss: 0.8743\n    Batch 290/438, Loss: 0.4951\n    Batch 300/438, Loss: 0.5551\n    Batch 310/438, Loss: 0.6422\n    Batch 320/438, Loss: 0.9656\n    Batch 330/438, Loss: 0.6199\n    Batch 340/438, Loss: 0.6725\n    Batch 350/438, Loss: 0.6962\n    Batch 360/438, Loss: 1.1826\n    Batch 370/438, Loss: 0.5694\n    Batch 380/438, Loss: 0.3896\n    Batch 390/438, Loss: 0.7289\n    Batch 400/438, Loss: 0.7982\n    Batch 410/438, Loss: 0.5543\n    Batch 420/438, Loss: 0.9109\n    Batch 430/438, Loss: 0.5061\n    Batch 438/438, Loss: 1.0148\n  DE Iter 1/4: Best = 0.7894\n    DE Eval 25: LR=0.000156, Dropout=0.08, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 1.1361\n    Batch 20/219, Loss: 1.1731\n    Batch 30/219, Loss: 0.8372\n    Batch 40/219, Loss: 0.8940\n    Batch 50/219, Loss: 0.7675\n    Batch 60/219, Loss: 0.6582\n    Batch 70/219, Loss: 1.0361\n    Batch 80/219, Loss: 0.9168\n    Batch 90/219, Loss: 0.8753\n    Batch 100/219, Loss: 0.7094\n    Batch 110/219, Loss: 0.8887\n    Batch 120/219, Loss: 0.8779\n    Batch 130/219, Loss: 0.8979\n    Batch 140/219, Loss: 0.7383\n    Batch 150/219, Loss: 0.7049\n    Batch 160/219, Loss: 0.8183\n    Batch 170/219, Loss: 1.1225\n    Batch 180/219, Loss: 0.6564\n    Batch 190/219, Loss: 0.6800\n    Batch 200/219, Loss: 0.6328\n    Batch 210/219, Loss: 0.8434\n    Batch 219/219, Loss: 0.7708\n    DE Eval 26: LR=0.000066, Dropout=0.18, BS=8\n    First batch loaded, starting training...\n    Batch 10/875, Loss: 1.2542\n    Batch 20/875, Loss: 1.0772\n    Batch 30/875, Loss: 0.9611\n    Batch 40/875, Loss: 1.0480\n    Batch 50/875, Loss: 1.0989\n    Batch 60/875, Loss: 1.2883\n    Batch 70/875, Loss: 0.5662\n    Batch 80/875, Loss: 0.7743\n    Batch 90/875, Loss: 0.8068\n    Batch 100/875, Loss: 0.5867\n    Batch 110/875, Loss: 0.7650\n    Batch 120/875, Loss: 0.9264\n    Batch 130/875, Loss: 0.5243\n    Batch 140/875, Loss: 0.4944\n    Batch 150/875, Loss: 0.3844\n    Batch 160/875, Loss: 0.3489\n    Batch 170/875, Loss: 0.3807\n    Batch 180/875, Loss: 0.4142\n    Batch 190/875, Loss: 0.4456\n    Batch 200/875, Loss: 0.5515\n    Batch 210/875, Loss: 0.4344\n    Batch 220/875, Loss: 0.5206\n    Batch 230/875, Loss: 0.4982\n    Batch 240/875, Loss: 0.9969\n    Batch 250/875, Loss: 0.6998\n    Batch 260/875, Loss: 0.3445\n    Batch 270/875, Loss: 1.9021\n    Batch 280/875, Loss: 0.5294\n    Batch 290/875, Loss: 0.8372\n    Batch 300/875, Loss: 0.7189\n    Batch 310/875, Loss: 0.4744\n    Batch 320/875, Loss: 0.3323\n    Batch 330/875, Loss: 0.4324\n    Batch 340/875, Loss: 0.3408\n    Batch 350/875, Loss: 0.5574\n    Batch 360/875, Loss: 0.4284\n    Batch 370/875, Loss: 0.5801\n    Batch 380/875, Loss: 0.4346\n    Batch 390/875, Loss: 0.8426\n    Batch 400/875, Loss: 0.5738\n    Batch 410/875, Loss: 0.5438\n    Batch 420/875, Loss: 0.3722\n    Batch 430/875, Loss: 0.3603\n    Batch 440/875, Loss: 0.2691\n    Batch 450/875, Loss: 0.6968\n    Batch 460/875, Loss: 0.6171\n    Batch 470/875, Loss: 0.5417\n    Batch 480/875, Loss: 0.1804\n    Batch 490/875, Loss: 0.3631\n    Batch 500/875, Loss: 0.7372\n    Batch 510/875, Loss: 0.5070\n    Batch 520/875, Loss: 0.5338\n    Batch 530/875, Loss: 0.2688\n    Batch 540/875, Loss: 0.5151\n    Batch 550/875, Loss: 0.2426\n    Batch 560/875, Loss: 0.4223\n    Batch 570/875, Loss: 0.4840\n    Batch 580/875, Loss: 0.3462\n    Batch 590/875, Loss: 0.4374\n    Batch 600/875, Loss: 0.4191\n    Batch 610/875, Loss: 0.6821\n    Batch 620/875, Loss: 0.5448\n    Batch 630/875, Loss: 0.3097\n    Batch 640/875, Loss: 0.2432\n    Batch 650/875, Loss: 0.7239\n    Batch 660/875, Loss: 0.1484\n    Batch 670/875, Loss: 0.1841\n    Batch 680/875, Loss: 0.3889\n    Batch 690/875, Loss: 0.3397\n    Batch 700/875, Loss: 0.5623\n    Batch 710/875, Loss: 0.3800\n    Batch 720/875, Loss: 0.3645\n    Batch 730/875, Loss: 0.3996\n    Batch 740/875, Loss: 0.1836\n    Batch 750/875, Loss: 0.1639\n    Batch 760/875, Loss: 0.4776\n    Batch 770/875, Loss: 0.2994\n    Batch 780/875, Loss: 0.2203\n    Batch 790/875, Loss: 0.3345\n    Batch 800/875, Loss: 0.1068\n    Batch 810/875, Loss: 0.4885\n    Batch 820/875, Loss: 0.2768\n    Batch 830/875, Loss: 0.2038\n    Batch 840/875, Loss: 0.2915\n    Batch 850/875, Loss: 0.5796\n    Batch 860/875, Loss: 1.2382\n    Batch 870/875, Loss: 0.1136\n    Batch 875/875, Loss: 0.2890\n    DE Eval 27: LR=0.000047, Dropout=0.09, BS=8\n    First batch loaded, starting training...\n    Batch 10/875, Loss: 1.3143\n    Batch 20/875, Loss: 1.2001\n    Batch 30/875, Loss: 1.1598\n    Batch 40/875, Loss: 1.6533\n    Batch 50/875, Loss: 0.9778\n    Batch 60/875, Loss: 0.7537\n    Batch 70/875, Loss: 0.8827\n    Batch 80/875, Loss: 0.7414\n    Batch 90/875, Loss: 1.3952\n    Batch 100/875, Loss: 1.0972\n    Batch 110/875, Loss: 0.7145\n    Batch 120/875, Loss: 0.9795\n    Batch 130/875, Loss: 0.4194\n    Batch 140/875, Loss: 1.1171\n    Batch 150/875, Loss: 0.7245\n    Batch 160/875, Loss: 0.4533\n    Batch 170/875, Loss: 0.8292\n    Batch 180/875, Loss: 0.9974\n    Batch 190/875, Loss: 0.4369\n    Batch 200/875, Loss: 0.4284\n    Batch 210/875, Loss: 0.5122\n    Batch 220/875, Loss: 0.4980\n    Batch 230/875, Loss: 0.5995\n    Batch 240/875, Loss: 0.5351\n    Batch 250/875, Loss: 0.8567\n    Batch 260/875, Loss: 0.9805\n    Batch 270/875, Loss: 0.7955\n    Batch 280/875, Loss: 0.4276\n    Batch 290/875, Loss: 0.7894\n    Batch 300/875, Loss: 0.8760\n    Batch 310/875, Loss: 0.7282\n    Batch 320/875, Loss: 0.4977\n    Batch 330/875, Loss: 0.7355\n    Batch 340/875, Loss: 0.9418\n    Batch 350/875, Loss: 0.5587\n    Batch 360/875, Loss: 0.3315\n    Batch 370/875, Loss: 0.8208\n    Batch 380/875, Loss: 0.3107\n    Batch 390/875, Loss: 0.3116\n    Batch 400/875, Loss: 0.7738\n    Batch 410/875, Loss: 0.6109\n    Batch 420/875, Loss: 0.5861\n    Batch 430/875, Loss: 0.4803\n    Batch 440/875, Loss: 1.5870\n    Batch 450/875, Loss: 0.3688\n    Batch 460/875, Loss: 1.6374\n    Batch 470/875, Loss: 0.4422\n    Batch 480/875, Loss: 0.6063\n    Batch 490/875, Loss: 0.4280\n    Batch 500/875, Loss: 0.3514\n    Batch 510/875, Loss: 0.3849\n    Batch 520/875, Loss: 0.4791\n    Batch 530/875, Loss: 0.6745\n    Batch 540/875, Loss: 0.3891\n    Batch 550/875, Loss: 0.5720\n    Batch 560/875, Loss: 0.5942\n    Batch 570/875, Loss: 0.4417\n    Batch 580/875, Loss: 1.4793\n    Batch 590/875, Loss: 0.3865\n    Batch 600/875, Loss: 0.4580\n    Batch 610/875, Loss: 0.6973\n    Batch 620/875, Loss: 0.4192\n    Batch 630/875, Loss: 0.5323\n    Batch 640/875, Loss: 0.3454\n    Batch 650/875, Loss: 0.5109\n    Batch 660/875, Loss: 0.2738\n    Batch 670/875, Loss: 0.3300\n    Batch 680/875, Loss: 0.5997\n    Batch 690/875, Loss: 0.5988\n    Batch 700/875, Loss: 0.4911\n    Batch 710/875, Loss: 0.3821\n    Batch 720/875, Loss: 0.2728\n    Batch 730/875, Loss: 0.3114\n    Batch 740/875, Loss: 1.3595\n    Batch 750/875, Loss: 0.2309\n    Batch 760/875, Loss: 0.2619\n    Batch 770/875, Loss: 0.5731\n    Batch 780/875, Loss: 0.3849\n    Batch 790/875, Loss: 0.1940\n    Batch 800/875, Loss: 1.1296\n    Batch 810/875, Loss: 0.2413\n    Batch 820/875, Loss: 0.2588\n    Batch 830/875, Loss: 1.0483\n    Batch 840/875, Loss: 0.4022\n    Batch 850/875, Loss: 0.2593\n    Batch 860/875, Loss: 0.2851\n    Batch 870/875, Loss: 0.2497\n    Batch 875/875, Loss: 0.3435\n    DE Eval 28: LR=0.000117, Dropout=0.00, BS=8\n    First batch loaded, starting training...\n    Batch 10/875, Loss: 1.4072\n    Batch 20/875, Loss: 1.0324\n    Batch 30/875, Loss: 0.8684\n    Batch 40/875, Loss: 0.5742\n    Batch 50/875, Loss: 1.5005\n    Batch 60/875, Loss: 0.7397\n    Batch 70/875, Loss: 1.1960\n    Batch 80/875, Loss: 0.5454\n    Batch 90/875, Loss: 0.5561\n    Batch 100/875, Loss: 0.4322\n    Batch 110/875, Loss: 2.0687\n    Batch 120/875, Loss: 0.9501\n    Batch 130/875, Loss: 0.4642\n    Batch 140/875, Loss: 0.4513\n    Batch 150/875, Loss: 0.6036\n    Batch 160/875, Loss: 1.8845\n    Batch 170/875, Loss: 0.6242\n    Batch 180/875, Loss: 0.3291\n    Batch 190/875, Loss: 0.5528\n    Batch 200/875, Loss: 0.4140\n    Batch 210/875, Loss: 0.3314\n    Batch 220/875, Loss: 0.4222\n    Batch 230/875, Loss: 0.3420\n    Batch 240/875, Loss: 0.2997\n    Batch 250/875, Loss: 0.2789\n    Batch 260/875, Loss: 0.4062\n    Batch 270/875, Loss: 0.4440\n    Batch 280/875, Loss: 0.5409\n    Batch 290/875, Loss: 0.3515\n    Batch 300/875, Loss: 0.5637\n    Batch 310/875, Loss: 0.2166\n    Batch 320/875, Loss: 1.8287\n    Batch 330/875, Loss: 0.3457\n    Batch 340/875, Loss: 0.3833\n    Batch 350/875, Loss: 0.4466\n    Batch 360/875, Loss: 0.1584\n    Batch 370/875, Loss: 0.5377\n    Batch 380/875, Loss: 0.5346\n    Batch 390/875, Loss: 0.2842\n    Batch 400/875, Loss: 0.2665\n    Batch 410/875, Loss: 0.3523\n    Batch 420/875, Loss: 0.9877\n    Batch 430/875, Loss: 0.5487\n    Batch 440/875, Loss: 0.2804\n    Batch 450/875, Loss: 0.3549\n    Batch 460/875, Loss: 0.1288\n    Batch 470/875, Loss: 0.2026\n    Batch 480/875, Loss: 0.3684\n    Batch 490/875, Loss: 0.2644\n    Batch 500/875, Loss: 0.1371\n    Batch 510/875, Loss: 0.4920\n    Batch 520/875, Loss: 1.8626\n    Batch 530/875, Loss: 0.9854\n    Batch 540/875, Loss: 0.2925\n    Batch 550/875, Loss: 0.3322\n    Batch 560/875, Loss: 0.9970\n    Batch 570/875, Loss: 0.4585\n    Batch 580/875, Loss: 0.3076\n    Batch 590/875, Loss: 0.1586\n    Batch 600/875, Loss: 0.6572\n    Batch 610/875, Loss: 0.1021\n    Batch 620/875, Loss: 0.1266\n    Batch 630/875, Loss: 0.2977\n    Batch 640/875, Loss: 0.3005\n    Batch 650/875, Loss: 0.1743\n    Batch 660/875, Loss: 0.9128\n    Batch 670/875, Loss: 0.7737\n    Batch 680/875, Loss: 0.1950\n    Batch 690/875, Loss: 0.4755\n    Batch 700/875, Loss: 0.1722\n    Batch 710/875, Loss: 0.2008\n    Batch 720/875, Loss: 0.3458\n    Batch 730/875, Loss: 0.1550\n    Batch 740/875, Loss: 0.2042\n    Batch 750/875, Loss: 0.0751\n    Batch 760/875, Loss: 0.5758\n    Batch 770/875, Loss: 0.6646\n    Batch 780/875, Loss: 0.1388\n    Batch 790/875, Loss: 0.1282\n    Batch 800/875, Loss: 0.1310\n    Batch 810/875, Loss: 0.7221\n    Batch 820/875, Loss: 0.2007\n    Batch 830/875, Loss: 0.3935\n    Batch 840/875, Loss: 0.6963\n    Batch 850/875, Loss: 1.5845\n    Batch 860/875, Loss: 0.2862\n    Batch 870/875, Loss: 0.5028\n    Batch 875/875, Loss: 0.2152\n    DE Eval 29: LR=0.000653, Dropout=0.50, BS=8\n    First batch loaded, starting training...\n    Batch 10/875, Loss: 1.3019\n    Batch 20/875, Loss: 0.8385\n    Batch 30/875, Loss: 1.0905\n    Batch 40/875, Loss: 1.2011\n    Batch 50/875, Loss: 1.0118\n    Batch 60/875, Loss: 0.8711\n    Batch 70/875, Loss: 0.6396\n    Batch 80/875, Loss: 0.5723\n    Batch 90/875, Loss: 0.5052\n    Batch 100/875, Loss: 1.0761\n    Batch 110/875, Loss: 1.0096\n    Batch 120/875, Loss: 1.6725\n    Batch 130/875, Loss: 0.9609\n    Batch 140/875, Loss: 0.6636\n    Batch 150/875, Loss: 0.5821\n    Batch 160/875, Loss: 0.7432\n    Batch 170/875, Loss: 0.8310\n    Batch 180/875, Loss: 1.4828\n    Batch 190/875, Loss: 0.6117\n    Batch 200/875, Loss: 1.7214\n    Batch 210/875, Loss: 1.1646\n    Batch 220/875, Loss: 0.8645\n    Batch 230/875, Loss: 0.8586\n    Batch 240/875, Loss: 0.8407\n    Batch 250/875, Loss: 0.7773\n    Batch 260/875, Loss: 0.4758\n    Batch 270/875, Loss: 1.0536\n    Batch 280/875, Loss: 0.6709\n    Batch 290/875, Loss: 1.0277\n    Batch 300/875, Loss: 0.6376\n    Batch 310/875, Loss: 0.9442\n    Batch 320/875, Loss: 0.4914\n    Batch 330/875, Loss: 0.9608\n    Batch 340/875, Loss: 1.0949\n    Batch 350/875, Loss: 0.6180\n    Batch 360/875, Loss: 0.8499\n    Batch 370/875, Loss: 0.8605\n    Batch 380/875, Loss: 1.1586\n    Batch 390/875, Loss: 0.8736\n    Batch 400/875, Loss: 0.6290\n    Batch 410/875, Loss: 1.3197\n    Batch 420/875, Loss: 0.5953\n    Batch 430/875, Loss: 0.9676\n    Batch 440/875, Loss: 0.9277\n    Batch 450/875, Loss: 1.2110\n    Batch 460/875, Loss: 0.4420\n    Batch 470/875, Loss: 1.4738\n    Batch 480/875, Loss: 0.7915\n    Batch 490/875, Loss: 1.4251\n    Batch 500/875, Loss: 0.4913\n    Batch 510/875, Loss: 0.5850\n    Batch 520/875, Loss: 0.8816\n    Batch 530/875, Loss: 0.5238\n    Batch 540/875, Loss: 1.5340\n    Batch 550/875, Loss: 0.4864\n    Batch 560/875, Loss: 0.5585\n    Batch 570/875, Loss: 0.9120\n    Batch 580/875, Loss: 0.3695\n    Batch 590/875, Loss: 0.4496\n    Batch 600/875, Loss: 1.0425\n    Batch 610/875, Loss: 1.3263\n    Batch 620/875, Loss: 2.0556\n    Batch 630/875, Loss: 0.5246\n    Batch 640/875, Loss: 0.7473\n    Batch 650/875, Loss: 0.7704\n    Batch 660/875, Loss: 0.9051\n    Batch 670/875, Loss: 0.2357\n    Batch 680/875, Loss: 0.6320\n    Batch 690/875, Loss: 0.6114\n    Batch 700/875, Loss: 0.7023\n    Batch 710/875, Loss: 0.5412\n    Batch 720/875, Loss: 0.8060\n    Batch 730/875, Loss: 1.2720\n    Batch 740/875, Loss: 0.7214\n    Batch 750/875, Loss: 0.4115\n    Batch 760/875, Loss: 0.8014\n    Batch 770/875, Loss: 0.6822\n    Batch 780/875, Loss: 0.6554\n    Batch 790/875, Loss: 1.4032\n    Batch 800/875, Loss: 0.7227\n    Batch 810/875, Loss: 0.7057\n    Batch 820/875, Loss: 0.6003\n    Batch 830/875, Loss: 0.6747\n    Batch 840/875, Loss: 0.4870\n    Batch 850/875, Loss: 1.1869\n    Batch 860/875, Loss: 0.7805\n    Batch 870/875, Loss: 0.8631\n    Batch 875/875, Loss: 1.8405\n    DE Eval 30: LR=0.000010, Dropout=0.15, BS=8\n    First batch loaded, starting training...\n    Batch 10/875, Loss: 1.3620\n    Batch 20/875, Loss: 1.4095\n    Batch 30/875, Loss: 1.3779\n    Batch 40/875, Loss: 1.3773\n    Batch 50/875, Loss: 1.4108\n    Batch 60/875, Loss: 1.3687\n    Batch 70/875, Loss: 1.3508\n    Batch 80/875, Loss: 1.3376\n    Batch 90/875, Loss: 1.3312\n    Batch 100/875, Loss: 1.2528\n    Batch 110/875, Loss: 1.3202\n    Batch 120/875, Loss: 1.2689\n    Batch 130/875, Loss: 1.2392\n    Batch 140/875, Loss: 1.3965\n    Batch 150/875, Loss: 1.1559\n    Batch 160/875, Loss: 1.3236\n    Batch 170/875, Loss: 1.2765\n    Batch 180/875, Loss: 1.0577\n    Batch 190/875, Loss: 1.0558\n    Batch 200/875, Loss: 1.1113\n    Batch 210/875, Loss: 1.1148\n    Batch 220/875, Loss: 1.1476\n    Batch 230/875, Loss: 1.1900\n    Batch 240/875, Loss: 0.9648\n    Batch 250/875, Loss: 1.2270\n    Batch 260/875, Loss: 1.0732\n    Batch 270/875, Loss: 1.0813\n    Batch 280/875, Loss: 0.8597\n    Batch 290/875, Loss: 1.6749\n    Batch 300/875, Loss: 0.9799\n    Batch 310/875, Loss: 1.0442\n    Batch 320/875, Loss: 0.7997\n    Batch 330/875, Loss: 0.9990\n    Batch 340/875, Loss: 0.7324\n    Batch 350/875, Loss: 1.7922\n    Batch 360/875, Loss: 0.8314\n    Batch 370/875, Loss: 0.6175\n    Batch 380/875, Loss: 0.5758\n    Batch 390/875, Loss: 1.1615\n    Batch 400/875, Loss: 1.0658\n    Batch 410/875, Loss: 0.6268\n    Batch 420/875, Loss: 0.7717\n    Batch 430/875, Loss: 1.0425\n    Batch 440/875, Loss: 0.5034\n    Batch 450/875, Loss: 1.8911\n    Batch 460/875, Loss: 1.1213\n    Batch 470/875, Loss: 1.0867\n    Batch 480/875, Loss: 0.8319\n    Batch 490/875, Loss: 2.0239\n    Batch 500/875, Loss: 0.6174\n    Batch 510/875, Loss: 0.5213\n    Batch 520/875, Loss: 0.5573\n    Batch 530/875, Loss: 1.0449\n    Batch 540/875, Loss: 0.5562\n    Batch 550/875, Loss: 0.9871\n    Batch 560/875, Loss: 0.5701\n    Batch 570/875, Loss: 0.6436\n    Batch 580/875, Loss: 0.5770\n    Batch 590/875, Loss: 0.6430\n    Batch 600/875, Loss: 0.7689\n    Batch 610/875, Loss: 2.0840\n    Batch 620/875, Loss: 1.1457\n    Batch 630/875, Loss: 0.4616\n    Batch 640/875, Loss: 1.0729\n    Batch 650/875, Loss: 0.5388\n    Batch 660/875, Loss: 0.6431\n    Batch 670/875, Loss: 1.9262\n    Batch 680/875, Loss: 0.5038\n    Batch 690/875, Loss: 0.4351\n    Batch 700/875, Loss: 0.5452\n    Batch 710/875, Loss: 0.4540\n    Batch 720/875, Loss: 0.9046\n    Batch 730/875, Loss: 0.5024\n    Batch 740/875, Loss: 0.4137\n    Batch 750/875, Loss: 0.8454\n    Batch 760/875, Loss: 0.3819\n    Batch 770/875, Loss: 1.8146\n    Batch 780/875, Loss: 0.8894\n    Batch 790/875, Loss: 0.4929\n    Batch 800/875, Loss: 0.4724\n    Batch 810/875, Loss: 0.5266\n    Batch 820/875, Loss: 0.5307\n    Batch 830/875, Loss: 0.8847\n    Batch 840/875, Loss: 0.9278\n    Batch 850/875, Loss: 1.0798\n    Batch 860/875, Loss: 0.8271\n    Batch 870/875, Loss: 0.6557\n    Batch 875/875, Loss: 1.9439\n    DE Eval 31: LR=0.001928, Dropout=0.41, BS=8\n    First batch loaded, starting training...\n    Batch 10/875, Loss: 1.2618\n    Batch 20/875, Loss: 0.9155\n    Batch 30/875, Loss: 1.6286\n    Batch 40/875, Loss: 1.2496\n    Batch 50/875, Loss: 2.0523\n    Batch 60/875, Loss: 0.5719\n    Batch 70/875, Loss: 1.0056\n    Batch 80/875, Loss: 0.5813\n    Batch 90/875, Loss: 0.5765\n    Batch 100/875, Loss: 1.0281\n    Batch 110/875, Loss: 0.6393\n    Batch 120/875, Loss: 0.6025\n    Batch 130/875, Loss: 1.1061\n    Batch 140/875, Loss: 0.4578\n    Batch 150/875, Loss: 1.0602\n    Batch 160/875, Loss: 0.9215\n    Batch 170/875, Loss: 0.8797\n    Batch 180/875, Loss: 0.8258\n    Batch 190/875, Loss: 0.7678\n    Batch 200/875, Loss: 0.9936\n    Batch 210/875, Loss: 0.5676\n    Batch 220/875, Loss: 0.6090\n    Batch 230/875, Loss: 0.7841\n    Batch 240/875, Loss: 0.8841\n    Batch 250/875, Loss: 0.5835\n    Batch 260/875, Loss: 0.4738\n    Batch 270/875, Loss: 1.2435\n    Batch 280/875, Loss: 0.9005\n    Batch 290/875, Loss: 0.6293\n    Batch 300/875, Loss: 0.7792\n    Batch 310/875, Loss: 0.6206\n    Batch 320/875, Loss: 0.8654\n    Batch 330/875, Loss: 0.7941\n    Batch 340/875, Loss: 1.1532\n    Batch 350/875, Loss: 0.4199\n    Batch 360/875, Loss: 0.5486\n    Batch 370/875, Loss: 0.2751\n    Batch 380/875, Loss: 0.6472\n    Batch 390/875, Loss: 0.3562\n    Batch 400/875, Loss: 0.4235\n    Batch 410/875, Loss: 0.7422\n    Batch 420/875, Loss: 1.0261\n    Batch 430/875, Loss: 0.5311\n    Batch 440/875, Loss: 0.3991\n    Batch 450/875, Loss: 0.9161\n    Batch 460/875, Loss: 0.5382\n    Batch 470/875, Loss: 0.7381\n    Batch 480/875, Loss: 1.4311\n    Batch 490/875, Loss: 0.5677\n    Batch 500/875, Loss: 1.2169\n    Batch 510/875, Loss: 0.6660\n    Batch 520/875, Loss: 1.4800\n    Batch 530/875, Loss: 0.3520\n    Batch 540/875, Loss: 0.8460\n    Batch 550/875, Loss: 0.5449\n    Batch 560/875, Loss: 1.4417\n    Batch 570/875, Loss: 0.5994\n    Batch 580/875, Loss: 0.5775\n    Batch 590/875, Loss: 0.5309\n    Batch 600/875, Loss: 0.7301\n    Batch 610/875, Loss: 0.2569\n    Batch 620/875, Loss: 0.6100\n    Batch 630/875, Loss: 0.6722\n    Batch 640/875, Loss: 1.8262\n    Batch 650/875, Loss: 0.8714\n    Batch 660/875, Loss: 0.4370\n    Batch 670/875, Loss: 0.4705\n    Batch 680/875, Loss: 0.4737\n    Batch 690/875, Loss: 0.6422\n    Batch 700/875, Loss: 1.5018\n    Batch 710/875, Loss: 0.3195\n    Batch 720/875, Loss: 0.9567\n    Batch 730/875, Loss: 1.6899\n    Batch 740/875, Loss: 0.3564\n    Batch 750/875, Loss: 0.6921\n    Batch 760/875, Loss: 0.3501\n    Batch 770/875, Loss: 0.5167\n    Batch 780/875, Loss: 0.2710\n    Batch 790/875, Loss: 0.9164\n    Batch 800/875, Loss: 0.1997\n    Batch 810/875, Loss: 0.4864\n    Batch 820/875, Loss: 0.3350\n    Batch 830/875, Loss: 0.3158\n    Batch 840/875, Loss: 0.5743\n    Batch 850/875, Loss: 0.6392\n    Batch 860/875, Loss: 0.7053\n    Batch 870/875, Loss: 0.3765\n    Batch 875/875, Loss: 0.4848\n    DE Eval 32: LR=0.000315, Dropout=0.32, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 1.0365\n    Batch 20/219, Loss: 0.9243\n    Batch 30/219, Loss: 0.8170\n    Batch 40/219, Loss: 0.7521\n    Batch 50/219, Loss: 0.8191\n    Batch 60/219, Loss: 0.8926\n    Batch 70/219, Loss: 0.7609\n    Batch 80/219, Loss: 1.0572\n    Batch 90/219, Loss: 1.0637\n    Batch 100/219, Loss: 0.7731\n    Batch 110/219, Loss: 0.8096\n    Batch 120/219, Loss: 0.9663\n    Batch 130/219, Loss: 0.7328\n    Batch 140/219, Loss: 0.5246\n    Batch 150/219, Loss: 0.7182\n    Batch 160/219, Loss: 0.6451\n    Batch 170/219, Loss: 0.6248\n    Batch 180/219, Loss: 0.8239\n    Batch 190/219, Loss: 0.9253\n    Batch 200/219, Loss: 0.8843\n    Batch 210/219, Loss: 0.4390\n    Batch 219/219, Loss: 0.5399\n    DE Eval 33: LR=0.000226, Dropout=0.60, BS=8\n    First batch loaded, starting training...\n    Batch 10/875, Loss: 1.2005\n    Batch 20/875, Loss: 1.2669\n    Batch 30/875, Loss: 0.8597\n    Batch 40/875, Loss: 1.8206\n    Batch 50/875, Loss: 0.8967\n    Batch 60/875, Loss: 1.0439\n    Batch 70/875, Loss: 1.0248\n    Batch 80/875, Loss: 1.7926\n    Batch 90/875, Loss: 2.2413\n    Batch 100/875, Loss: 0.7646\n    Batch 110/875, Loss: 0.8399\n    Batch 120/875, Loss: 0.5735\n    Batch 130/875, Loss: 0.3113\n    Batch 140/875, Loss: 0.4313\n    Batch 150/875, Loss: 0.3237\n    Batch 160/875, Loss: 0.3012\n    Batch 170/875, Loss: 1.9522\n    Batch 180/875, Loss: 0.5152\n    Batch 190/875, Loss: 0.4846\n    Batch 200/875, Loss: 0.4488\n    Batch 210/875, Loss: 0.3322\n    Batch 220/875, Loss: 0.6906\n    Batch 230/875, Loss: 0.4179\n    Batch 240/875, Loss: 0.9721\n    Batch 250/875, Loss: 0.3310\n    Batch 260/875, Loss: 0.7849\n    Batch 270/875, Loss: 1.8890\n    Batch 280/875, Loss: 0.5665\n    Batch 290/875, Loss: 0.6366\n    Batch 300/875, Loss: 0.3360\n    Batch 310/875, Loss: 0.6686\n    Batch 320/875, Loss: 0.5445\n    Batch 330/875, Loss: 0.4876\n    Batch 340/875, Loss: 0.5664\n    Batch 350/875, Loss: 0.3256\n    Batch 360/875, Loss: 0.3100\n    Batch 370/875, Loss: 0.3437\n    Batch 380/875, Loss: 1.6786\n    Batch 390/875, Loss: 0.4855\n    Batch 400/875, Loss: 0.7475\n    Batch 410/875, Loss: 0.5916\n    Batch 420/875, Loss: 1.6612\n    Batch 430/875, Loss: 0.3579\n    Batch 440/875, Loss: 0.5257\n    Batch 450/875, Loss: 0.4805\n    Batch 460/875, Loss: 0.2522\n    Batch 470/875, Loss: 0.3103\n    Batch 480/875, Loss: 0.5964\n    Batch 490/875, Loss: 0.3411\n    Batch 500/875, Loss: 0.7679\n    Batch 510/875, Loss: 0.1818\n    Batch 520/875, Loss: 0.1487\n    Batch 530/875, Loss: 0.4843\n    Batch 540/875, Loss: 0.2680\n    Batch 550/875, Loss: 0.7175\n    Batch 560/875, Loss: 0.1863\n    Batch 570/875, Loss: 0.3503\n    Batch 580/875, Loss: 0.6161\n    Batch 590/875, Loss: 0.1440\n    Batch 600/875, Loss: 0.2032\n    Batch 610/875, Loss: 0.3946\n    Batch 620/875, Loss: 0.4042\n    Batch 630/875, Loss: 0.6237\n    Batch 640/875, Loss: 0.4629\n    Batch 650/875, Loss: 0.2032\n    Batch 660/875, Loss: 0.2394\n    Batch 670/875, Loss: 0.7306\n    Batch 680/875, Loss: 0.2671\n    Batch 690/875, Loss: 0.2385\n    Batch 700/875, Loss: 0.2225\n    Batch 710/875, Loss: 0.6851\n    Batch 720/875, Loss: 0.2632\n    Batch 730/875, Loss: 0.1811\n    Batch 740/875, Loss: 0.2111\n    Batch 750/875, Loss: 0.1525\n    Batch 760/875, Loss: 0.1149\n    Batch 770/875, Loss: 0.0975\n    Batch 780/875, Loss: 0.4787\n    Batch 790/875, Loss: 0.2481\n    Batch 800/875, Loss: 0.1289\n    Batch 810/875, Loss: 0.2783\n    Batch 820/875, Loss: 0.7883\n    Batch 830/875, Loss: 0.4440\n    Batch 840/875, Loss: 0.1960\n    Batch 850/875, Loss: 0.1677\n    Batch 860/875, Loss: 0.1331\n    Batch 870/875, Loss: 0.2001\n    Batch 875/875, Loss: 0.5086\n    DE Eval 34: LR=0.000010, Dropout=0.00, BS=8\n    First batch loaded, starting training...\n    Batch 10/875, Loss: 1.4498\n    Batch 20/875, Loss: 1.3780\n    Batch 30/875, Loss: 1.4244\n    Batch 40/875, Loss: 1.3870\n    Batch 50/875, Loss: 1.3876\n    Batch 60/875, Loss: 1.2992\n    Batch 70/875, Loss: 1.4055\n    Batch 80/875, Loss: 1.3213\n    Batch 90/875, Loss: 1.4160\n    Batch 100/875, Loss: 1.3378\n    Batch 110/875, Loss: 1.2434\n    Batch 120/875, Loss: 1.1450\n    Batch 130/875, Loss: 1.3155\n    Batch 140/875, Loss: 1.1078\n    Batch 150/875, Loss: 1.1674\n    Batch 160/875, Loss: 1.0947\n    Batch 170/875, Loss: 0.9477\n    Batch 180/875, Loss: 0.9349\n    Batch 190/875, Loss: 0.9178\n    Batch 200/875, Loss: 0.7999\n    Batch 210/875, Loss: 1.7181\n    Batch 220/875, Loss: 1.1368\n    Batch 230/875, Loss: 0.9550\n    Batch 240/875, Loss: 0.8930\n    Batch 250/875, Loss: 1.7150\n    Batch 260/875, Loss: 0.9666\n    Batch 270/875, Loss: 1.0548\n    Batch 280/875, Loss: 0.9613\n    Batch 290/875, Loss: 0.6614\n    Batch 300/875, Loss: 0.8509\n    Batch 310/875, Loss: 0.7171\n    Batch 320/875, Loss: 0.9401\n    Batch 330/875, Loss: 1.8091\n    Batch 340/875, Loss: 0.6746\n    Batch 350/875, Loss: 0.6560\n    Batch 360/875, Loss: 1.0057\n    Batch 370/875, Loss: 1.1333\n    Batch 380/875, Loss: 0.7430\n    Batch 390/875, Loss: 0.6255\n    Batch 400/875, Loss: 0.5134\n    Batch 410/875, Loss: 0.6913\n    Batch 420/875, Loss: 0.4941\n    Batch 430/875, Loss: 0.9465\n    Batch 440/875, Loss: 0.9067\n    Batch 450/875, Loss: 0.6970\n    Batch 460/875, Loss: 0.6281\n    Batch 470/875, Loss: 0.7290\n    Batch 480/875, Loss: 0.9615\n    Batch 490/875, Loss: 0.6879\n    Batch 500/875, Loss: 1.1175\n    Batch 510/875, Loss: 0.7451\n    Batch 520/875, Loss: 1.8590\n    Batch 530/875, Loss: 0.8697\n    Batch 540/875, Loss: 0.8735\n    Batch 550/875, Loss: 0.5884\n    Batch 560/875, Loss: 1.8400\n    Batch 570/875, Loss: 0.6314\n    Batch 580/875, Loss: 1.9187\n    Batch 590/875, Loss: 0.6407\n    Batch 600/875, Loss: 1.9275\n    Batch 610/875, Loss: 1.2104\n    Batch 620/875, Loss: 1.0426\n    Batch 630/875, Loss: 0.4914\n    Batch 640/875, Loss: 0.6241\n    Batch 650/875, Loss: 0.3519\n    Batch 660/875, Loss: 0.9359\n    Batch 670/875, Loss: 0.9169\n    Batch 680/875, Loss: 0.6701\n    Batch 690/875, Loss: 0.6195\n    Batch 700/875, Loss: 1.9300\n    Batch 710/875, Loss: 0.9024\n    Batch 720/875, Loss: 0.5340\n    Batch 730/875, Loss: 0.5898\n    Batch 740/875, Loss: 0.5660\n    Batch 750/875, Loss: 0.5535\n    Batch 760/875, Loss: 0.5405\n    Batch 770/875, Loss: 2.0435\n    Batch 780/875, Loss: 0.4618\n    Batch 790/875, Loss: 0.9685\n    Batch 800/875, Loss: 0.3962\n    Batch 810/875, Loss: 0.3771\n    Batch 820/875, Loss: 0.6836\n    Batch 830/875, Loss: 0.5362\n    Batch 840/875, Loss: 0.9707\n    Batch 850/875, Loss: 0.8623\n    Batch 860/875, Loss: 0.4262\n    Batch 870/875, Loss: 0.4386\n    Batch 875/875, Loss: 0.5145\n    DE Eval 35: LR=0.000283, Dropout=0.38, BS=8\n    First batch loaded, starting training...\n    Batch 10/875, Loss: 1.1681\n    Batch 20/875, Loss: 1.0353\n    Batch 30/875, Loss: 0.7127\n    Batch 40/875, Loss: 0.6027\n    Batch 50/875, Loss: 0.6608\n    Batch 60/875, Loss: 0.8194\n    Batch 70/875, Loss: 0.9994\n    Batch 80/875, Loss: 0.4862\n    Batch 90/875, Loss: 0.6847\n    Batch 100/875, Loss: 1.2403\n    Batch 110/875, Loss: 0.7642\n    Batch 120/875, Loss: 0.7719\n    Batch 130/875, Loss: 1.5761\n    Batch 140/875, Loss: 0.4253\n    Batch 150/875, Loss: 0.4849\n    Batch 160/875, Loss: 0.3597\n    Batch 170/875, Loss: 1.2308\n    Batch 180/875, Loss: 0.3239\n    Batch 190/875, Loss: 0.4505\n    Batch 200/875, Loss: 0.2504\n    Batch 210/875, Loss: 0.2957\n    Batch 220/875, Loss: 0.2924\n    Batch 230/875, Loss: 0.1660\n    Batch 240/875, Loss: 0.5623\n    Batch 250/875, Loss: 0.3472\n    Batch 260/875, Loss: 0.5607\n    Batch 270/875, Loss: 0.5814\n    Batch 280/875, Loss: 0.4406\n    Batch 290/875, Loss: 0.4363\n    Batch 300/875, Loss: 1.2347\n    Batch 310/875, Loss: 1.2680\n    Batch 320/875, Loss: 0.1567\n    Batch 330/875, Loss: 0.4116\n    Batch 340/875, Loss: 0.7913\n    Batch 350/875, Loss: 0.6525\n    Batch 360/875, Loss: 0.1957\n    Batch 370/875, Loss: 0.3324\n    Batch 380/875, Loss: 0.1833\n    Batch 390/875, Loss: 0.3703\n    Batch 400/875, Loss: 0.2600\n    Batch 410/875, Loss: 0.5172\n    Batch 420/875, Loss: 0.1087\n    Batch 430/875, Loss: 0.5321\n    Batch 440/875, Loss: 0.2604\n    Batch 450/875, Loss: 0.8888\n    Batch 460/875, Loss: 0.6540\n    Batch 470/875, Loss: 0.2613\n    Batch 480/875, Loss: 0.2416\n    Batch 490/875, Loss: 0.5306\n    Batch 500/875, Loss: 0.3216\n    Batch 510/875, Loss: 0.2088\n    Batch 520/875, Loss: 0.1561\n    Batch 530/875, Loss: 0.4320\n    Batch 540/875, Loss: 0.4413\n    Batch 550/875, Loss: 1.2223\n    Batch 560/875, Loss: 0.5798\n    Batch 570/875, Loss: 0.2472\n    Batch 580/875, Loss: 1.1835\n    Batch 590/875, Loss: 0.4243\n    Batch 600/875, Loss: 0.1922\n    Batch 610/875, Loss: 0.2423\n    Batch 620/875, Loss: 0.3895\n    Batch 630/875, Loss: 0.1712\n    Batch 640/875, Loss: 0.3269\n    Batch 650/875, Loss: 0.1535\n    Batch 660/875, Loss: 0.1174\n    Batch 670/875, Loss: 0.1853\n    Batch 680/875, Loss: 0.2460\n    Batch 690/875, Loss: 0.1600\n    Batch 700/875, Loss: 0.2177\n    Batch 710/875, Loss: 0.4132\n    Batch 720/875, Loss: 0.2962\n    Batch 730/875, Loss: 0.0866\n    Batch 740/875, Loss: 0.1462\n    Batch 750/875, Loss: 0.0820\n    Batch 760/875, Loss: 0.3285\n    Batch 770/875, Loss: 0.8080\n    Batch 780/875, Loss: 0.5435\n    Batch 790/875, Loss: 0.2659\n    Batch 800/875, Loss: 0.1824\n    Batch 810/875, Loss: 0.2231\n    Batch 820/875, Loss: 0.1138\n    Batch 830/875, Loss: 0.2870\n    Batch 840/875, Loss: 0.5632\n    Batch 850/875, Loss: 0.1278\n    Batch 860/875, Loss: 0.5209\n    Batch 870/875, Loss: 0.2117\n    Batch 875/875, Loss: 0.1069\n    DE Eval 36: LR=0.000028, Dropout=0.30, BS=16\n    First batch loaded, starting training...\n    Batch 10/438, Loss: 1.3005\n    Batch 20/438, Loss: 1.3241\n    Batch 30/438, Loss: 1.3664\n    Batch 40/438, Loss: 1.2577\n    Batch 50/438, Loss: 1.1340\n    Batch 60/438, Loss: 1.0765\n    Batch 70/438, Loss: 1.2909\n    Batch 80/438, Loss: 1.0966\n    Batch 90/438, Loss: 0.9575\n    Batch 100/438, Loss: 1.0024\n    Batch 110/438, Loss: 1.0486\n    Batch 120/438, Loss: 1.0043\n    Batch 130/438, Loss: 1.2759\n    Batch 140/438, Loss: 1.0337\n    Batch 150/438, Loss: 1.0793\n    Batch 160/438, Loss: 1.0005\n    Batch 170/438, Loss: 0.9205\n    Batch 180/438, Loss: 1.3374\n    Batch 190/438, Loss: 1.0062\n    Batch 200/438, Loss: 1.5021\n    Batch 210/438, Loss: 0.8740\n    Batch 220/438, Loss: 0.8277\n    Batch 230/438, Loss: 1.0512\n    Batch 240/438, Loss: 1.0189\n    Batch 250/438, Loss: 1.5357\n    Batch 260/438, Loss: 0.7669\n    Batch 270/438, Loss: 1.0261\n    Batch 280/438, Loss: 1.1786\n    Batch 290/438, Loss: 1.1948\n    Batch 300/438, Loss: 0.6761\n    Batch 310/438, Loss: 0.9000\n    Batch 320/438, Loss: 0.9884\n    Batch 330/438, Loss: 0.8022\n    Batch 340/438, Loss: 0.8988\n    Batch 350/438, Loss: 0.7436\n    Batch 360/438, Loss: 0.9160\n    Batch 370/438, Loss: 1.2229\n    Batch 380/438, Loss: 1.0913\n    Batch 390/438, Loss: 0.7129\n    Batch 400/438, Loss: 1.1746\n    Batch 410/438, Loss: 0.7573\n    Batch 420/438, Loss: 0.8946\n    Batch 430/438, Loss: 0.8264\n    Batch 438/438, Loss: 1.9679\n  DE Iter 2/4: Best = 0.7895\n    DE Eval 37: LR=0.000080, Dropout=0.60, BS=8\n    First batch loaded, starting training...\n    Batch 10/875, Loss: 1.4183\n    Batch 20/875, Loss: 1.3311\n    Batch 30/875, Loss: 1.2253\n    Batch 40/875, Loss: 1.2994\n    Batch 50/875, Loss: 1.2050\n    Batch 60/875, Loss: 1.0084\n    Batch 70/875, Loss: 1.0753\n    Batch 80/875, Loss: 0.6055\n    Batch 90/875, Loss: 0.7627\n    Batch 100/875, Loss: 0.8217\n    Batch 110/875, Loss: 0.5436\n    Batch 120/875, Loss: 0.9544\n    Batch 130/875, Loss: 0.9152\n    Batch 140/875, Loss: 0.5939\n    Batch 150/875, Loss: 0.4902\n    Batch 160/875, Loss: 1.1506\n    Batch 170/875, Loss: 0.6744\n    Batch 180/875, Loss: 0.8500\n    Batch 190/875, Loss: 0.3495\n    Batch 200/875, Loss: 0.9179\n    Batch 210/875, Loss: 0.5506\n    Batch 220/875, Loss: 0.4838\n    Batch 230/875, Loss: 0.4579\n    Batch 240/875, Loss: 0.5525\n    Batch 250/875, Loss: 0.8754\n    Batch 260/875, Loss: 0.4666\n    Batch 270/875, Loss: 0.4002\n    Batch 280/875, Loss: 0.7220\n    Batch 290/875, Loss: 0.2915\n    Batch 300/875, Loss: 2.0540\n    Batch 310/875, Loss: 1.7609\n    Batch 320/875, Loss: 0.4559\n    Batch 330/875, Loss: 0.3843\n    Batch 340/875, Loss: 0.4911\n    Batch 350/875, Loss: 0.6100\n    Batch 360/875, Loss: 0.7722\n    Batch 370/875, Loss: 0.4098\n    Batch 380/875, Loss: 0.6597\n    Batch 390/875, Loss: 2.1444\n    Batch 400/875, Loss: 0.5660\n    Batch 410/875, Loss: 0.7210\n    Batch 420/875, Loss: 0.5665\n    Batch 430/875, Loss: 0.7525\n    Batch 440/875, Loss: 0.4223\n    Batch 450/875, Loss: 0.4573\n    Batch 460/875, Loss: 0.3857\n    Batch 470/875, Loss: 0.4873\n    Batch 480/875, Loss: 0.3645\n    Batch 490/875, Loss: 0.3798\n    Batch 500/875, Loss: 0.5561\n    Batch 510/875, Loss: 0.4321\n    Batch 520/875, Loss: 1.3514\n    Batch 530/875, Loss: 0.4076\n    Batch 540/875, Loss: 0.6326\n    Batch 550/875, Loss: 1.4924\n    Batch 560/875, Loss: 0.2636\n    Batch 570/875, Loss: 0.3415\n    Batch 580/875, Loss: 0.7638\n    Batch 590/875, Loss: 0.4580\n    Batch 600/875, Loss: 0.2245\n    Batch 610/875, Loss: 0.4187\n    Batch 620/875, Loss: 1.2531\n    Batch 630/875, Loss: 0.3735\n    Batch 640/875, Loss: 0.2243\n    Batch 650/875, Loss: 0.2903\n    Batch 660/875, Loss: 0.5411\n    Batch 670/875, Loss: 0.6497\n    Batch 680/875, Loss: 0.9193\n    Batch 690/875, Loss: 0.4092\n    Batch 700/875, Loss: 0.3525\n    Batch 710/875, Loss: 0.4694\n    Batch 720/875, Loss: 0.9946\n    Batch 730/875, Loss: 0.2736\n    Batch 740/875, Loss: 0.3600\n    Batch 750/875, Loss: 0.2751\n    Batch 760/875, Loss: 0.3127\n    Batch 770/875, Loss: 0.9951\n    Batch 780/875, Loss: 0.3166\n    Batch 790/875, Loss: 0.2740\n    Batch 800/875, Loss: 0.4402\n    Batch 810/875, Loss: 0.5080\n    Batch 820/875, Loss: 1.5832\n    Batch 830/875, Loss: 0.3049\n    Batch 840/875, Loss: 1.3592\n    Batch 850/875, Loss: 0.7910\n    Batch 860/875, Loss: 0.2166\n    Batch 870/875, Loss: 0.3260\n    Batch 875/875, Loss: 0.1846\n    DE Eval 38: LR=0.001268, Dropout=0.30, BS=8\n    First batch loaded, starting training...\n    Batch 10/875, Loss: 1.3509\n    Batch 20/875, Loss: 0.9278\n    Batch 30/875, Loss: 1.2291\n    Batch 40/875, Loss: 0.4573\n    Batch 50/875, Loss: 0.5920\n    Batch 60/875, Loss: 0.5315\n    Batch 70/875, Loss: 0.7397\n    Batch 80/875, Loss: 1.1101\n    Batch 90/875, Loss: 1.2235\n    Batch 100/875, Loss: 0.4461\n    Batch 110/875, Loss: 0.6659\n    Batch 120/875, Loss: 0.8262\n    Batch 130/875, Loss: 0.7016\n    Batch 140/875, Loss: 0.3215\n    Batch 150/875, Loss: 0.3078\n    Batch 160/875, Loss: 0.5409\n    Batch 170/875, Loss: 0.3564\n    Batch 180/875, Loss: 0.8785\n    Batch 190/875, Loss: 1.3170\n    Batch 200/875, Loss: 0.2656\n    Batch 210/875, Loss: 0.6440\n    Batch 220/875, Loss: 0.5857\n    Batch 230/875, Loss: 0.9612\n    Batch 240/875, Loss: 1.2328\n    Batch 250/875, Loss: 0.6227\n    Batch 260/875, Loss: 0.6167\n    Batch 270/875, Loss: 0.5181\n    Batch 280/875, Loss: 0.6051\n    Batch 290/875, Loss: 0.4302\n    Batch 300/875, Loss: 0.3183\n    Batch 310/875, Loss: 0.4031\n    Batch 320/875, Loss: 0.2839\n    Batch 330/875, Loss: 0.8107\n    Batch 340/875, Loss: 0.8213\n    Batch 350/875, Loss: 0.2121\n    Batch 360/875, Loss: 0.2327\n    Batch 370/875, Loss: 0.4406\n    Batch 380/875, Loss: 0.4946\n    Batch 390/875, Loss: 0.7262\n    Batch 400/875, Loss: 0.4970\n    Batch 410/875, Loss: 0.4763\n    Batch 420/875, Loss: 0.6905\n    Batch 430/875, Loss: 1.7122\n    Batch 440/875, Loss: 0.8860\n    Batch 450/875, Loss: 0.5080\n    Batch 460/875, Loss: 0.1268\n    Batch 470/875, Loss: 0.7297\n    Batch 480/875, Loss: 0.6632\n    Batch 490/875, Loss: 0.3921\n    Batch 500/875, Loss: 0.6977\n    Batch 510/875, Loss: 0.3475\n    Batch 520/875, Loss: 0.2285\n    Batch 530/875, Loss: 0.5039\n    Batch 540/875, Loss: 0.2475\n    Batch 550/875, Loss: 0.1936\n    Batch 560/875, Loss: 0.7103\n    Batch 570/875, Loss: 0.3003\n    Batch 580/875, Loss: 0.4274\n    Batch 590/875, Loss: 0.5567\n    Batch 600/875, Loss: 0.1423\n    Batch 610/875, Loss: 0.1719\n    Batch 620/875, Loss: 0.2305\n    Batch 630/875, Loss: 0.2721\n    Batch 640/875, Loss: 0.4690\n    Batch 650/875, Loss: 1.9199\n    Batch 660/875, Loss: 0.3062\n    Batch 670/875, Loss: 0.5561\n    Batch 680/875, Loss: 0.5191\n    Batch 690/875, Loss: 0.6287\n    Batch 700/875, Loss: 0.6367\n    Batch 710/875, Loss: 0.5174\n    Batch 720/875, Loss: 0.4722\n    Batch 730/875, Loss: 0.3320\n    Batch 740/875, Loss: 0.1840\n    Batch 750/875, Loss: 0.2119\n    Batch 760/875, Loss: 0.4050\n    Batch 770/875, Loss: 0.3141\n    Batch 780/875, Loss: 0.4546\n    Batch 790/875, Loss: 0.3984\n    Batch 800/875, Loss: 0.4609\n    Batch 810/875, Loss: 0.2029\n    Batch 820/875, Loss: 0.5588\n    Batch 830/875, Loss: 0.5123\n    Batch 840/875, Loss: 0.5813\n    Batch 850/875, Loss: 0.1386\n    Batch 860/875, Loss: 0.1900\n    Batch 870/875, Loss: 0.3205\n    Batch 875/875, Loss: 0.5234\n    DE Eval 39: LR=0.002677, Dropout=0.58, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.8519\n    Batch 20/219, Loss: 1.1880\n    Batch 30/219, Loss: 0.7891\n    Batch 40/219, Loss: 0.8365\n    Batch 50/219, Loss: 0.6480\n    Batch 60/219, Loss: 1.1879\n    Batch 70/219, Loss: 0.8884\n    Batch 80/219, Loss: 0.7365\n    Batch 90/219, Loss: 1.1390\n    Batch 100/219, Loss: 0.8274\n    Batch 110/219, Loss: 0.9645\n    Batch 120/219, Loss: 0.9646\n    Batch 130/219, Loss: 0.5652\n    Batch 140/219, Loss: 0.8216\n    Batch 150/219, Loss: 0.5052\n    Batch 160/219, Loss: 0.7979\n    Batch 170/219, Loss: 0.7170\n    Batch 180/219, Loss: 0.7360\n    Batch 190/219, Loss: 0.7661\n    Batch 200/219, Loss: 0.5985\n    Batch 210/219, Loss: 0.4713\n    Batch 219/219, Loss: 1.3630\n    DE Eval 40: LR=0.000013, Dropout=0.34, BS=8\n    First batch loaded, starting training...\n    Batch 10/875, Loss: 1.3959\n    Batch 20/875, Loss: 1.3513\n    Batch 30/875, Loss: 1.3587\n    Batch 40/875, Loss: 1.2972\n    Batch 50/875, Loss: 1.3001\n    Batch 60/875, Loss: 1.2084\n    Batch 70/875, Loss: 1.3439\n    Batch 80/875, Loss: 1.3969\n    Batch 90/875, Loss: 1.1585\n    Batch 100/875, Loss: 1.1434\n    Batch 110/875, Loss: 1.0951\n    Batch 120/875, Loss: 1.0374\n    Batch 130/875, Loss: 1.1209\n    Batch 140/875, Loss: 1.2542\n    Batch 150/875, Loss: 0.9967\n    Batch 160/875, Loss: 0.9133\n    Batch 170/875, Loss: 1.1825\n    Batch 180/875, Loss: 1.2223\n    Batch 190/875, Loss: 0.9843\n    Batch 200/875, Loss: 0.7697\n    Batch 210/875, Loss: 1.0787\n    Batch 220/875, Loss: 0.9763\n    Batch 230/875, Loss: 1.7083\n    Batch 240/875, Loss: 1.1275\n    Batch 250/875, Loss: 1.7734\n    Batch 260/875, Loss: 1.7234\n    Batch 270/875, Loss: 0.7543\n    Batch 280/875, Loss: 1.7349\n    Batch 290/875, Loss: 0.7999\n    Batch 300/875, Loss: 0.5716\n    Batch 310/875, Loss: 1.0842\n    Batch 320/875, Loss: 1.2152\n    Batch 330/875, Loss: 0.7953\n    Batch 340/875, Loss: 1.8778\n    Batch 350/875, Loss: 0.6905\n    Batch 360/875, Loss: 0.8820\n    Batch 370/875, Loss: 0.6858\n    Batch 380/875, Loss: 0.4650\n    Batch 390/875, Loss: 0.5839\n    Batch 400/875, Loss: 1.0982\n    Batch 410/875, Loss: 0.7843\n    Batch 420/875, Loss: 1.0049\n    Batch 430/875, Loss: 1.9529\n    Batch 440/875, Loss: 0.6498\n    Batch 450/875, Loss: 0.5423\n    Batch 460/875, Loss: 0.5784\n    Batch 470/875, Loss: 2.0745\n    Batch 480/875, Loss: 0.5276\n    Batch 490/875, Loss: 0.4921\n    Batch 500/875, Loss: 0.5186\n    Batch 510/875, Loss: 0.6706\n    Batch 520/875, Loss: 0.4648\n    Batch 530/875, Loss: 0.6693\n    Batch 540/875, Loss: 0.9830\n    Batch 550/875, Loss: 1.9326\n    Batch 560/875, Loss: 0.7262\n    Batch 570/875, Loss: 0.7340\n    Batch 580/875, Loss: 0.4625\n    Batch 590/875, Loss: 0.9493\n    Batch 600/875, Loss: 0.6275\n    Batch 610/875, Loss: 0.3804\n    Batch 620/875, Loss: 0.7805\n    Batch 630/875, Loss: 0.5783\n    Batch 640/875, Loss: 0.5006\n    Batch 650/875, Loss: 0.4341\n    Batch 660/875, Loss: 0.5033\n    Batch 670/875, Loss: 0.8230\n    Batch 680/875, Loss: 0.6336\n    Batch 690/875, Loss: 0.8785\n    Batch 700/875, Loss: 0.7564\n    Batch 710/875, Loss: 0.4266\n    Batch 720/875, Loss: 0.5256\n    Batch 730/875, Loss: 0.5664\n    Batch 740/875, Loss: 0.5382\n    Batch 750/875, Loss: 0.5593\n    Batch 760/875, Loss: 0.9005\n    Batch 770/875, Loss: 0.6031\n    Batch 780/875, Loss: 0.5713\n    Batch 790/875, Loss: 0.6241\n    Batch 800/875, Loss: 0.9406\n    Batch 810/875, Loss: 0.9196\n    Batch 820/875, Loss: 0.4617\n    Batch 830/875, Loss: 0.4778\n    Batch 840/875, Loss: 0.5539\n    Batch 850/875, Loss: 0.4035\n    Batch 860/875, Loss: 0.4432\n    Batch 870/875, Loss: 1.9622\n    Batch 875/875, Loss: 0.8668\n    DE Eval 41: LR=0.000035, Dropout=0.60, BS=8\n    First batch loaded, starting training...\n    Batch 10/875, Loss: 1.3666\n    Batch 20/875, Loss: 1.3445\n    Batch 30/875, Loss: 1.3996\n    Batch 40/875, Loss: 1.4750\n    Batch 50/875, Loss: 1.2708\n    Batch 60/875, Loss: 1.2664\n    Batch 70/875, Loss: 1.1479\n    Batch 80/875, Loss: 1.1031\n    Batch 90/875, Loss: 1.0480\n    Batch 100/875, Loss: 1.0703\n    Batch 110/875, Loss: 0.9038\n    Batch 120/875, Loss: 1.8051\n    Batch 130/875, Loss: 1.6491\n    Batch 140/875, Loss: 0.9192\n    Batch 150/875, Loss: 0.8189\n    Batch 160/875, Loss: 1.1282\n    Batch 170/875, Loss: 1.8334\n    Batch 180/875, Loss: 0.5428\n    Batch 190/875, Loss: 0.6954\n    Batch 200/875, Loss: 0.7464\n    Batch 210/875, Loss: 1.1347\n    Batch 220/875, Loss: 0.5476\n    Batch 230/875, Loss: 0.9498\n    Batch 240/875, Loss: 0.7287\n    Batch 250/875, Loss: 0.7678\n    Batch 260/875, Loss: 0.7167\n    Batch 270/875, Loss: 1.9488\n    Batch 280/875, Loss: 0.4917\n    Batch 290/875, Loss: 0.6083\n    Batch 300/875, Loss: 1.0811\n    Batch 310/875, Loss: 0.6388\n    Batch 320/875, Loss: 1.9788\n    Batch 330/875, Loss: 0.6125\n    Batch 340/875, Loss: 1.0144\n    Batch 350/875, Loss: 0.5360\n    Batch 360/875, Loss: 0.8924\n    Batch 370/875, Loss: 0.4951\n    Batch 380/875, Loss: 0.4759\n    Batch 390/875, Loss: 0.6460\n    Batch 400/875, Loss: 0.5393\n    Batch 410/875, Loss: 0.5301\n    Batch 420/875, Loss: 0.6272\n    Batch 430/875, Loss: 0.9420\n    Batch 440/875, Loss: 0.5763\n    Batch 450/875, Loss: 0.3898\n    Batch 460/875, Loss: 0.3816\n    Batch 470/875, Loss: 0.8880\n    Batch 480/875, Loss: 0.7359\n    Batch 490/875, Loss: 0.2971\n    Batch 500/875, Loss: 0.6389\n    Batch 510/875, Loss: 0.4552\n    Batch 520/875, Loss: 0.4449\n    Batch 530/875, Loss: 1.1092\n    Batch 540/875, Loss: 0.3949\n    Batch 550/875, Loss: 0.5209\n    Batch 560/875, Loss: 0.4453\n    Batch 570/875, Loss: 0.3890\n    Batch 580/875, Loss: 0.4505\n    Batch 590/875, Loss: 1.8172\n    Batch 600/875, Loss: 0.4166\n    Batch 610/875, Loss: 0.6041\n    Batch 620/875, Loss: 2.0483\n    Batch 630/875, Loss: 0.8545\n    Batch 640/875, Loss: 0.3308\n    Batch 650/875, Loss: 0.5237\n    Batch 660/875, Loss: 0.7441\n    Batch 670/875, Loss: 0.7263\n    Batch 680/875, Loss: 0.3928\n    Batch 690/875, Loss: 0.8231\n    Batch 700/875, Loss: 0.6292\n    Batch 710/875, Loss: 0.7395\n    Batch 720/875, Loss: 0.6736\n    Batch 730/875, Loss: 0.4272\n    Batch 740/875, Loss: 0.6459\n    Batch 750/875, Loss: 0.7074\n    Batch 760/875, Loss: 0.4147\n    Batch 770/875, Loss: 0.7942\n    Batch 780/875, Loss: 0.3244\n    Batch 790/875, Loss: 0.4170\n    Batch 800/875, Loss: 0.9882\n    Batch 810/875, Loss: 0.4420\n    Batch 820/875, Loss: 0.3576\n    Batch 830/875, Loss: 0.4031\n    Batch 840/875, Loss: 0.4456\n    Batch 850/875, Loss: 0.6712\n    Batch 860/875, Loss: 1.8487\n    Batch 870/875, Loss: 0.4862\n    Batch 875/875, Loss: 0.4160\n    DE Eval 42: LR=0.000143, Dropout=0.60, BS=8\n    First batch loaded, starting training...\n    Batch 10/875, Loss: 1.3548\n    Batch 20/875, Loss: 1.2861\n    Batch 30/875, Loss: 1.0074\n    Batch 40/875, Loss: 1.6815\n    Batch 50/875, Loss: 0.8209\n    Batch 60/875, Loss: 0.8086\n    Batch 70/875, Loss: 0.6781\n    Batch 80/875, Loss: 0.5503\n    Batch 90/875, Loss: 1.0293\n    Batch 100/875, Loss: 0.8596\n    Batch 110/875, Loss: 0.8240\n    Batch 120/875, Loss: 0.8636\n    Batch 130/875, Loss: 0.5190\n    Batch 140/875, Loss: 0.4806\n    Batch 150/875, Loss: 0.6042\n    Batch 160/875, Loss: 0.6145\n    Batch 170/875, Loss: 0.6671\n    Batch 180/875, Loss: 0.4941\n    Batch 190/875, Loss: 1.1352\n    Batch 200/875, Loss: 0.4507\n    Batch 210/875, Loss: 1.6147\n    Batch 220/875, Loss: 0.6008\n    Batch 230/875, Loss: 0.7156\n    Batch 240/875, Loss: 0.5205\n    Batch 250/875, Loss: 0.3986\n    Batch 260/875, Loss: 1.3664\n    Batch 270/875, Loss: 0.5835\n    Batch 280/875, Loss: 0.4392\n    Batch 290/875, Loss: 0.7820\n    Batch 300/875, Loss: 0.7451\n    Batch 310/875, Loss: 1.7912\n    Batch 320/875, Loss: 0.6137\n    Batch 330/875, Loss: 0.4926\n    Batch 340/875, Loss: 0.4799\n    Batch 350/875, Loss: 0.6698\n    Batch 360/875, Loss: 0.6317\n    Batch 370/875, Loss: 0.8742\n    Batch 380/875, Loss: 0.3161\n    Batch 390/875, Loss: 0.6069\n    Batch 400/875, Loss: 0.5315\n    Batch 410/875, Loss: 0.6050\n    Batch 420/875, Loss: 0.3059\n    Batch 430/875, Loss: 0.5832\n    Batch 440/875, Loss: 0.3330\n    Batch 450/875, Loss: 0.2092\n    Batch 460/875, Loss: 1.5942\n    Batch 470/875, Loss: 0.7010\n    Batch 480/875, Loss: 0.2062\n    Batch 490/875, Loss: 0.5369\n    Batch 500/875, Loss: 0.2879\n    Batch 510/875, Loss: 0.5022\n    Batch 520/875, Loss: 0.3663\n    Batch 530/875, Loss: 0.2363\n    Batch 540/875, Loss: 0.3461\n    Batch 550/875, Loss: 0.5275\n    Batch 560/875, Loss: 0.6309\n    Batch 570/875, Loss: 0.3337\n    Batch 580/875, Loss: 0.2686\n    Batch 590/875, Loss: 0.2914\n    Batch 600/875, Loss: 0.6705\n    Batch 610/875, Loss: 0.2042\n    Batch 620/875, Loss: 0.4930\n    Batch 630/875, Loss: 0.1501\n    Batch 640/875, Loss: 0.2001\n    Batch 650/875, Loss: 0.5689\n    Batch 660/875, Loss: 0.2015\n    Batch 670/875, Loss: 0.2799\n    Batch 680/875, Loss: 0.1620\n    Batch 690/875, Loss: 0.2306\n    Batch 700/875, Loss: 0.3859\n    Batch 710/875, Loss: 0.3215\n    Batch 720/875, Loss: 0.5167\n    Batch 730/875, Loss: 1.6030\n    Batch 740/875, Loss: 0.5552\n    Batch 750/875, Loss: 0.5266\n    Batch 760/875, Loss: 0.5065\n    Batch 770/875, Loss: 0.4230\n    Batch 780/875, Loss: 1.2005\n    Batch 790/875, Loss: 0.3528\n    Batch 800/875, Loss: 0.1569\n    Batch 810/875, Loss: 0.2688\n    Batch 820/875, Loss: 0.3001\n    Batch 830/875, Loss: 0.1815\n    Batch 840/875, Loss: 0.1564\n    Batch 850/875, Loss: 0.4942\n    Batch 860/875, Loss: 0.1662\n    Batch 870/875, Loss: 0.2229\n    Batch 875/875, Loss: 0.2720\n    DE Eval 43: LR=0.000049, Dropout=0.41, BS=8\n    First batch loaded, starting training...\n    Batch 10/875, Loss: 1.4711\n    Batch 20/875, Loss: 1.3689\n    Batch 30/875, Loss: 1.3946\n    Batch 40/875, Loss: 1.1663\n    Batch 50/875, Loss: 1.2017\n    Batch 60/875, Loss: 1.1768\n    Batch 70/875, Loss: 1.0245\n    Batch 80/875, Loss: 1.6224\n    Batch 90/875, Loss: 1.2370\n    Batch 100/875, Loss: 1.2205\n    Batch 110/875, Loss: 1.6192\n    Batch 120/875, Loss: 0.6528\n    Batch 130/875, Loss: 0.9074\n    Batch 140/875, Loss: 2.0214\n    Batch 150/875, Loss: 1.0798\n    Batch 160/875, Loss: 0.5515\n    Batch 170/875, Loss: 0.5743\n    Batch 180/875, Loss: 2.0182\n    Batch 190/875, Loss: 0.9256\n    Batch 200/875, Loss: 1.8049\n    Batch 210/875, Loss: 0.7146\n    Batch 220/875, Loss: 0.4442\n    Batch 230/875, Loss: 0.8260\n    Batch 240/875, Loss: 0.5105\n    Batch 250/875, Loss: 1.9042\n    Batch 260/875, Loss: 0.6974\n    Batch 270/875, Loss: 0.8176\n    Batch 280/875, Loss: 0.8139\n    Batch 290/875, Loss: 0.7999\n    Batch 300/875, Loss: 1.7769\n    Batch 310/875, Loss: 0.7543\n    Batch 320/875, Loss: 0.4865\n    Batch 330/875, Loss: 0.7043\n    Batch 340/875, Loss: 0.7167\n    Batch 350/875, Loss: 1.8633\n    Batch 360/875, Loss: 0.4142\n    Batch 370/875, Loss: 0.6858\n    Batch 380/875, Loss: 0.3757\n    Batch 390/875, Loss: 0.3915\n    Batch 400/875, Loss: 0.4013\n    Batch 410/875, Loss: 0.4278\n    Batch 420/875, Loss: 0.6491\n    Batch 430/875, Loss: 0.5994\n    Batch 440/875, Loss: 0.3667\n    Batch 450/875, Loss: 0.4685\n    Batch 460/875, Loss: 0.2632\n    Batch 470/875, Loss: 0.4737\n    Batch 480/875, Loss: 0.3161\n    Batch 490/875, Loss: 0.4463\n    Batch 500/875, Loss: 1.4396\n    Batch 510/875, Loss: 0.5675\n    Batch 520/875, Loss: 0.6116\n    Batch 530/875, Loss: 0.3678\n    Batch 540/875, Loss: 0.2985\n    Batch 550/875, Loss: 0.2555\n    Batch 560/875, Loss: 0.3387\n    Batch 570/875, Loss: 1.6344\n    Batch 580/875, Loss: 0.3731\n    Batch 590/875, Loss: 0.3061\n    Batch 600/875, Loss: 0.4442\n    Batch 610/875, Loss: 0.5280\n    Batch 620/875, Loss: 0.3997\n    Batch 630/875, Loss: 0.7125\n    Batch 640/875, Loss: 0.5597\n    Batch 650/875, Loss: 0.2827\n    Batch 660/875, Loss: 0.4114\n    Batch 670/875, Loss: 0.3266\n    Batch 680/875, Loss: 0.2100\n    Batch 690/875, Loss: 0.7591\n    Batch 700/875, Loss: 0.3718\n    Batch 710/875, Loss: 0.4561\n    Batch 720/875, Loss: 1.2585\n    Batch 730/875, Loss: 0.2854\n    Batch 740/875, Loss: 0.3492\n    Batch 750/875, Loss: 0.4026\n    Batch 760/875, Loss: 0.6022\n    Batch 770/875, Loss: 0.2216\n    Batch 780/875, Loss: 1.7490\n    Batch 790/875, Loss: 0.4113\n    Batch 800/875, Loss: 0.2816\n    Batch 810/875, Loss: 0.2852\n    Batch 820/875, Loss: 0.3091\n    Batch 830/875, Loss: 0.5293\n    Batch 840/875, Loss: 0.6248\n    Batch 850/875, Loss: 1.0349\n    Batch 860/875, Loss: 0.4101\n    Batch 870/875, Loss: 0.3024\n    Batch 875/875, Loss: 0.2458\n    DE Eval 44: LR=0.000253, Dropout=0.23, BS=8\n    First batch loaded, starting training...\n    Batch 10/875, Loss: 1.1596\n    Batch 20/875, Loss: 0.6603\n    Batch 30/875, Loss: 1.2626\n    Batch 40/875, Loss: 1.8851\n    Batch 50/875, Loss: 1.7223\n    Batch 60/875, Loss: 0.6624\n    Batch 70/875, Loss: 0.5206\n    Batch 80/875, Loss: 0.9530\n    Batch 90/875, Loss: 0.4234\n    Batch 100/875, Loss: 1.6873\n    Batch 110/875, Loss: 0.4236\n    Batch 120/875, Loss: 0.5713\n    Batch 130/875, Loss: 1.9869\n    Batch 140/875, Loss: 0.3351\n    Batch 150/875, Loss: 0.7636\n    Batch 160/875, Loss: 0.2907\n    Batch 170/875, Loss: 0.7480\n    Batch 180/875, Loss: 0.7967\n    Batch 190/875, Loss: 0.3835\n    Batch 200/875, Loss: 0.7648\n    Batch 210/875, Loss: 0.5490\n    Batch 220/875, Loss: 0.2221\n    Batch 230/875, Loss: 0.3188\n    Batch 240/875, Loss: 0.2695\n    Batch 250/875, Loss: 0.3132\n    Batch 260/875, Loss: 1.2452\n    Batch 270/875, Loss: 0.9281\n    Batch 280/875, Loss: 0.1572\n    Batch 290/875, Loss: 0.7487\n    Batch 300/875, Loss: 0.2620\n    Batch 310/875, Loss: 0.5359\n    Batch 320/875, Loss: 0.2677\n    Batch 330/875, Loss: 0.3804\n    Batch 340/875, Loss: 0.3082\n    Batch 350/875, Loss: 0.5231\n    Batch 360/875, Loss: 0.6478\n    Batch 370/875, Loss: 0.4051\n    Batch 380/875, Loss: 0.5923\n    Batch 390/875, Loss: 0.2537\n    Batch 400/875, Loss: 0.6300\n    Batch 410/875, Loss: 0.5598\n    Batch 420/875, Loss: 0.7757\n    Batch 430/875, Loss: 0.2697\n    Batch 440/875, Loss: 0.2333\n    Batch 450/875, Loss: 0.2826\n    Batch 460/875, Loss: 0.3765\n    Batch 470/875, Loss: 0.7618\n    Batch 480/875, Loss: 0.1894\n    Batch 490/875, Loss: 0.1493\n    Batch 500/875, Loss: 0.3940\n    Batch 510/875, Loss: 0.8661\n    Batch 520/875, Loss: 0.4116\n    Batch 530/875, Loss: 0.1524\n    Batch 540/875, Loss: 0.7659\n    Batch 550/875, Loss: 0.3164\n    Batch 560/875, Loss: 0.3946\n    Batch 570/875, Loss: 0.7725\n    Batch 580/875, Loss: 0.2860\n    Batch 590/875, Loss: 0.3979\n    Batch 600/875, Loss: 0.1160\n    Batch 610/875, Loss: 0.0642\n    Batch 620/875, Loss: 0.0608\n    Batch 630/875, Loss: 0.2026\n    Batch 640/875, Loss: 0.1448\n    Batch 650/875, Loss: 0.2546\n    Batch 660/875, Loss: 0.2819\n    Batch 670/875, Loss: 0.6836\n    Batch 680/875, Loss: 0.1186\n    Batch 690/875, Loss: 0.1154\n    Batch 700/875, Loss: 0.2445\n    Batch 710/875, Loss: 0.2339\n    Batch 720/875, Loss: 0.1270\n    Batch 730/875, Loss: 0.2096\n    Batch 740/875, Loss: 0.0650\n    Batch 750/875, Loss: 0.1813\n    Batch 760/875, Loss: 1.1234\n    Batch 770/875, Loss: 0.0923\n    Batch 780/875, Loss: 0.1781\n    Batch 790/875, Loss: 0.3943\n    Batch 800/875, Loss: 0.0652\n    Batch 810/875, Loss: 0.1404\n    Batch 820/875, Loss: 0.6539\n    Batch 830/875, Loss: 0.2625\n    Batch 840/875, Loss: 0.2858\n    Batch 850/875, Loss: 0.4072\n    Batch 860/875, Loss: 0.0825\n    Batch 870/875, Loss: 0.4388\n    Batch 875/875, Loss: 0.2976\n    DE Eval 45: LR=0.000358, Dropout=0.20, BS=8\n    First batch loaded, starting training...\n    Batch 10/875, Loss: 1.0095\n    Batch 20/875, Loss: 2.4007\n    Batch 30/875, Loss: 0.6005\n    Batch 40/875, Loss: 0.5928\n    Batch 50/875, Loss: 1.8934\n    Batch 60/875, Loss: 0.8692\n    Batch 70/875, Loss: 0.7093\n    Batch 80/875, Loss: 1.0058\n    Batch 90/875, Loss: 0.5706\n    Batch 100/875, Loss: 0.8800\n    Batch 110/875, Loss: 0.4574\n    Batch 120/875, Loss: 0.6807\n    Batch 130/875, Loss: 0.7467\n    Batch 140/875, Loss: 2.1339\n    Batch 150/875, Loss: 0.3137\n    Batch 160/875, Loss: 0.3713\n    Batch 170/875, Loss: 0.3117\n    Batch 180/875, Loss: 0.3689\n    Batch 190/875, Loss: 0.2474\n    Batch 200/875, Loss: 0.9114\n    Batch 210/875, Loss: 1.2279\n    Batch 220/875, Loss: 0.4812\n    Batch 230/875, Loss: 0.5268\n    Batch 240/875, Loss: 0.3132\n    Batch 250/875, Loss: 0.2116\n    Batch 260/875, Loss: 0.7061\n    Batch 270/875, Loss: 0.6151\n    Batch 280/875, Loss: 0.4861\n    Batch 290/875, Loss: 0.8303\n    Batch 300/875, Loss: 0.5311\n    Batch 310/875, Loss: 0.2551\n    Batch 320/875, Loss: 0.3666\n    Batch 330/875, Loss: 0.2934\n    Batch 340/875, Loss: 0.6049\n    Batch 350/875, Loss: 0.4470\n    Batch 360/875, Loss: 0.4025\n    Batch 370/875, Loss: 0.3474\n    Batch 380/875, Loss: 0.4912\n    Batch 390/875, Loss: 0.8437\n    Batch 400/875, Loss: 0.3805\n    Batch 410/875, Loss: 0.2938\n    Batch 420/875, Loss: 0.9639\n    Batch 430/875, Loss: 0.2443\n    Batch 440/875, Loss: 0.2043\n    Batch 450/875, Loss: 0.8519\n    Batch 460/875, Loss: 0.2226\n    Batch 470/875, Loss: 0.4453\n    Batch 480/875, Loss: 1.2663\n    Batch 490/875, Loss: 0.3035\n    Batch 500/875, Loss: 0.1147\n    Batch 510/875, Loss: 0.6082\n    Batch 520/875, Loss: 0.2566\n    Batch 530/875, Loss: 0.2423\n    Batch 540/875, Loss: 0.1251\n    Batch 550/875, Loss: 0.1769\n    Batch 560/875, Loss: 0.3808\n    Batch 570/875, Loss: 0.3212\n    Batch 580/875, Loss: 0.5131\n    Batch 590/875, Loss: 0.2682\n    Batch 600/875, Loss: 0.2589\n    Batch 610/875, Loss: 0.5154\n    Batch 620/875, Loss: 0.2958\n    Batch 630/875, Loss: 0.3689\n    Batch 640/875, Loss: 0.1263\n    Batch 650/875, Loss: 0.2646\n    Batch 660/875, Loss: 0.1264\n    Batch 670/875, Loss: 0.2641\n    Batch 680/875, Loss: 0.3681\n    Batch 690/875, Loss: 0.1907\n    Batch 700/875, Loss: 0.1423\n    Batch 710/875, Loss: 0.4765\n    Batch 720/875, Loss: 0.4116\n    Batch 730/875, Loss: 0.2579\n    Batch 740/875, Loss: 0.2929\n    Batch 750/875, Loss: 0.9196\n    Batch 760/875, Loss: 0.1580\n    Batch 770/875, Loss: 0.2357\n    Batch 780/875, Loss: 0.1446\n    Batch 790/875, Loss: 0.4777\n    Batch 800/875, Loss: 0.2381\n    Batch 810/875, Loss: 0.1927\n    Batch 820/875, Loss: 0.1405\n    Batch 830/875, Loss: 0.1664\n    Batch 840/875, Loss: 0.2451\n    Batch 850/875, Loss: 0.2199\n    Batch 860/875, Loss: 0.4457\n    Batch 870/875, Loss: 0.3397\n    Batch 875/875, Loss: 1.2043\n    DE Eval 46: LR=0.000303, Dropout=0.05, BS=8\n    First batch loaded, starting training...\n    Batch 10/875, Loss: 1.3382\n    Batch 20/875, Loss: 0.4782\n    Batch 30/875, Loss: 1.6061\n    Batch 40/875, Loss: 0.8865\n    Batch 50/875, Loss: 1.7088\n    Batch 60/875, Loss: 0.4063\n    Batch 70/875, Loss: 0.6886\n    Batch 80/875, Loss: 0.5190\n    Batch 90/875, Loss: 0.8405\n    Batch 100/875, Loss: 0.2804\n    Batch 110/875, Loss: 0.3980\n    Batch 120/875, Loss: 0.2562\n    Batch 130/875, Loss: 1.3163\n    Batch 140/875, Loss: 0.4522\n    Batch 150/875, Loss: 0.3011\n    Batch 160/875, Loss: 0.3504\n    Batch 170/875, Loss: 0.6663\n    Batch 180/875, Loss: 0.5532\n    Batch 190/875, Loss: 0.1933\n    Batch 200/875, Loss: 0.4589\n    Batch 210/875, Loss: 0.6183\n    Batch 220/875, Loss: 0.3066\n    Batch 230/875, Loss: 0.3088\n    Batch 240/875, Loss: 0.6038\n    Batch 250/875, Loss: 0.5202\n    Batch 260/875, Loss: 0.2540\n    Batch 270/875, Loss: 0.2623\n    Batch 280/875, Loss: 1.1644\n    Batch 290/875, Loss: 0.2995\n    Batch 300/875, Loss: 0.3248\n    Batch 310/875, Loss: 0.5430\n    Batch 320/875, Loss: 0.4506\n    Batch 330/875, Loss: 0.4476\n    Batch 340/875, Loss: 0.1971\n    Batch 350/875, Loss: 1.7691\n    Batch 360/875, Loss: 0.2067\n    Batch 370/875, Loss: 0.1794\n    Batch 380/875, Loss: 0.6660\n    Batch 390/875, Loss: 0.2565\n    Batch 400/875, Loss: 0.2202\n    Batch 410/875, Loss: 0.4374\n    Batch 420/875, Loss: 0.2426\n    Batch 430/875, Loss: 0.1748\n    Batch 440/875, Loss: 0.5297\n    Batch 450/875, Loss: 0.1278\n    Batch 460/875, Loss: 0.3836\n    Batch 470/875, Loss: 0.3209\n    Batch 480/875, Loss: 0.2339\n    Batch 490/875, Loss: 0.2716\n    Batch 500/875, Loss: 0.2140\n    Batch 510/875, Loss: 0.3027\n    Batch 520/875, Loss: 0.7914\n    Batch 530/875, Loss: 0.3748\n    Batch 540/875, Loss: 0.1708\n    Batch 550/875, Loss: 0.3407\n    Batch 560/875, Loss: 0.6188\n    Batch 570/875, Loss: 0.3420\n    Batch 580/875, Loss: 1.2282\n    Batch 590/875, Loss: 0.1486\n    Batch 600/875, Loss: 0.3130\n    Batch 610/875, Loss: 0.2314\n    Batch 620/875, Loss: 0.1507\n    Batch 630/875, Loss: 0.3200\n    Batch 640/875, Loss: 0.3348\n    Batch 650/875, Loss: 0.5984\n    Batch 660/875, Loss: 0.2584\n    Batch 670/875, Loss: 0.1976\n    Batch 680/875, Loss: 0.4936\n    Batch 690/875, Loss: 0.2316\n    Batch 700/875, Loss: 0.3280\n    Batch 710/875, Loss: 0.4198\n    Batch 720/875, Loss: 0.0867\n    Batch 730/875, Loss: 1.1990\n    Batch 740/875, Loss: 0.2544\n    Batch 750/875, Loss: 0.3340\n    Batch 760/875, Loss: 0.4600\n    Batch 770/875, Loss: 0.0651\n    Batch 780/875, Loss: 0.3120\n    Batch 790/875, Loss: 0.3516\n    Batch 800/875, Loss: 0.0694\n    Batch 810/875, Loss: 0.2762\n    Batch 820/875, Loss: 0.1136\n    Batch 830/875, Loss: 0.1915\n    Batch 840/875, Loss: 0.2556\n    Batch 850/875, Loss: 0.1545\n    Batch 860/875, Loss: 0.1365\n    Batch 870/875, Loss: 0.2824\n    Batch 875/875, Loss: 0.2563\n    DE Eval 47: LR=0.000069, Dropout=0.12, BS=8\n    First batch loaded, starting training...\n    Batch 10/875, Loss: 1.3653\n    Batch 20/875, Loss: 1.2887\n    Batch 30/875, Loss: 1.2005\n    Batch 40/875, Loss: 1.0351\n    Batch 50/875, Loss: 0.9096\n    Batch 60/875, Loss: 0.9741\n    Batch 70/875, Loss: 0.6512\n    Batch 80/875, Loss: 1.7838\n    Batch 90/875, Loss: 0.6743\n    Batch 100/875, Loss: 0.7043\n    Batch 110/875, Loss: 0.6454\n    Batch 120/875, Loss: 0.4967\n    Batch 130/875, Loss: 0.2971\n    Batch 140/875, Loss: 0.5299\n    Batch 150/875, Loss: 0.9167\n    Batch 160/875, Loss: 0.5245\n    Batch 170/875, Loss: 0.4167\n    Batch 180/875, Loss: 0.4693\n    Batch 190/875, Loss: 0.9200\n    Batch 200/875, Loss: 0.3529\n    Batch 210/875, Loss: 0.4975\n    Batch 220/875, Loss: 0.7414\n    Batch 230/875, Loss: 0.7485\n    Batch 240/875, Loss: 0.8494\n    Batch 250/875, Loss: 0.3911\n    Batch 260/875, Loss: 0.3739\n    Batch 270/875, Loss: 0.3274\n    Batch 280/875, Loss: 0.9360\n    Batch 290/875, Loss: 0.5661\n    Batch 300/875, Loss: 0.4021\n    Batch 310/875, Loss: 1.5635\n    Batch 320/875, Loss: 0.4478\n    Batch 330/875, Loss: 0.5599\n    Batch 340/875, Loss: 0.4313\n    Batch 350/875, Loss: 0.3180\n    Batch 360/875, Loss: 0.3338\n    Batch 370/875, Loss: 0.2723\n    Batch 380/875, Loss: 0.5131\n    Batch 390/875, Loss: 0.5388\n    Batch 400/875, Loss: 0.3494\n    Batch 410/875, Loss: 0.2914\n    Batch 420/875, Loss: 1.6994\n    Batch 430/875, Loss: 0.3767\n    Batch 440/875, Loss: 0.3651\n    Batch 450/875, Loss: 0.3295\n    Batch 460/875, Loss: 0.5675\n    Batch 470/875, Loss: 0.2390\n    Batch 480/875, Loss: 0.2890\n    Batch 490/875, Loss: 0.3276\n    Batch 500/875, Loss: 1.3397\n    Batch 510/875, Loss: 0.6670\n    Batch 520/875, Loss: 0.4199\n    Batch 530/875, Loss: 0.8143\n    Batch 540/875, Loss: 0.1835\n    Batch 550/875, Loss: 0.3914\n    Batch 560/875, Loss: 0.3780\n    Batch 570/875, Loss: 0.7790\n    Batch 580/875, Loss: 0.2617\n    Batch 590/875, Loss: 0.2396\n    Batch 600/875, Loss: 0.5354\n    Batch 610/875, Loss: 1.1463\n    Batch 620/875, Loss: 0.4443\n    Batch 630/875, Loss: 0.2501\n    Batch 640/875, Loss: 0.6884\n    Batch 650/875, Loss: 0.6877\n    Batch 660/875, Loss: 0.3661\n    Batch 670/875, Loss: 0.2991\n    Batch 680/875, Loss: 0.2725\n    Batch 690/875, Loss: 0.3581\n    Batch 700/875, Loss: 0.2973\n    Batch 710/875, Loss: 0.3495\n    Batch 720/875, Loss: 0.2893\n    Batch 730/875, Loss: 0.1813\n    Batch 740/875, Loss: 0.2848\n    Batch 750/875, Loss: 0.6859\n    Batch 760/875, Loss: 0.2393\n    Batch 770/875, Loss: 0.7010\n    Batch 780/875, Loss: 0.4821\n    Batch 790/875, Loss: 0.2217\n    Batch 800/875, Loss: 0.7825\n    Batch 810/875, Loss: 0.6419\n    Batch 820/875, Loss: 0.3048\n    Batch 830/875, Loss: 0.4969\n    Batch 840/875, Loss: 0.3834\n    Batch 850/875, Loss: 0.4685\n    Batch 860/875, Loss: 0.2153\n    Batch 870/875, Loss: 0.4351\n    Batch 875/875, Loss: 0.2221\n    DE Eval 48: LR=0.000059, Dropout=0.24, BS=8\n    First batch loaded, starting training...\n    Batch 10/875, Loss: 1.3735\n    Batch 20/875, Loss: 1.3254\n    Batch 30/875, Loss: 1.2655\n    Batch 40/875, Loss: 1.1114\n    Batch 50/875, Loss: 1.0520\n    Batch 60/875, Loss: 1.6997\n    Batch 70/875, Loss: 1.4562\n    Batch 80/875, Loss: 0.7550\n    Batch 90/875, Loss: 0.5616\n    Batch 100/875, Loss: 0.6925\n    Batch 110/875, Loss: 0.6599\n    Batch 120/875, Loss: 0.7672\n    Batch 130/875, Loss: 0.7045\n    Batch 140/875, Loss: 0.9070\n    Batch 150/875, Loss: 0.5518\n    Batch 160/875, Loss: 0.4321\n    Batch 170/875, Loss: 1.0815\n    Batch 180/875, Loss: 0.4927\n    Batch 190/875, Loss: 0.8396\n    Batch 200/875, Loss: 0.4409\n    Batch 210/875, Loss: 0.4690\n    Batch 220/875, Loss: 0.5995\n    Batch 230/875, Loss: 0.4597\n    Batch 240/875, Loss: 0.8424\n    Batch 250/875, Loss: 1.9706\n    Batch 260/875, Loss: 0.8070\n    Batch 270/875, Loss: 0.5127\n    Batch 280/875, Loss: 0.4290\n    Batch 290/875, Loss: 0.5124\n    Batch 300/875, Loss: 1.6882\n    Batch 310/875, Loss: 0.4077\n    Batch 320/875, Loss: 0.3136\n    Batch 330/875, Loss: 0.5328\n    Batch 340/875, Loss: 0.4227\n    Batch 350/875, Loss: 0.4645\n    Batch 360/875, Loss: 0.4673\n    Batch 370/875, Loss: 0.4584\n    Batch 380/875, Loss: 0.7106\n    Batch 390/875, Loss: 0.3159\n    Batch 400/875, Loss: 0.6173\n    Batch 410/875, Loss: 0.6751\n    Batch 420/875, Loss: 0.3975\n    Batch 430/875, Loss: 0.6617\n    Batch 440/875, Loss: 0.6592\n    Batch 450/875, Loss: 1.5280\n    Batch 460/875, Loss: 1.2728\n    Batch 470/875, Loss: 1.2397\n    Batch 480/875, Loss: 0.3010\n    Batch 490/875, Loss: 0.6613\n    Batch 500/875, Loss: 1.5174\n    Batch 510/875, Loss: 0.5792\n    Batch 520/875, Loss: 0.7473\n    Batch 530/875, Loss: 1.2670\n    Batch 540/875, Loss: 0.4383\n    Batch 550/875, Loss: 0.6818\n    Batch 560/875, Loss: 0.6068\n    Batch 570/875, Loss: 0.1750\n    Batch 580/875, Loss: 0.3525\n    Batch 590/875, Loss: 0.2159\n    Batch 600/875, Loss: 0.2703\n    Batch 610/875, Loss: 0.2999\n    Batch 620/875, Loss: 0.3421\n    Batch 630/875, Loss: 0.2103\n    Batch 640/875, Loss: 0.3600\n    Batch 650/875, Loss: 0.4462\n    Batch 660/875, Loss: 0.3632\n    Batch 670/875, Loss: 2.0493\n    Batch 680/875, Loss: 1.5572\n    Batch 690/875, Loss: 0.6348\n    Batch 700/875, Loss: 0.5184\n    Batch 710/875, Loss: 0.6409\n    Batch 720/875, Loss: 0.4082\n    Batch 730/875, Loss: 0.4709\n    Batch 740/875, Loss: 0.6997\n    Batch 750/875, Loss: 0.2229\n    Batch 760/875, Loss: 0.3931\n    Batch 770/875, Loss: 0.7083\n    Batch 780/875, Loss: 0.2111\n    Batch 790/875, Loss: 0.3875\n    Batch 800/875, Loss: 0.2400\n    Batch 810/875, Loss: 0.4120\n    Batch 820/875, Loss: 0.3277\n    Batch 830/875, Loss: 0.2552\n    Batch 840/875, Loss: 0.5206\n    Batch 850/875, Loss: 0.6088\n    Batch 860/875, Loss: 0.3426\n    Batch 870/875, Loss: 0.2490\n    Batch 875/875, Loss: 1.3666\n  DE Iter 3/4: Best = 0.7895\n    DE Eval 49: LR=0.000080, Dropout=0.09, BS=8\n    First batch loaded, starting training...\n    Batch 10/875, Loss: 1.3887\n    Batch 20/875, Loss: 1.2324\n    Batch 30/875, Loss: 1.3023\n    Batch 40/875, Loss: 1.0507\n    Batch 50/875, Loss: 0.6490\n    Batch 60/875, Loss: 2.0538\n    Batch 70/875, Loss: 0.9850\n    Batch 80/875, Loss: 1.1012\n    Batch 90/875, Loss: 0.9872\n    Batch 100/875, Loss: 0.6045\n    Batch 110/875, Loss: 0.7989\n    Batch 120/875, Loss: 0.5245\n    Batch 130/875, Loss: 0.9820\n    Batch 140/875, Loss: 0.4656\n    Batch 150/875, Loss: 0.8266\n    Batch 160/875, Loss: 0.6185\n    Batch 170/875, Loss: 0.5992\n    Batch 180/875, Loss: 0.8398\n    Batch 190/875, Loss: 0.3998\n    Batch 200/875, Loss: 0.4760\n    Batch 210/875, Loss: 0.4952\n    Batch 220/875, Loss: 0.6277\n    Batch 230/875, Loss: 0.4425\n    Batch 240/875, Loss: 0.4004\n    Batch 250/875, Loss: 0.2930\n    Batch 260/875, Loss: 0.7707\n    Batch 270/875, Loss: 0.4243\n    Batch 280/875, Loss: 0.4033\n    Batch 290/875, Loss: 0.4050\n    Batch 300/875, Loss: 0.4581\n    Batch 310/875, Loss: 0.3489\n    Batch 320/875, Loss: 1.4089\n    Batch 330/875, Loss: 0.5421\n    Batch 340/875, Loss: 0.7069\n    Batch 350/875, Loss: 0.4131\n    Batch 360/875, Loss: 0.7260\n    Batch 370/875, Loss: 0.5447\n    Batch 380/875, Loss: 0.8424\n    Batch 390/875, Loss: 0.5604\n    Batch 400/875, Loss: 1.8681\n    Batch 410/875, Loss: 0.3131\n    Batch 420/875, Loss: 0.5720\n    Batch 430/875, Loss: 0.8712\n    Batch 440/875, Loss: 0.5813\n    Batch 450/875, Loss: 0.2879\n    Batch 460/875, Loss: 0.2338\n    Batch 470/875, Loss: 1.5985\n    Batch 480/875, Loss: 0.2607\n    Batch 490/875, Loss: 0.5687\n    Batch 500/875, Loss: 0.5957\n    Batch 510/875, Loss: 0.4629\n    Batch 520/875, Loss: 0.3639\n    Batch 530/875, Loss: 0.5069\n    Batch 540/875, Loss: 0.3247\n    Batch 550/875, Loss: 0.2409\n    Batch 560/875, Loss: 0.5382\n    Batch 570/875, Loss: 0.3056\n    Batch 580/875, Loss: 0.2267\n    Batch 590/875, Loss: 0.3607\n    Batch 600/875, Loss: 0.2377\n    Batch 610/875, Loss: 0.2399\n    Batch 620/875, Loss: 0.2365\n    Batch 630/875, Loss: 0.8537\n    Batch 640/875, Loss: 0.4409\n    Batch 650/875, Loss: 0.1929\n    Batch 660/875, Loss: 0.7509\n    Batch 670/875, Loss: 0.3727\n    Batch 680/875, Loss: 0.4204\n    Batch 690/875, Loss: 0.4063\n    Batch 700/875, Loss: 0.1312\n    Batch 710/875, Loss: 0.3460\n    Batch 720/875, Loss: 1.2360\n    Batch 730/875, Loss: 0.3202\n    Batch 740/875, Loss: 0.2304\n    Batch 750/875, Loss: 1.3835\n    Batch 760/875, Loss: 0.3672\n    Batch 770/875, Loss: 0.4074\n    Batch 780/875, Loss: 0.1510\n    Batch 790/875, Loss: 0.3805\n    Batch 800/875, Loss: 0.3382\n    Batch 810/875, Loss: 0.5057\n    Batch 820/875, Loss: 0.1020\n    Batch 830/875, Loss: 0.2536\n    Batch 840/875, Loss: 0.2563\n    Batch 850/875, Loss: 0.8779\n    Batch 860/875, Loss: 0.1025\n    Batch 870/875, Loss: 0.1750\n    Batch 875/875, Loss: 0.2763\n    DE Eval 50: LR=0.000085, Dropout=0.24, BS=8\n    First batch loaded, starting training...\n    Batch 10/875, Loss: 1.2453\n    Batch 20/875, Loss: 1.1310\n    Batch 30/875, Loss: 1.0263\n    Batch 40/875, Loss: 0.8394\n    Batch 50/875, Loss: 0.7375\n    Batch 60/875, Loss: 0.6945\n    Batch 70/875, Loss: 1.0682\n    Batch 80/875, Loss: 1.0807\n    Batch 90/875, Loss: 0.7945\n    Batch 100/875, Loss: 1.4139\n    Batch 110/875, Loss: 0.5031\n    Batch 120/875, Loss: 0.7140\n    Batch 130/875, Loss: 1.5379\n    Batch 140/875, Loss: 0.9108\n    Batch 150/875, Loss: 0.9418\n    Batch 160/875, Loss: 0.6022\n    Batch 170/875, Loss: 0.4488\n    Batch 180/875, Loss: 0.9799\n    Batch 190/875, Loss: 1.7665\n    Batch 200/875, Loss: 0.4889\n    Batch 210/875, Loss: 1.6632\n    Batch 220/875, Loss: 0.5403\n    Batch 230/875, Loss: 0.4720\n    Batch 240/875, Loss: 0.4798\n    Batch 250/875, Loss: 1.7711\n    Batch 260/875, Loss: 0.6974\n    Batch 270/875, Loss: 0.7458\n    Batch 280/875, Loss: 1.5758\n    Batch 290/875, Loss: 0.4373\n    Batch 300/875, Loss: 0.5897\n    Batch 310/875, Loss: 0.7727\n    Batch 320/875, Loss: 1.1757\n    Batch 330/875, Loss: 0.5734\n    Batch 340/875, Loss: 0.4096\n    Batch 350/875, Loss: 0.4010\n    Batch 360/875, Loss: 0.2957\n    Batch 370/875, Loss: 0.6204\n    Batch 380/875, Loss: 0.5004\n    Batch 390/875, Loss: 0.5821\n    Batch 400/875, Loss: 0.2825\n    Batch 410/875, Loss: 0.3640\n    Batch 420/875, Loss: 0.5726\n    Batch 430/875, Loss: 1.2195\n    Batch 440/875, Loss: 0.4843\n    Batch 450/875, Loss: 0.3420\n    Batch 460/875, Loss: 0.2921\n    Batch 470/875, Loss: 0.2604\n    Batch 480/875, Loss: 1.8289\n    Batch 490/875, Loss: 0.5029\n    Batch 500/875, Loss: 0.6941\n    Batch 510/875, Loss: 0.5679\n    Batch 520/875, Loss: 0.3900\n    Batch 530/875, Loss: 0.5196\n    Batch 540/875, Loss: 0.3641\n    Batch 550/875, Loss: 0.3065\n    Batch 560/875, Loss: 0.4298\n    Batch 570/875, Loss: 0.2297\n    Batch 580/875, Loss: 0.5128\n    Batch 590/875, Loss: 0.2884\n    Batch 600/875, Loss: 0.4439\n    Batch 610/875, Loss: 0.6580\n    Batch 620/875, Loss: 0.5198\n    Batch 630/875, Loss: 0.2269\n    Batch 640/875, Loss: 0.2566\n    Batch 650/875, Loss: 0.1715\n    Batch 660/875, Loss: 0.4357\n    Batch 670/875, Loss: 0.3394\n    Batch 680/875, Loss: 0.1732\n    Batch 690/875, Loss: 0.3215\n    Batch 700/875, Loss: 0.3770\n    Batch 710/875, Loss: 1.4394\n    Batch 720/875, Loss: 0.1835\n    Batch 730/875, Loss: 0.1256\n    Batch 740/875, Loss: 0.5275\n    Batch 750/875, Loss: 0.1044\n    Batch 760/875, Loss: 0.1937\n    Batch 770/875, Loss: 0.1365\n    Batch 780/875, Loss: 0.4834\n    Batch 790/875, Loss: 0.3641\n    Batch 800/875, Loss: 0.4260\n    Batch 810/875, Loss: 0.5217\n    Batch 820/875, Loss: 0.3400\n    Batch 830/875, Loss: 0.3020\n    Batch 840/875, Loss: 0.3737\n    Batch 850/875, Loss: 0.5475\n    Batch 860/875, Loss: 0.4813\n    Batch 870/875, Loss: 0.2657\n    Batch 875/875, Loss: 0.3131\n    DE Eval 51: LR=0.000107, Dropout=0.30, BS=8\n    First batch loaded, starting training...\n    Batch 10/875, Loss: 1.4029\n    Batch 20/875, Loss: 1.1614\n    Batch 30/875, Loss: 1.0466\n    Batch 40/875, Loss: 1.2530\n    Batch 50/875, Loss: 0.6012\n    Batch 60/875, Loss: 0.4313\n    Batch 70/875, Loss: 1.1144\n    Batch 80/875, Loss: 1.1104\n    Batch 90/875, Loss: 1.0002\n    Batch 100/875, Loss: 0.5267\n    Batch 110/875, Loss: 0.9926\n    Batch 120/875, Loss: 0.8566\n    Batch 130/875, Loss: 0.3218\n    Batch 140/875, Loss: 1.6571\n    Batch 150/875, Loss: 0.4143\n    Batch 160/875, Loss: 0.4131\n    Batch 170/875, Loss: 0.8427\n    Batch 180/875, Loss: 0.5763\n    Batch 190/875, Loss: 0.6633\n    Batch 200/875, Loss: 0.6982\n    Batch 210/875, Loss: 0.8006\n    Batch 220/875, Loss: 0.4164\n    Batch 230/875, Loss: 1.4158\n    Batch 240/875, Loss: 1.6359\n    Batch 250/875, Loss: 0.2701\n    Batch 260/875, Loss: 0.7691\n    Batch 270/875, Loss: 0.3327\n    Batch 280/875, Loss: 0.3177\n    Batch 290/875, Loss: 1.1515\n    Batch 300/875, Loss: 0.3165\n    Batch 310/875, Loss: 0.3851\n    Batch 320/875, Loss: 2.2799\n    Batch 330/875, Loss: 0.4130\n    Batch 340/875, Loss: 0.1904\n    Batch 350/875, Loss: 0.3830\n    Batch 360/875, Loss: 0.2174\n    Batch 370/875, Loss: 0.2861\n    Batch 380/875, Loss: 0.4725\n    Batch 390/875, Loss: 0.7154\n    Batch 400/875, Loss: 0.5220\n    Batch 410/875, Loss: 0.2517\n    Batch 420/875, Loss: 0.7370\n    Batch 430/875, Loss: 0.3497\n    Batch 440/875, Loss: 0.8488\n    Batch 450/875, Loss: 0.4924\n    Batch 460/875, Loss: 0.3372\n    Batch 470/875, Loss: 0.2308\n    Batch 480/875, Loss: 0.4425\n    Batch 490/875, Loss: 0.1563\n    Batch 500/875, Loss: 0.8961\n    Batch 510/875, Loss: 0.6512\n    Batch 520/875, Loss: 0.1408\n    Batch 530/875, Loss: 0.3106\n    Batch 540/875, Loss: 0.3343\n    Batch 550/875, Loss: 1.1430\n    Batch 560/875, Loss: 1.0191\n    Batch 570/875, Loss: 0.2399\n    Batch 580/875, Loss: 0.3951\n    Batch 590/875, Loss: 0.7463\n    Batch 600/875, Loss: 0.5956\n    Batch 610/875, Loss: 0.3445\n    Batch 620/875, Loss: 0.2183\n    Batch 630/875, Loss: 0.1658\n    Batch 640/875, Loss: 0.4405\n    Batch 650/875, Loss: 0.2972\n    Batch 660/875, Loss: 0.2817\n    Batch 670/875, Loss: 0.5483\n    Batch 680/875, Loss: 0.4784\n    Batch 690/875, Loss: 0.3060\n    Batch 700/875, Loss: 0.2331\n    Batch 710/875, Loss: 0.3543\n    Batch 720/875, Loss: 0.2528\n    Batch 730/875, Loss: 0.4471\n    Batch 740/875, Loss: 0.3235\n    Batch 750/875, Loss: 0.6064\n    Batch 760/875, Loss: 0.8620\n    Batch 770/875, Loss: 0.2834\n    Batch 780/875, Loss: 0.2062\n    Batch 790/875, Loss: 0.3475\n    Batch 800/875, Loss: 0.2152\n    Batch 810/875, Loss: 0.7318\n    Batch 820/875, Loss: 0.4671\n    Batch 830/875, Loss: 0.3796\n    Batch 840/875, Loss: 0.3078\n    Batch 850/875, Loss: 0.1900\n    Batch 860/875, Loss: 0.3999\n    Batch 870/875, Loss: 0.0894\n    Batch 875/875, Loss: 0.4112\n    DE Eval 52: LR=0.000030, Dropout=0.30, BS=8\n    First batch loaded, starting training...\n    Batch 10/875, Loss: 1.3991\n    Batch 20/875, Loss: 1.3280\n    Batch 30/875, Loss: 1.3701\n    Batch 40/875, Loss: 1.2552\n    Batch 50/875, Loss: 1.1498\n    Batch 60/875, Loss: 1.0579\n    Batch 70/875, Loss: 1.2117\n    Batch 80/875, Loss: 1.5839\n    Batch 90/875, Loss: 1.6752\n    Batch 100/875, Loss: 1.4827\n    Batch 110/875, Loss: 0.9202\n    Batch 120/875, Loss: 0.9156\n    Batch 130/875, Loss: 0.6746\n    Batch 140/875, Loss: 1.8849\n    Batch 150/875, Loss: 0.7383\n    Batch 160/875, Loss: 0.5439\n    Batch 170/875, Loss: 0.5644\n    Batch 180/875, Loss: 1.0823\n    Batch 190/875, Loss: 0.7164\n    Batch 200/875, Loss: 0.6887\n    Batch 210/875, Loss: 0.6593\n    Batch 220/875, Loss: 1.9377\n    Batch 230/875, Loss: 1.8500\n    Batch 240/875, Loss: 0.5555\n    Batch 250/875, Loss: 1.9084\n    Batch 260/875, Loss: 0.9519\n    Batch 270/875, Loss: 0.6376\n    Batch 280/875, Loss: 0.5544\n    Batch 290/875, Loss: 0.9964\n    Batch 300/875, Loss: 0.6305\n    Batch 310/875, Loss: 1.7158\n    Batch 320/875, Loss: 0.8818\n    Batch 330/875, Loss: 0.4192\n    Batch 340/875, Loss: 0.5456\n    Batch 350/875, Loss: 1.9871\n    Batch 360/875, Loss: 0.6053\n    Batch 370/875, Loss: 0.7984\n    Batch 380/875, Loss: 0.8382\n    Batch 390/875, Loss: 0.5158\n    Batch 400/875, Loss: 0.9804\n    Batch 410/875, Loss: 0.7997\n    Batch 420/875, Loss: 0.9720\n    Batch 430/875, Loss: 0.3752\n    Batch 440/875, Loss: 1.0190\n    Batch 450/875, Loss: 0.5024\n    Batch 460/875, Loss: 2.0181\n    Batch 470/875, Loss: 0.3259\n    Batch 480/875, Loss: 0.4875\n    Batch 490/875, Loss: 0.8255\n    Batch 500/875, Loss: 0.5324\n    Batch 510/875, Loss: 0.6653\n    Batch 520/875, Loss: 0.7607\n    Batch 530/875, Loss: 0.8224\n    Batch 540/875, Loss: 0.9911\n    Batch 550/875, Loss: 0.4052\n    Batch 560/875, Loss: 0.4509\n    Batch 570/875, Loss: 0.9301\n    Batch 580/875, Loss: 0.7788\n    Batch 590/875, Loss: 0.8244\n    Batch 600/875, Loss: 0.6943\n    Batch 610/875, Loss: 0.7442\n    Batch 620/875, Loss: 0.4372\n    Batch 630/875, Loss: 0.3218\n    Batch 640/875, Loss: 0.2994\n    Batch 650/875, Loss: 0.4529\n    Batch 660/875, Loss: 0.7941\n    Batch 670/875, Loss: 0.3961\n    Batch 680/875, Loss: 0.4196\n    Batch 690/875, Loss: 0.8003\n    Batch 700/875, Loss: 0.4907\n    Batch 710/875, Loss: 0.3657\n    Batch 720/875, Loss: 0.6584\n    Batch 730/875, Loss: 0.8242\n    Batch 740/875, Loss: 0.4455\n    Batch 750/875, Loss: 0.6545\n    Batch 760/875, Loss: 0.5210\n    Batch 770/875, Loss: 0.3500\n    Batch 780/875, Loss: 0.4585\n    Batch 790/875, Loss: 1.5562\n    Batch 800/875, Loss: 0.5933\n    Batch 810/875, Loss: 0.2968\n    Batch 820/875, Loss: 0.2597\n    Batch 830/875, Loss: 0.4101\n    Batch 840/875, Loss: 0.2983\n    Batch 850/875, Loss: 0.3338\n    Batch 860/875, Loss: 0.3835\n    Batch 870/875, Loss: 0.2566\n    Batch 875/875, Loss: 0.3100\n    DE Eval 53: LR=0.000549, Dropout=0.29, BS=8\n    First batch loaded, starting training...\n    Batch 10/875, Loss: 0.9368\n    Batch 20/875, Loss: 0.9692\n    Batch 30/875, Loss: 0.5726\n    Batch 40/875, Loss: 0.9171\n    Batch 50/875, Loss: 0.5558\n    Batch 60/875, Loss: 0.7959\n    Batch 70/875, Loss: 0.5423\n    Batch 80/875, Loss: 0.7275\n    Batch 90/875, Loss: 0.8280\n    Batch 100/875, Loss: 0.6993\n    Batch 110/875, Loss: 1.0091\n    Batch 120/875, Loss: 1.8624\n    Batch 130/875, Loss: 0.4452\n    Batch 140/875, Loss: 2.1001\n    Batch 150/875, Loss: 0.7614\n    Batch 160/875, Loss: 2.2428\n    Batch 170/875, Loss: 0.3430\n    Batch 180/875, Loss: 0.3422\n    Batch 190/875, Loss: 0.3461\n    Batch 200/875, Loss: 0.3636\n    Batch 210/875, Loss: 0.3536\n    Batch 220/875, Loss: 0.5762\n    Batch 230/875, Loss: 2.0025\n    Batch 240/875, Loss: 0.6668\n    Batch 250/875, Loss: 2.4603\n    Batch 260/875, Loss: 0.2148\n    Batch 270/875, Loss: 0.3735\n    Batch 280/875, Loss: 0.2404\n    Batch 290/875, Loss: 0.3351\n    Batch 300/875, Loss: 0.3858\n    Batch 310/875, Loss: 0.5548\n    Batch 320/875, Loss: 0.2569\n    Batch 330/875, Loss: 0.2925\n    Batch 340/875, Loss: 0.3272\n    Batch 350/875, Loss: 0.5196\n    Batch 360/875, Loss: 0.4749\n    Batch 370/875, Loss: 0.5652\n    Batch 380/875, Loss: 0.2205\n    Batch 390/875, Loss: 0.3488\n    Batch 400/875, Loss: 0.4420\n    Batch 410/875, Loss: 0.5435\n    Batch 420/875, Loss: 1.8820\n    Batch 430/875, Loss: 0.5853\n    Batch 440/875, Loss: 0.3923\n    Batch 450/875, Loss: 0.5034\n    Batch 460/875, Loss: 0.2081\n    Batch 470/875, Loss: 0.4382\n    Batch 480/875, Loss: 0.2104\n    Batch 490/875, Loss: 0.4398\n    Batch 500/875, Loss: 0.2273\n    Batch 510/875, Loss: 0.7819\n    Batch 520/875, Loss: 0.2957\n    Batch 530/875, Loss: 0.2527\n    Batch 540/875, Loss: 0.7256\n    Batch 550/875, Loss: 0.1689\n    Batch 560/875, Loss: 1.0966\n    Batch 570/875, Loss: 0.6010\n    Batch 580/875, Loss: 0.1818\n    Batch 590/875, Loss: 0.2907\n    Batch 600/875, Loss: 0.7485\n    Batch 610/875, Loss: 0.1506\n    Batch 620/875, Loss: 0.2196\n    Batch 630/875, Loss: 0.2839\n    Batch 640/875, Loss: 2.3576\n    Batch 650/875, Loss: 0.3789\n    Batch 660/875, Loss: 0.2904\n    Batch 670/875, Loss: 0.2032\n    Batch 680/875, Loss: 0.1994\n    Batch 690/875, Loss: 0.3699\n    Batch 700/875, Loss: 0.1589\n    Batch 710/875, Loss: 0.4308\n    Batch 720/875, Loss: 0.1827\n    Batch 730/875, Loss: 0.1715\n    Batch 740/875, Loss: 0.1316\n    Batch 750/875, Loss: 0.2343\n    Batch 760/875, Loss: 0.2251\n    Batch 770/875, Loss: 0.1418\n    Batch 780/875, Loss: 0.2114\n    Batch 790/875, Loss: 0.1932\n    Batch 800/875, Loss: 1.3355\n    Batch 810/875, Loss: 0.3232\n    Batch 820/875, Loss: 0.1696\n    Batch 830/875, Loss: 0.4290\n    Batch 840/875, Loss: 0.2835\n    Batch 850/875, Loss: 0.2459\n    Batch 860/875, Loss: 0.1678\n    Batch 870/875, Loss: 0.3564\n    Batch 875/875, Loss: 0.7802\n    DE Eval 54: LR=0.000158, Dropout=0.40, BS=8\n    First batch loaded, starting training...\n    Batch 10/875, Loss: 1.3784\n    Batch 20/875, Loss: 1.2044\n    Batch 30/875, Loss: 1.2520\n    Batch 40/875, Loss: 0.8222\n    Batch 50/875, Loss: 1.2236\n    Batch 60/875, Loss: 0.6052\n    Batch 70/875, Loss: 0.5364\n    Batch 80/875, Loss: 0.4765\n    Batch 90/875, Loss: 2.1670\n    Batch 100/875, Loss: 0.6764\n    Batch 110/875, Loss: 1.8332\n    Batch 120/875, Loss: 0.6567\n    Batch 130/875, Loss: 0.6268\n    Batch 140/875, Loss: 0.6322\n    Batch 150/875, Loss: 0.5448\n    Batch 160/875, Loss: 0.6438\n    Batch 170/875, Loss: 0.3713\n    Batch 180/875, Loss: 0.4031\n    Batch 190/875, Loss: 0.2519\n    Batch 200/875, Loss: 0.2957\n    Batch 210/875, Loss: 0.6440\n    Batch 220/875, Loss: 0.6668\n    Batch 230/875, Loss: 1.8623\n    Batch 240/875, Loss: 0.5568\n    Batch 250/875, Loss: 0.4580\n    Batch 260/875, Loss: 0.5085\n    Batch 270/875, Loss: 0.3482\n    Batch 280/875, Loss: 0.5143\n    Batch 290/875, Loss: 0.9778\n    Batch 300/875, Loss: 0.4471\n    Batch 310/875, Loss: 0.4639\n    Batch 320/875, Loss: 0.4522\n    Batch 330/875, Loss: 0.3817\n    Batch 340/875, Loss: 0.2078\n    Batch 350/875, Loss: 0.2084\n    Batch 360/875, Loss: 0.7885\n    Batch 370/875, Loss: 0.3877\n    Batch 380/875, Loss: 0.4825\n    Batch 390/875, Loss: 0.5824\n    Batch 400/875, Loss: 0.2020\n    Batch 410/875, Loss: 0.6210\n    Batch 420/875, Loss: 0.1658\n    Batch 430/875, Loss: 0.2486\n    Batch 440/875, Loss: 0.4001\n    Batch 450/875, Loss: 0.4839\n    Batch 460/875, Loss: 0.3630\n    Batch 470/875, Loss: 0.2770\n    Batch 480/875, Loss: 0.3875\n    Batch 490/875, Loss: 0.3689\n    Batch 500/875, Loss: 0.2870\n    Batch 510/875, Loss: 0.2243\n    Batch 520/875, Loss: 0.5508\n    Batch 530/875, Loss: 0.1466\n    Batch 540/875, Loss: 0.2637\n    Batch 550/875, Loss: 0.3929\n    Batch 560/875, Loss: 0.1655\n    Batch 570/875, Loss: 0.2294\n    Batch 580/875, Loss: 0.3054\n    Batch 590/875, Loss: 0.2883\n    Batch 600/875, Loss: 0.1752\n    Batch 610/875, Loss: 0.8254\n    Batch 620/875, Loss: 0.5092\n    Batch 630/875, Loss: 0.2238\n    Batch 640/875, Loss: 0.2339\n    Batch 650/875, Loss: 0.2938\n    Batch 660/875, Loss: 0.1738\n    Batch 670/875, Loss: 0.1607\n    Batch 680/875, Loss: 0.7702\n    Batch 690/875, Loss: 0.4478\n    Batch 700/875, Loss: 0.1182\n    Batch 710/875, Loss: 0.3254\n    Batch 720/875, Loss: 0.1651\n    Batch 730/875, Loss: 0.6968\n    Batch 740/875, Loss: 0.2622\n    Batch 750/875, Loss: 0.3293\n    Batch 760/875, Loss: 0.1045\n    Batch 770/875, Loss: 0.2695\n    Batch 780/875, Loss: 0.5443\n    Batch 790/875, Loss: 0.2795\n    Batch 800/875, Loss: 0.1694\n    Batch 810/875, Loss: 0.2734\n    Batch 820/875, Loss: 0.2090\n    Batch 830/875, Loss: 0.4190\n    Batch 840/875, Loss: 0.5401\n    Batch 850/875, Loss: 0.4863\n    Batch 860/875, Loss: 0.2330\n    Batch 870/875, Loss: 0.1704\n    Batch 875/875, Loss: 0.2784\n    DE Eval 55: LR=0.000384, Dropout=0.32, BS=8\n    First batch loaded, starting training...\n    Batch 10/875, Loss: 1.5420\n    Batch 20/875, Loss: 1.1579\n    Batch 30/875, Loss: 1.8607\n    Batch 40/875, Loss: 1.0769\n    Batch 50/875, Loss: 1.2600\n    Batch 60/875, Loss: 1.5423\n    Batch 70/875, Loss: 0.8451\n    Batch 80/875, Loss: 0.8800\n    Batch 90/875, Loss: 0.5923\n    Batch 100/875, Loss: 1.5975\n    Batch 110/875, Loss: 0.7453\n    Batch 120/875, Loss: 1.1421\n    Batch 130/875, Loss: 0.3249\n    Batch 140/875, Loss: 0.3617\n    Batch 150/875, Loss: 0.3182\n    Batch 160/875, Loss: 0.5431\n    Batch 170/875, Loss: 0.2360\n    Batch 180/875, Loss: 0.6548\n    Batch 190/875, Loss: 0.6206\n    Batch 200/875, Loss: 0.2777\n    Batch 210/875, Loss: 0.4479\n    Batch 220/875, Loss: 0.7434\n    Batch 230/875, Loss: 0.4291\n    Batch 240/875, Loss: 0.2456\n    Batch 250/875, Loss: 0.3302\n    Batch 260/875, Loss: 0.5904\n    Batch 270/875, Loss: 0.3502\n    Batch 280/875, Loss: 2.3596\n    Batch 290/875, Loss: 0.8162\n    Batch 300/875, Loss: 0.9037\n    Batch 310/875, Loss: 0.3509\n    Batch 320/875, Loss: 0.6111\n    Batch 330/875, Loss: 0.5355\n    Batch 340/875, Loss: 0.1682\n    Batch 350/875, Loss: 0.2155\n    Batch 360/875, Loss: 0.8270\n    Batch 370/875, Loss: 0.8223\n    Batch 380/875, Loss: 0.5355\n    Batch 390/875, Loss: 0.2421\n    Batch 400/875, Loss: 0.3209\n    Batch 410/875, Loss: 0.3010\n    Batch 420/875, Loss: 0.1952\n    Batch 430/875, Loss: 0.7682\n    Batch 440/875, Loss: 0.2011\n    Batch 450/875, Loss: 0.1377\n    Batch 460/875, Loss: 0.4699\n    Batch 470/875, Loss: 0.2347\n    Batch 480/875, Loss: 0.5029\n    Batch 490/875, Loss: 0.9113\n    Batch 500/875, Loss: 0.4892\n    Batch 510/875, Loss: 0.4162\n    Batch 520/875, Loss: 0.2380\n    Batch 530/875, Loss: 0.2280\n    Batch 540/875, Loss: 0.7376\n    Batch 550/875, Loss: 0.8445\n    Batch 560/875, Loss: 0.3986\n    Batch 570/875, Loss: 0.5272\n    Batch 580/875, Loss: 1.0589\n    Batch 590/875, Loss: 0.2536\n    Batch 600/875, Loss: 0.4145\n    Batch 610/875, Loss: 0.8307\n    Batch 620/875, Loss: 0.2391\n    Batch 630/875, Loss: 0.6877\n    Batch 640/875, Loss: 0.4364\n    Batch 650/875, Loss: 0.2151\n    Batch 660/875, Loss: 0.3900\n    Batch 670/875, Loss: 0.3805\n    Batch 680/875, Loss: 0.2710\n    Batch 690/875, Loss: 0.1553\n    Batch 700/875, Loss: 0.2175\n    Batch 710/875, Loss: 0.0886\n    Batch 720/875, Loss: 0.4687\n    Batch 730/875, Loss: 0.1489\n    Batch 740/875, Loss: 0.2540\n    Batch 750/875, Loss: 0.2135\n    Batch 760/875, Loss: 0.2912\n    Batch 770/875, Loss: 0.2253\n    Batch 780/875, Loss: 0.3690\n    Batch 790/875, Loss: 0.3031\n    Batch 800/875, Loss: 0.2600\n    Batch 810/875, Loss: 0.0652\n    Batch 820/875, Loss: 0.2893\n    Batch 830/875, Loss: 0.3018\n    Batch 840/875, Loss: 0.2357\n    Batch 850/875, Loss: 0.3686\n    Batch 860/875, Loss: 0.3157\n    Batch 870/875, Loss: 0.1053\n    Batch 875/875, Loss: 0.5066\n    DE Eval 56: LR=0.000078, Dropout=0.27, BS=8\n    First batch loaded, starting training...\n    Batch 10/875, Loss: 1.2580\n    Batch 20/875, Loss: 1.1788\n    Batch 30/875, Loss: 1.5704\n    Batch 40/875, Loss: 1.0003\n    Batch 50/875, Loss: 0.7709\n    Batch 60/875, Loss: 1.1083\n    Batch 70/875, Loss: 1.8928\n    Batch 80/875, Loss: 0.9514\n    Batch 90/875, Loss: 0.9053\n    Batch 100/875, Loss: 0.5374\n    Batch 110/875, Loss: 0.3353\n    Batch 120/875, Loss: 0.6881\n    Batch 130/875, Loss: 1.4071\n    Batch 140/875, Loss: 0.5671\n    Batch 150/875, Loss: 0.9635\n    Batch 160/875, Loss: 0.4399\n    Batch 170/875, Loss: 1.0127\n    Batch 180/875, Loss: 0.6233\n    Batch 190/875, Loss: 0.4865\n    Batch 200/875, Loss: 0.3818\n    Batch 210/875, Loss: 0.5110\n    Batch 220/875, Loss: 0.4986\n    Batch 230/875, Loss: 0.7630\n    Batch 240/875, Loss: 0.3153\n    Batch 250/875, Loss: 0.7400\n    Batch 260/875, Loss: 0.5802\n    Batch 270/875, Loss: 0.5606\n    Batch 280/875, Loss: 0.4237\n    Batch 290/875, Loss: 0.4448\n    Batch 300/875, Loss: 0.7274\n    Batch 310/875, Loss: 0.7611\n    Batch 320/875, Loss: 0.5074\n    Batch 330/875, Loss: 0.5481\n    Batch 340/875, Loss: 0.3355\n    Batch 350/875, Loss: 0.4811\n    Batch 360/875, Loss: 0.7889\n    Batch 370/875, Loss: 0.5677\n    Batch 380/875, Loss: 1.2175\n    Batch 390/875, Loss: 0.4630\n    Batch 400/875, Loss: 0.6857\n    Batch 410/875, Loss: 0.2534\n    Batch 420/875, Loss: 0.4230\n    Batch 430/875, Loss: 0.2206\n    Batch 440/875, Loss: 0.3308\n    Batch 450/875, Loss: 0.2365\n    Batch 460/875, Loss: 0.4866\n    Batch 470/875, Loss: 0.2411\n    Batch 480/875, Loss: 0.2384\n    Batch 490/875, Loss: 0.3337\n    Batch 500/875, Loss: 0.6170\n    Batch 510/875, Loss: 0.4153\n    Batch 520/875, Loss: 0.2396\n    Batch 530/875, Loss: 0.3355\n    Batch 540/875, Loss: 0.2705\n    Batch 550/875, Loss: 1.8956\n    Batch 560/875, Loss: 0.1992\n    Batch 570/875, Loss: 0.3681\n    Batch 580/875, Loss: 0.8055\n    Batch 590/875, Loss: 0.4893\n    Batch 600/875, Loss: 0.1744\n    Batch 610/875, Loss: 0.4016\n    Batch 620/875, Loss: 0.3678\n    Batch 630/875, Loss: 0.2093\n    Batch 640/875, Loss: 0.1538\n    Batch 650/875, Loss: 1.4897\n    Batch 660/875, Loss: 1.6018\n    Batch 670/875, Loss: 0.4712\n    Batch 680/875, Loss: 0.3237\n    Batch 690/875, Loss: 0.3263\n    Batch 700/875, Loss: 0.2404\n    Batch 710/875, Loss: 0.3209\n    Batch 720/875, Loss: 0.5364\n    Batch 730/875, Loss: 0.3664\n    Batch 740/875, Loss: 0.2060\n    Batch 750/875, Loss: 0.4093\n    Batch 760/875, Loss: 0.2338\n    Batch 770/875, Loss: 0.2740\n    Batch 780/875, Loss: 0.1357\n    Batch 790/875, Loss: 1.0368\n    Batch 800/875, Loss: 1.9503\n    Batch 810/875, Loss: 1.2070\n    Batch 820/875, Loss: 0.3484\n    Batch 830/875, Loss: 0.3820\n    Batch 840/875, Loss: 0.1705\n    Batch 850/875, Loss: 0.1517\n    Batch 860/875, Loss: 0.3525\n    Batch 870/875, Loss: 0.1923\n    Batch 875/875, Loss: 0.2566\n    DE Eval 57: LR=0.000578, Dropout=0.58, BS=8\n    First batch loaded, starting training...\n    Batch 10/875, Loss: 1.3069\n    Batch 20/875, Loss: 1.7003\n    Batch 30/875, Loss: 1.6581\n    Batch 40/875, Loss: 0.7638\n    Batch 50/875, Loss: 1.1194\n    Batch 60/875, Loss: 0.2783\n    Batch 70/875, Loss: 0.4062\n    Batch 80/875, Loss: 0.9981\n    Batch 90/875, Loss: 0.4102\n    Batch 100/875, Loss: 0.5248\n    Batch 110/875, Loss: 0.6209\n    Batch 120/875, Loss: 0.9792\n    Batch 130/875, Loss: 1.6591\n    Batch 140/875, Loss: 0.7222\n    Batch 150/875, Loss: 0.5454\n    Batch 160/875, Loss: 0.7716\n    Batch 170/875, Loss: 0.4909\n    Batch 180/875, Loss: 0.5348\n    Batch 190/875, Loss: 0.8790\n    Batch 200/875, Loss: 0.4637\n    Batch 210/875, Loss: 0.9825\n    Batch 220/875, Loss: 0.5834\n    Batch 230/875, Loss: 0.6087\n    Batch 240/875, Loss: 0.4631\n    Batch 250/875, Loss: 0.6714\n    Batch 260/875, Loss: 0.4578\n    Batch 270/875, Loss: 0.3999\n    Batch 280/875, Loss: 0.3306\n    Batch 290/875, Loss: 1.6281\n    Batch 300/875, Loss: 0.4396\n    Batch 310/875, Loss: 0.8087\n    Batch 320/875, Loss: 0.2934\n    Batch 330/875, Loss: 0.5487\n    Batch 340/875, Loss: 0.8404\n    Batch 350/875, Loss: 0.5386\n    Batch 360/875, Loss: 0.4256\n    Batch 370/875, Loss: 0.2817\n    Batch 380/875, Loss: 0.3068\n    Batch 390/875, Loss: 0.1677\n    Batch 400/875, Loss: 0.2008\n    Batch 410/875, Loss: 0.3134\n    Batch 420/875, Loss: 0.3640\n    Batch 430/875, Loss: 0.3645\n    Batch 440/875, Loss: 0.4614\n    Batch 450/875, Loss: 0.4210\n    Batch 460/875, Loss: 0.2859\n    Batch 470/875, Loss: 0.6353\n    Batch 480/875, Loss: 0.5981\n    Batch 490/875, Loss: 0.5499\n    Batch 500/875, Loss: 0.1968\n    Batch 510/875, Loss: 0.5064\n    Batch 520/875, Loss: 0.6368\n    Batch 530/875, Loss: 0.1794\n    Batch 540/875, Loss: 0.5580\n    Batch 550/875, Loss: 1.5254\n    Batch 560/875, Loss: 1.0066\n    Batch 570/875, Loss: 0.4267\n    Batch 580/875, Loss: 0.4572\n    Batch 590/875, Loss: 0.2401\n    Batch 600/875, Loss: 0.1427\n    Batch 610/875, Loss: 0.5812\n    Batch 620/875, Loss: 0.1136\n    Batch 630/875, Loss: 0.2417\n    Batch 640/875, Loss: 0.3236\n    Batch 650/875, Loss: 0.8613\n    Batch 660/875, Loss: 0.5839\n    Batch 670/875, Loss: 0.2388\n    Batch 680/875, Loss: 0.1900\n    Batch 690/875, Loss: 1.1401\n    Batch 700/875, Loss: 0.2213\n    Batch 710/875, Loss: 0.2996\n    Batch 720/875, Loss: 0.1418\n    Batch 730/875, Loss: 1.3476\n    Batch 740/875, Loss: 0.4847\n    Batch 750/875, Loss: 0.4468\n    Batch 760/875, Loss: 0.2837\n    Batch 770/875, Loss: 0.3365\n    Batch 780/875, Loss: 0.1871\n    Batch 790/875, Loss: 0.9472\n    Batch 800/875, Loss: 0.3889\n    Batch 810/875, Loss: 0.4306\n    Batch 820/875, Loss: 1.3507\n    Batch 830/875, Loss: 0.4821\n    Batch 840/875, Loss: 0.2574\n    Batch 850/875, Loss: 0.3387\n    Batch 860/875, Loss: 0.2524\n    Batch 870/875, Loss: 0.1843\n    Batch 875/875, Loss: 0.4152\n    DE Eval 58: LR=0.000086, Dropout=0.00, BS=8\n    First batch loaded, starting training...\n    Batch 10/875, Loss: 1.3139\n    Batch 20/875, Loss: 1.1129\n    Batch 30/875, Loss: 1.1617\n    Batch 40/875, Loss: 1.1084\n    Batch 50/875, Loss: 1.0592\n    Batch 60/875, Loss: 1.1306\n    Batch 70/875, Loss: 1.9054\n    Batch 80/875, Loss: 0.7882\n    Batch 90/875, Loss: 0.7142\n    Batch 100/875, Loss: 0.3877\n    Batch 110/875, Loss: 0.6759\n    Batch 120/875, Loss: 0.3423\n    Batch 130/875, Loss: 0.9816\n    Batch 140/875, Loss: 0.7708\n    Batch 150/875, Loss: 0.3940\n    Batch 160/875, Loss: 0.5275\n    Batch 170/875, Loss: 0.8488\n    Batch 180/875, Loss: 0.3683\n    Batch 190/875, Loss: 0.5134\n    Batch 200/875, Loss: 0.9464\n    Batch 210/875, Loss: 0.3191\n    Batch 220/875, Loss: 0.6231\n    Batch 230/875, Loss: 0.2498\n    Batch 240/875, Loss: 0.5664\n    Batch 250/875, Loss: 0.3437\n    Batch 260/875, Loss: 0.4442\n    Batch 270/875, Loss: 0.6743\n    Batch 280/875, Loss: 0.3962\n    Batch 290/875, Loss: 0.2848\n    Batch 300/875, Loss: 0.3360\n    Batch 310/875, Loss: 0.4434\n    Batch 320/875, Loss: 0.3101\n    Batch 330/875, Loss: 0.3705\n    Batch 340/875, Loss: 0.3512\n    Batch 350/875, Loss: 0.4790\n    Batch 360/875, Loss: 0.1494\n    Batch 370/875, Loss: 0.3699\n    Batch 380/875, Loss: 0.3643\n    Batch 390/875, Loss: 0.4364\n    Batch 400/875, Loss: 1.4953\n    Batch 410/875, Loss: 0.9797\n    Batch 420/875, Loss: 0.7186\n    Batch 430/875, Loss: 0.3135\n    Batch 440/875, Loss: 0.2644\n    Batch 450/875, Loss: 0.4928\n    Batch 460/875, Loss: 0.7277\n    Batch 470/875, Loss: 0.7114\n    Batch 480/875, Loss: 0.6008\n    Batch 490/875, Loss: 0.9486\n    Batch 500/875, Loss: 0.2500\n    Batch 510/875, Loss: 0.2067\n    Batch 520/875, Loss: 1.5558\n    Batch 530/875, Loss: 0.1716\n    Batch 540/875, Loss: 0.5468\n    Batch 550/875, Loss: 0.2525\n    Batch 560/875, Loss: 1.2206\n    Batch 570/875, Loss: 0.1915\n    Batch 580/875, Loss: 0.2483\n    Batch 590/875, Loss: 0.2091\n    Batch 600/875, Loss: 0.6143\n    Batch 610/875, Loss: 0.3972\n    Batch 620/875, Loss: 0.2404\n    Batch 630/875, Loss: 0.4539\n    Batch 640/875, Loss: 0.3240\n    Batch 650/875, Loss: 0.4510\n    Batch 660/875, Loss: 0.3052\n    Batch 670/875, Loss: 0.5588\n    Batch 680/875, Loss: 0.1929\n    Batch 690/875, Loss: 0.3031\n    Batch 700/875, Loss: 0.4371\n    Batch 710/875, Loss: 0.2704\n    Batch 720/875, Loss: 0.2178\n    Batch 730/875, Loss: 0.2581\n    Batch 740/875, Loss: 0.3208\n    Batch 750/875, Loss: 0.3454\n    Batch 760/875, Loss: 0.4133\n    Batch 770/875, Loss: 0.1859\n    Batch 780/875, Loss: 0.1340\n    Batch 790/875, Loss: 0.2452\n    Batch 800/875, Loss: 0.1891\n    Batch 810/875, Loss: 0.2100\n    Batch 820/875, Loss: 0.2318\n    Batch 830/875, Loss: 0.1102\n    Batch 840/875, Loss: 0.1891\n    Batch 850/875, Loss: 0.1651\n    Batch 860/875, Loss: 0.2447\n    Batch 870/875, Loss: 0.3575\n    Batch 875/875, Loss: 0.1975\n    DE Eval 59: LR=0.000259, Dropout=0.60, BS=8\n    First batch loaded, starting training...\n    Batch 10/875, Loss: 1.3268\n    Batch 20/875, Loss: 1.2146\n    Batch 30/875, Loss: 0.3996\n    Batch 40/875, Loss: 0.7667\n    Batch 50/875, Loss: 0.5443\n    Batch 60/875, Loss: 1.7208\n    Batch 70/875, Loss: 0.6055\n    Batch 80/875, Loss: 1.7423\n    Batch 90/875, Loss: 1.4415\n    Batch 100/875, Loss: 0.6600\n    Batch 110/875, Loss: 0.2790\n    Batch 120/875, Loss: 0.3967\n    Batch 130/875, Loss: 0.8694\n    Batch 140/875, Loss: 0.2898\n    Batch 150/875, Loss: 0.6109\n    Batch 160/875, Loss: 0.6727\n    Batch 170/875, Loss: 1.9075\n    Batch 180/875, Loss: 0.5040\n    Batch 190/875, Loss: 0.4725\n    Batch 200/875, Loss: 0.4075\n    Batch 210/875, Loss: 2.0621\n    Batch 220/875, Loss: 0.7665\n    Batch 230/875, Loss: 0.5848\n    Batch 240/875, Loss: 0.7719\n    Batch 250/875, Loss: 0.5006\n    Batch 260/875, Loss: 0.2589\n    Batch 270/875, Loss: 1.5454\n    Batch 280/875, Loss: 0.4259\n    Batch 290/875, Loss: 1.4926\n    Batch 300/875, Loss: 1.3648\n    Batch 310/875, Loss: 0.2294\n    Batch 320/875, Loss: 0.6819\n    Batch 330/875, Loss: 0.7801\n    Batch 340/875, Loss: 0.3717\n    Batch 350/875, Loss: 0.7983\n    Batch 360/875, Loss: 0.8629\n    Batch 370/875, Loss: 0.8191\n    Batch 380/875, Loss: 1.3466\n    Batch 390/875, Loss: 0.5625\n    Batch 400/875, Loss: 0.7193\n    Batch 410/875, Loss: 0.5729\n    Batch 420/875, Loss: 0.7988\n    Batch 430/875, Loss: 0.4183\n    Batch 440/875, Loss: 0.2445\n    Batch 450/875, Loss: 1.2174\n    Batch 460/875, Loss: 0.5144\n    Batch 470/875, Loss: 0.4641\n    Batch 480/875, Loss: 0.8442\n    Batch 490/875, Loss: 0.1232\n    Batch 500/875, Loss: 0.7469\n    Batch 510/875, Loss: 0.3937\n    Batch 520/875, Loss: 0.9621\n    Batch 530/875, Loss: 1.4098\n    Batch 540/875, Loss: 0.3814\n    Batch 550/875, Loss: 0.4677\n    Batch 560/875, Loss: 0.3408\n    Batch 570/875, Loss: 0.2433\n    Batch 580/875, Loss: 0.1058\n    Batch 590/875, Loss: 0.3169\n    Batch 600/875, Loss: 0.3132\n    Batch 610/875, Loss: 0.6076\n    Batch 620/875, Loss: 0.3481\n    Batch 630/875, Loss: 0.1696\n    Batch 640/875, Loss: 0.2346\n    Batch 650/875, Loss: 0.4080\n    Batch 660/875, Loss: 0.1775\n    Batch 670/875, Loss: 0.9146\n    Batch 680/875, Loss: 0.4085\n    Batch 690/875, Loss: 0.6567\n    Batch 700/875, Loss: 0.5352\n    Batch 710/875, Loss: 0.1343\n    Batch 720/875, Loss: 0.1902\n    Batch 730/875, Loss: 0.2718\n    Batch 740/875, Loss: 0.3186\n    Batch 750/875, Loss: 0.2155\n    Batch 760/875, Loss: 0.3091\n    Batch 770/875, Loss: 0.2772\n    Batch 780/875, Loss: 2.2308\n    Batch 790/875, Loss: 0.3142\n    Batch 800/875, Loss: 0.2295\n    Batch 810/875, Loss: 0.4835\n    Batch 820/875, Loss: 0.9516\n    Batch 830/875, Loss: 0.2643\n    Batch 840/875, Loss: 0.4501\n    Batch 850/875, Loss: 0.4748\n    Batch 860/875, Loss: 0.2222\n    Batch 870/875, Loss: 0.4944\n    Batch 875/875, Loss: 0.1950\n    DE Eval 60: LR=0.000017, Dropout=0.09, BS=8\n    First batch loaded, starting training...\n    Batch 10/875, Loss: 1.3618\n    Batch 20/875, Loss: 1.3516\n    Batch 30/875, Loss: 1.3157\n    Batch 40/875, Loss: 1.3591\n    Batch 50/875, Loss: 1.2607\n    Batch 60/875, Loss: 1.2547\n    Batch 70/875, Loss: 1.2414\n    Batch 80/875, Loss: 1.2005\n    Batch 90/875, Loss: 1.1635\n    Batch 100/875, Loss: 0.9610\n    Batch 110/875, Loss: 1.6252\n    Batch 120/875, Loss: 1.0764\n    Batch 130/875, Loss: 0.8466\n    Batch 140/875, Loss: 1.0209\n    Batch 150/875, Loss: 0.6740\n    Batch 160/875, Loss: 0.7938\n    Batch 170/875, Loss: 1.8169\n    Batch 180/875, Loss: 1.7658\n    Batch 190/875, Loss: 0.9989\n    Batch 200/875, Loss: 1.0200\n    Batch 210/875, Loss: 0.5851\n    Batch 220/875, Loss: 0.9739\n    Batch 230/875, Loss: 0.6017\n    Batch 240/875, Loss: 1.8668\n    Batch 250/875, Loss: 1.1379\n    Batch 260/875, Loss: 0.6373\n    Batch 270/875, Loss: 0.7016\n    Batch 280/875, Loss: 1.1645\n    Batch 290/875, Loss: 0.7130\n    Batch 300/875, Loss: 0.6801\n    Batch 310/875, Loss: 1.1913\n    Batch 320/875, Loss: 0.6286\n    Batch 330/875, Loss: 0.8652\n    Batch 340/875, Loss: 0.4717\n    Batch 350/875, Loss: 0.6159\n    Batch 360/875, Loss: 0.6224\n    Batch 370/875, Loss: 0.9539\n    Batch 380/875, Loss: 0.9923\n    Batch 390/875, Loss: 1.9133\n    Batch 400/875, Loss: 0.5853\n    Batch 410/875, Loss: 0.5997\n    Batch 420/875, Loss: 0.6220\n    Batch 430/875, Loss: 0.5187\n    Batch 440/875, Loss: 0.4752\n    Batch 450/875, Loss: 0.5144\n    Batch 460/875, Loss: 0.8673\n    Batch 470/875, Loss: 0.5455\n    Batch 480/875, Loss: 1.9191\n    Batch 490/875, Loss: 0.5730\n    Batch 500/875, Loss: 0.4679\n    Batch 510/875, Loss: 0.8499\n    Batch 520/875, Loss: 0.8381\n    Batch 530/875, Loss: 0.4886\n    Batch 540/875, Loss: 0.6674\n    Batch 550/875, Loss: 0.5645\n    Batch 560/875, Loss: 0.4003\n    Batch 570/875, Loss: 0.5201\n    Batch 580/875, Loss: 0.5333\n    Batch 590/875, Loss: 0.8491\n    Batch 600/875, Loss: 0.5662\n    Batch 610/875, Loss: 0.8511\n    Batch 620/875, Loss: 0.3913\n    Batch 630/875, Loss: 0.8992\n    Batch 640/875, Loss: 0.4539\n    Batch 650/875, Loss: 0.5193\n    Batch 660/875, Loss: 0.5417\n    Batch 670/875, Loss: 0.5416\n    Batch 680/875, Loss: 0.4614\n    Batch 690/875, Loss: 0.3484\n    Batch 700/875, Loss: 0.3988\n    Batch 710/875, Loss: 0.5207\n    Batch 720/875, Loss: 0.4068\n    Batch 730/875, Loss: 0.8411\n    Batch 740/875, Loss: 0.8970\n    Batch 750/875, Loss: 0.4785\n    Batch 760/875, Loss: 0.8550\n    Batch 770/875, Loss: 0.5354\n    Batch 780/875, Loss: 0.5933\n    Batch 790/875, Loss: 0.8456\n    Batch 800/875, Loss: 0.6639\n    Batch 810/875, Loss: 1.6291\n    Batch 820/875, Loss: 0.4419\n    Batch 830/875, Loss: 0.3881\n    Batch 840/875, Loss: 1.8094\n    Batch 850/875, Loss: 0.4660\n    Batch 860/875, Loss: 0.3954\n    Batch 870/875, Loss: 0.8267\n    Batch 875/875, Loss: 0.7036\n  DE Iter 4/4: Best = 0.7978\n\nDE Result: Score=0.7978, Time=2474.9s\n  Best Hyperparams: {'lr': np.float64(0.00010737168288322857), 'weight_decay': np.float64(0.005581826577230304), 'dropout': np.float64(0.29981082567262096), 'unfreeze_epoch': 0, 'batch_size': 8, 'augment_strength': np.float64(1.1080169655574243)}\n\n--- Running GWO ---\n    GWO Eval 1: LR=0.000019, Dropout=0.08, BS=8\n    First batch loaded, starting training...\n    Batch 10/875, Loss: 1.3431\n    Batch 20/875, Loss: 1.3462\n    Batch 30/875, Loss: 1.2293\n    Batch 40/875, Loss: 1.2406\n    Batch 50/875, Loss: 1.4901\n    Batch 60/875, Loss: 1.2108\n    Batch 70/875, Loss: 1.1235\n    Batch 80/875, Loss: 1.0899\n    Batch 90/875, Loss: 1.0072\n    Batch 100/875, Loss: 0.9999\n    Batch 110/875, Loss: 1.1266\n    Batch 120/875, Loss: 1.0257\n    Batch 130/875, Loss: 1.1281\n    Batch 140/875, Loss: 1.0733\n    Batch 150/875, Loss: 1.0127\n    Batch 160/875, Loss: 1.8245\n    Batch 170/875, Loss: 0.7034\n    Batch 180/875, Loss: 1.0466\n    Batch 190/875, Loss: 1.8301\n    Batch 200/875, Loss: 0.8350\n    Batch 210/875, Loss: 0.6699\n    Batch 220/875, Loss: 0.7006\n    Batch 230/875, Loss: 1.0700\n    Batch 240/875, Loss: 0.7038\n    Batch 250/875, Loss: 0.9792\n    Batch 260/875, Loss: 1.2860\n    Batch 270/875, Loss: 0.8708\n    Batch 280/875, Loss: 0.6242\n    Batch 290/875, Loss: 0.5713\n    Batch 300/875, Loss: 1.2631\n    Batch 310/875, Loss: 0.5821\n    Batch 320/875, Loss: 0.9496\n    Batch 330/875, Loss: 0.6521\n    Batch 340/875, Loss: 0.6845\n    Batch 350/875, Loss: 0.7485\n    Batch 360/875, Loss: 0.4536\n    Batch 370/875, Loss: 0.4458\n    Batch 380/875, Loss: 0.5599\n    Batch 390/875, Loss: 0.9097\n    Batch 400/875, Loss: 0.6196\n    Batch 410/875, Loss: 1.9213\n    Batch 420/875, Loss: 0.5506\n    Batch 430/875, Loss: 0.3632\n    Batch 440/875, Loss: 0.8705\n    Batch 450/875, Loss: 0.6541\n    Batch 460/875, Loss: 0.8359\n    Batch 470/875, Loss: 2.0800\n    Batch 480/875, Loss: 0.6655\n    Batch 490/875, Loss: 1.9131\n    Batch 500/875, Loss: 0.4258\n    Batch 510/875, Loss: 0.6181\n    Batch 520/875, Loss: 0.5045\n    Batch 530/875, Loss: 0.9515\n    Batch 540/875, Loss: 0.7561\n    Batch 550/875, Loss: 0.4799\n    Batch 560/875, Loss: 0.9014\n    Batch 570/875, Loss: 0.5738\n    Batch 580/875, Loss: 0.6980\n    Batch 590/875, Loss: 0.5639\n    Batch 600/875, Loss: 0.7697\n    Batch 610/875, Loss: 0.8951\n    Batch 620/875, Loss: 0.6201\n    Batch 630/875, Loss: 0.3096\n    Batch 640/875, Loss: 1.8636\n    Batch 650/875, Loss: 0.5189\n    Batch 660/875, Loss: 0.6818\n    Batch 670/875, Loss: 1.7669\n    Batch 680/875, Loss: 0.4538\n    Batch 690/875, Loss: 0.4916\n    Batch 700/875, Loss: 0.8264\n    Batch 710/875, Loss: 0.7026\n    Batch 720/875, Loss: 0.6082\n    Batch 730/875, Loss: 0.7090\n    Batch 740/875, Loss: 1.8715\n    Batch 750/875, Loss: 0.5298\n    Batch 760/875, Loss: 0.6032\n    Batch 770/875, Loss: 1.6003\n    Batch 780/875, Loss: 0.5519\n    Batch 790/875, Loss: 0.5080\n    Batch 800/875, Loss: 0.3812\n    Batch 810/875, Loss: 0.7611\n    Batch 820/875, Loss: 0.6813\n    Batch 830/875, Loss: 0.4184\n    Batch 840/875, Loss: 0.6442\n    Batch 850/875, Loss: 0.3968\n    Batch 860/875, Loss: 0.7309\n    Batch 870/875, Loss: 0.8071\n    Batch 875/875, Loss: 0.4562\n    GWO Eval 2: LR=0.000447, Dropout=0.02, BS=16\n    First batch loaded, starting training...\n    Batch 10/438, Loss: 1.1879\n    Batch 20/438, Loss: 1.7582\n    Batch 30/438, Loss: 0.5965\n    Batch 40/438, Loss: 0.5969\n    Batch 50/438, Loss: 1.1272\n    Batch 60/438, Loss: 0.9793\n    Batch 70/438, Loss: 1.1000\n    Batch 80/438, Loss: 1.0775\n    Batch 90/438, Loss: 0.5579\n    Batch 100/438, Loss: 0.5238\n    Batch 110/438, Loss: 0.9718\n    Batch 120/438, Loss: 0.7338\n    Batch 130/438, Loss: 1.2092\n    Batch 140/438, Loss: 0.6350\n    Batch 150/438, Loss: 0.6129\n    Batch 160/438, Loss: 1.1616\n    Batch 170/438, Loss: 0.5381\n    Batch 180/438, Loss: 0.4359\n    Batch 190/438, Loss: 0.6420\n    Batch 200/438, Loss: 1.0775\n    Batch 210/438, Loss: 0.6340\n    Batch 220/438, Loss: 0.3827\n    Batch 230/438, Loss: 0.9517\n    Batch 240/438, Loss: 1.0883\n    Batch 250/438, Loss: 0.6287\n    Batch 260/438, Loss: 0.6253\n    Batch 270/438, Loss: 1.0478\n    Batch 280/438, Loss: 0.6665\n    Batch 290/438, Loss: 0.6287\n    Batch 300/438, Loss: 0.5986\n    Batch 310/438, Loss: 0.7758\n    Batch 320/438, Loss: 0.6484\n    Batch 330/438, Loss: 0.7389\n    Batch 340/438, Loss: 0.4388\n    Batch 350/438, Loss: 0.7393\n    Batch 360/438, Loss: 0.4613\n    Batch 370/438, Loss: 0.6117\n    Batch 380/438, Loss: 0.5081\n    Batch 390/438, Loss: 0.6800\n    Batch 400/438, Loss: 0.4286\n    Batch 410/438, Loss: 0.6376\n    Batch 420/438, Loss: 0.8602\n    Batch 430/438, Loss: 0.6808\n    Batch 438/438, Loss: 1.5549\n    GWO Eval 3: LR=0.000043, Dropout=0.52, BS=16\n    First batch loaded, starting training...\n    Batch 10/438, Loss: 1.3614\n    Batch 20/438, Loss: 1.2549\n    Batch 30/438, Loss: 1.1611\n    Batch 40/438, Loss: 0.9907\n    Batch 50/438, Loss: 0.9494\n    Batch 60/438, Loss: 0.9929\n    Batch 70/438, Loss: 0.8899\n    Batch 80/438, Loss: 0.6496\n    Batch 90/438, Loss: 1.0591\n    Batch 100/438, Loss: 0.6185\n    Batch 110/438, Loss: 0.5470\n    Batch 120/438, Loss: 0.5655\n    Batch 130/438, Loss: 0.5393\n    Batch 140/438, Loss: 1.0468\n    Batch 150/438, Loss: 0.6091\n    Batch 160/438, Loss: 0.8400\n    Batch 170/438, Loss: 1.4589\n    Batch 180/438, Loss: 0.7256\n    Batch 190/438, Loss: 1.3461\n    Batch 200/438, Loss: 1.0851\n    Batch 210/438, Loss: 0.6754\n    Batch 220/438, Loss: 0.8448\n    Batch 230/438, Loss: 0.9796\n    Batch 240/438, Loss: 0.9648\n    Batch 250/438, Loss: 0.6608\n    Batch 260/438, Loss: 0.3514\n    Batch 270/438, Loss: 0.5871\n    Batch 280/438, Loss: 0.3750\n    Batch 290/438, Loss: 0.5200\n    Batch 300/438, Loss: 0.9005\n    Batch 310/438, Loss: 0.7523\n    Batch 320/438, Loss: 0.4905\n    Batch 330/438, Loss: 0.6416\n    Batch 340/438, Loss: 0.8643\n    Batch 350/438, Loss: 0.7502\n    Batch 360/438, Loss: 0.7304\n    Batch 370/438, Loss: 0.3618\n    Batch 380/438, Loss: 0.4669\n    Batch 390/438, Loss: 0.3341\n    Batch 400/438, Loss: 0.5982\n    Batch 410/438, Loss: 0.3893\n    Batch 420/438, Loss: 1.0458\n    Batch 430/438, Loss: 0.7550\n    Batch 438/438, Loss: 0.7597\n    GWO Eval 4: LR=0.000015, Dropout=0.51, BS=16\n    First batch loaded, starting training...\n    Batch 10/438, Loss: 1.2933\n    Batch 20/438, Loss: 1.3356\n    Batch 30/438, Loss: 1.3516\n    Batch 40/438, Loss: 1.2464\n    Batch 50/438, Loss: 1.2600\n    Batch 60/438, Loss: 1.3303\n    Batch 70/438, Loss: 1.2964\n    Batch 80/438, Loss: 1.1913\n    Batch 90/438, Loss: 1.2506\n    Batch 100/438, Loss: 1.4162\n    Batch 110/438, Loss: 1.1806\n    Batch 120/438, Loss: 1.1567\n    Batch 130/438, Loss: 1.1002\n    Batch 140/438, Loss: 1.2686\n    Batch 150/438, Loss: 1.0385\n    Batch 160/438, Loss: 0.9597\n    Batch 170/438, Loss: 1.0985\n    Batch 180/438, Loss: 1.4723\n    Batch 190/438, Loss: 1.2526\n    Batch 200/438, Loss: 1.0262\n    Batch 210/438, Loss: 1.3516\n    Batch 220/438, Loss: 0.8993\n    Batch 230/438, Loss: 0.8809\n    Batch 240/438, Loss: 1.2811\n    Batch 250/438, Loss: 1.0857\n    Batch 260/438, Loss: 1.0213\n    Batch 270/438, Loss: 1.1122\n    Batch 280/438, Loss: 1.0128\n    Batch 290/438, Loss: 0.8497\n    Batch 300/438, Loss: 1.2240\n    Batch 310/438, Loss: 1.5191\n    Batch 320/438, Loss: 0.9245\n    Batch 330/438, Loss: 0.9759\n    Batch 340/438, Loss: 0.6976\n    Batch 350/438, Loss: 0.9165\n    Batch 360/438, Loss: 1.0583\n    Batch 370/438, Loss: 1.0203\n    Batch 380/438, Loss: 1.3390\n    Batch 390/438, Loss: 0.8361\n    Batch 400/438, Loss: 0.8030\n    Batch 410/438, Loss: 0.9453\n    Batch 420/438, Loss: 1.0855\n    Batch 430/438, Loss: 0.9220\n    Batch 438/438, Loss: 2.0019\n    GWO Eval 5: LR=0.000127, Dropout=0.05, BS=8\n    First batch loaded, starting training...\n    Batch 10/875, Loss: 1.3140\n    Batch 20/875, Loss: 1.2280\n    Batch 30/875, Loss: 1.8817\n    Batch 40/875, Loss: 1.1630\n    Batch 50/875, Loss: 0.9944\n    Batch 60/875, Loss: 1.3293\n    Batch 70/875, Loss: 1.3082\n    Batch 80/875, Loss: 0.8551\n    Batch 90/875, Loss: 1.2782\n    Batch 100/875, Loss: 0.6976\n    Batch 110/875, Loss: 1.1327\n    Batch 120/875, Loss: 0.7663\n    Batch 130/875, Loss: 1.1249\n    Batch 140/875, Loss: 0.9194\n    Batch 150/875, Loss: 1.6370\n    Batch 160/875, Loss: 0.6569\n    Batch 170/875, Loss: 1.1851\n    Batch 180/875, Loss: 0.7714\n    Batch 190/875, Loss: 0.8281\n    Batch 200/875, Loss: 1.2578\n    Batch 210/875, Loss: 0.6887\n    Batch 220/875, Loss: 0.8817\n    Batch 230/875, Loss: 0.6319\n    Batch 240/875, Loss: 0.9409\n    Batch 250/875, Loss: 0.7018\n    Batch 260/875, Loss: 0.7292\n    Batch 270/875, Loss: 0.9563\n    Batch 280/875, Loss: 0.8705\n    Batch 290/875, Loss: 0.8524\n    Batch 300/875, Loss: 0.9658\n    Batch 310/875, Loss: 0.7734\n    Batch 320/875, Loss: 0.7179\n    Batch 330/875, Loss: 0.9539\n    Batch 340/875, Loss: 1.0000\n    Batch 350/875, Loss: 1.0740\n    Batch 360/875, Loss: 0.9110\n    Batch 370/875, Loss: 0.9342\n    Batch 380/875, Loss: 0.9093\n    Batch 390/875, Loss: 1.0895\n    Batch 400/875, Loss: 0.5832\n    Batch 410/875, Loss: 1.1430\n    Batch 420/875, Loss: 0.8461\n    Batch 430/875, Loss: 0.7849\n    Batch 440/875, Loss: 0.6432\n    Batch 450/875, Loss: 0.6012\n    Batch 460/875, Loss: 1.0447\n    Batch 470/875, Loss: 1.1332\n    Batch 480/875, Loss: 0.8241\n    Batch 490/875, Loss: 0.8379\n    Batch 500/875, Loss: 0.8446\n    Batch 510/875, Loss: 1.7244\n    Batch 520/875, Loss: 1.5926\n    Batch 530/875, Loss: 0.6408\n    Batch 540/875, Loss: 1.2035\n    Batch 550/875, Loss: 0.7953\n    Batch 560/875, Loss: 0.7674\n    Batch 570/875, Loss: 0.9151\n    Batch 580/875, Loss: 1.0977\n    Batch 590/875, Loss: 1.3389\n    Batch 600/875, Loss: 0.5363\n    Batch 610/875, Loss: 0.4089\n    Batch 620/875, Loss: 0.5780\n    Batch 630/875, Loss: 0.8489\n    Batch 640/875, Loss: 0.7509\n    Batch 650/875, Loss: 0.6350\n    Batch 660/875, Loss: 0.6539\n    Batch 670/875, Loss: 0.6154\n    Batch 680/875, Loss: 0.7439\n    Batch 690/875, Loss: 0.7884\n    Batch 700/875, Loss: 0.3888\n    Batch 710/875, Loss: 0.5848\n    Batch 720/875, Loss: 0.6494\n    Batch 730/875, Loss: 0.9138\n    Batch 740/875, Loss: 0.4563\n    Batch 750/875, Loss: 0.5295\n    Batch 760/875, Loss: 0.8502\n    Batch 770/875, Loss: 1.8348\n    Batch 780/875, Loss: 0.5745\n    Batch 790/875, Loss: 0.3421\n    Batch 800/875, Loss: 0.3010\n    Batch 810/875, Loss: 0.4873\n    Batch 820/875, Loss: 0.5550\n    Batch 830/875, Loss: 0.6476\n    Batch 840/875, Loss: 1.8200\n    Batch 850/875, Loss: 0.4741\n    Batch 860/875, Loss: 0.5404\n    Batch 870/875, Loss: 0.4260\n    Batch 875/875, Loss: 0.6351\n    GWO Eval 6: LR=0.000010, Dropout=0.29, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 1.3448\n    Batch 20/219, Loss: 1.2982\n    Batch 30/219, Loss: 1.2736\n    Batch 40/219, Loss: 1.2276\n    Batch 50/219, Loss: 1.2247\n    Batch 60/219, Loss: 1.1421\n    Batch 70/219, Loss: 1.1679\n    Batch 80/219, Loss: 1.1266\n    Batch 90/219, Loss: 1.3612\n    Batch 100/219, Loss: 1.3054\n    Batch 110/219, Loss: 1.2459\n    Batch 120/219, Loss: 1.0125\n    Batch 130/219, Loss: 1.0341\n    Batch 140/219, Loss: 1.2111\n    Batch 150/219, Loss: 1.0543\n    Batch 160/219, Loss: 1.0292\n    Batch 170/219, Loss: 1.0481\n    Batch 180/219, Loss: 1.0397\n    Batch 190/219, Loss: 1.3070\n    Batch 200/219, Loss: 1.1207\n    Batch 210/219, Loss: 1.1505\n    Batch 219/219, Loss: 1.4748\n    GWO Eval 7: LR=0.001797, Dropout=0.06, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.7939\n    Batch 20/219, Loss: 0.4028\n    Batch 30/219, Loss: 0.3431\n    Batch 40/219, Loss: 0.6586\n    Batch 50/219, Loss: 0.3686\n    Batch 60/219, Loss: 0.5676\n    Batch 70/219, Loss: 0.3313\n    Batch 80/219, Loss: 0.4359\n    Batch 90/219, Loss: 0.4254\n    Batch 100/219, Loss: 0.2218\n    Batch 110/219, Loss: 0.1501\n    Batch 120/219, Loss: 1.4440\n    Batch 130/219, Loss: 0.6097\n    Batch 140/219, Loss: 0.5408\n    Batch 150/219, Loss: 0.6622\n    Batch 160/219, Loss: 0.3366\n    Batch 170/219, Loss: 0.2789\n    Batch 180/219, Loss: 0.2845\n    Batch 190/219, Loss: 0.3031\n    Batch 200/219, Loss: 0.7449\n    Batch 210/219, Loss: 0.4690\n    Batch 219/219, Loss: 0.4110\n    GWO Eval 8: LR=0.001389, Dropout=0.38, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.8188\n    Batch 20/219, Loss: 0.6185\n    Batch 30/219, Loss: 0.4750\n    Batch 40/219, Loss: 0.3997\n    Batch 50/219, Loss: 0.4976\n    Batch 60/219, Loss: 0.6711\n    Batch 70/219, Loss: 0.2711\n    Batch 80/219, Loss: 0.6306\n    Batch 90/219, Loss: 0.3235\n    Batch 100/219, Loss: 0.4120\n    Batch 110/219, Loss: 0.3156\n    Batch 120/219, Loss: 0.2456\n    Batch 130/219, Loss: 0.3021\n    Batch 140/219, Loss: 0.4165\n    Batch 150/219, Loss: 0.7487\n    Batch 160/219, Loss: 0.3091\n    Batch 170/219, Loss: 0.7169\n    Batch 180/219, Loss: 0.2872\n    Batch 190/219, Loss: 0.2746\n    Batch 200/219, Loss: 0.2553\n    Batch 210/219, Loss: 0.3099\n    Batch 219/219, Loss: 0.2464\n    GWO Eval 9: LR=0.000100, Dropout=0.44, BS=16\n    First batch loaded, starting training...\n    Batch 10/438, Loss: 1.2496\n    Batch 20/438, Loss: 1.0321\n    Batch 30/438, Loss: 1.1038\n    Batch 40/438, Loss: 0.8327\n    Batch 50/438, Loss: 1.1875\n    Batch 60/438, Loss: 0.9731\n    Batch 70/438, Loss: 1.6132\n    Batch 80/438, Loss: 1.0469\n    Batch 90/438, Loss: 0.9263\n    Batch 100/438, Loss: 1.0600\n    Batch 110/438, Loss: 0.9820\n    Batch 120/438, Loss: 0.8175\n    Batch 130/438, Loss: 1.9026\n    Batch 140/438, Loss: 0.9209\n    Batch 150/438, Loss: 1.4797\n    Batch 160/438, Loss: 0.8962\n    Batch 170/438, Loss: 1.1031\n    Batch 180/438, Loss: 1.3215\n    Batch 190/438, Loss: 0.7873\n    Batch 200/438, Loss: 0.9211\n    Batch 210/438, Loss: 0.7787\n    Batch 220/438, Loss: 0.7142\n    Batch 230/438, Loss: 1.1803\n    Batch 240/438, Loss: 0.8232\n    Batch 250/438, Loss: 0.9147\n    Batch 260/438, Loss: 1.4665\n    Batch 270/438, Loss: 0.4979\n    Batch 280/438, Loss: 0.8991\n    Batch 290/438, Loss: 0.8035\n    Batch 300/438, Loss: 0.6904\n    Batch 310/438, Loss: 0.9960\n    Batch 320/438, Loss: 0.9998\n    Batch 330/438, Loss: 1.0084\n    Batch 340/438, Loss: 1.8389\n    Batch 350/438, Loss: 0.6426\n    Batch 360/438, Loss: 0.6871\n    Batch 370/438, Loss: 0.7036\n    Batch 380/438, Loss: 1.1055\n    Batch 390/438, Loss: 0.8475\n    Batch 400/438, Loss: 0.5976\n    Batch 410/438, Loss: 0.6889\n    Batch 420/438, Loss: 0.8211\n    Batch 430/438, Loss: 0.8219\n    Batch 438/438, Loss: 0.8015\n    GWO Eval 10: LR=0.000632, Dropout=0.40, BS=16\n    First batch loaded, starting training...\n    Batch 10/438, Loss: 0.7310\n    Batch 20/438, Loss: 1.0994\n    Batch 30/438, Loss: 0.7789\n    Batch 40/438, Loss: 0.8382\n    Batch 50/438, Loss: 1.0114\n    Batch 60/438, Loss: 0.5029\n    Batch 70/438, Loss: 1.0177\n    Batch 80/438, Loss: 0.7839\n    Batch 90/438, Loss: 0.4829\n    Batch 100/438, Loss: 0.5140\n    Batch 110/438, Loss: 1.0036\n    Batch 120/438, Loss: 0.6976\n    Batch 130/438, Loss: 0.7684\n    Batch 140/438, Loss: 0.4505\n    Batch 150/438, Loss: 0.4305\n    Batch 160/438, Loss: 1.5069\n    Batch 170/438, Loss: 0.4009\n    Batch 180/438, Loss: 1.0639\n    Batch 190/438, Loss: 1.4328\n    Batch 200/438, Loss: 0.5823\n    Batch 210/438, Loss: 0.5819\n    Batch 220/438, Loss: 1.2297\n    Batch 230/438, Loss: 0.3132\n    Batch 240/438, Loss: 0.9039\n    Batch 250/438, Loss: 0.6875\n    Batch 260/438, Loss: 0.8728\n    Batch 270/438, Loss: 0.4737\n    Batch 280/438, Loss: 0.7672\n    Batch 290/438, Loss: 0.5603\n    Batch 300/438, Loss: 1.3061\n    Batch 310/438, Loss: 0.4240\n    Batch 320/438, Loss: 0.7969\n    Batch 330/438, Loss: 0.5144\n    Batch 340/438, Loss: 0.8672\n    Batch 350/438, Loss: 0.3733\n    Batch 360/438, Loss: 0.3692\n    Batch 370/438, Loss: 0.8536\n    Batch 380/438, Loss: 0.8971\n    Batch 390/438, Loss: 0.7155\n    Batch 400/438, Loss: 0.6065\n    Batch 410/438, Loss: 0.7950\n    Batch 420/438, Loss: 0.8984\n    Batch 430/438, Loss: 0.7995\n    Batch 438/438, Loss: 0.2415\n    GWO Eval 11: LR=0.000032, Dropout=0.48, BS=16\n    First batch loaded, starting training...\n    Batch 10/438, Loss: 1.3821\n    Batch 20/438, Loss: 1.3515\n    Batch 30/438, Loss: 1.3285\n    Batch 40/438, Loss: 1.1647\n    Batch 50/438, Loss: 1.1565\n    Batch 60/438, Loss: 1.1524\n    Batch 70/438, Loss: 1.0294\n    Batch 80/438, Loss: 0.8487\n    Batch 90/438, Loss: 1.1972\n    Batch 100/438, Loss: 0.7769\n    Batch 110/438, Loss: 0.8898\n    Batch 120/438, Loss: 0.7887\n    Batch 130/438, Loss: 0.7834\n    Batch 140/438, Loss: 0.6755\n    Batch 150/438, Loss: 0.8218\n    Batch 160/438, Loss: 0.5573\n    Batch 170/438, Loss: 0.5454\n    Batch 180/438, Loss: 0.5258\n    Batch 190/438, Loss: 0.7060\n    Batch 200/438, Loss: 0.6836\n    Batch 210/438, Loss: 0.5153\n    Batch 220/438, Loss: 0.9260\n    Batch 230/438, Loss: 0.4116\n    Batch 240/438, Loss: 1.0012\n    Batch 250/438, Loss: 0.9041\n    Batch 260/438, Loss: 0.5537\n    Batch 270/438, Loss: 0.3629\n    Batch 280/438, Loss: 0.4796\n    Batch 290/438, Loss: 0.4359\n    Batch 300/438, Loss: 0.9192\n    Batch 310/438, Loss: 0.9579\n    Batch 320/438, Loss: 0.3851\n    Batch 330/438, Loss: 0.4618\n    Batch 340/438, Loss: 0.6102\n    Batch 350/438, Loss: 0.9846\n    Batch 360/438, Loss: 0.3682\n    Batch 370/438, Loss: 0.3513\n    Batch 380/438, Loss: 0.5518\n    Batch 390/438, Loss: 0.5959\n    Batch 400/438, Loss: 0.4707\n    Batch 410/438, Loss: 0.4547\n    Batch 420/438, Loss: 0.4287\n    Batch 430/438, Loss: 0.8524\n    Batch 438/438, Loss: 0.4540\n    GWO Eval 12: LR=0.000023, Dropout=0.19, BS=16\n    First batch loaded, starting training...\n    Batch 10/438, Loss: 1.3695\n    Batch 20/438, Loss: 1.2915\n    Batch 30/438, Loss: 1.2888\n    Batch 40/438, Loss: 1.1243\n    Batch 50/438, Loss: 1.1303\n    Batch 60/438, Loss: 1.0669\n    Batch 70/438, Loss: 0.8480\n    Batch 80/438, Loss: 0.9758\n    Batch 90/438, Loss: 0.6362\n    Batch 100/438, Loss: 1.0775\n    Batch 110/438, Loss: 1.0440\n    Batch 120/438, Loss: 0.7412\n    Batch 130/438, Loss: 0.8034\n    Batch 140/438, Loss: 0.6528\n    Batch 150/438, Loss: 0.9862\n    Batch 160/438, Loss: 1.1173\n    Batch 170/438, Loss: 1.2636\n    Batch 180/438, Loss: 0.4835\n    Batch 190/438, Loss: 1.4497\n    Batch 200/438, Loss: 0.7409\n    Batch 210/438, Loss: 0.5185\n    Batch 220/438, Loss: 1.0389\n    Batch 230/438, Loss: 0.9406\n    Batch 240/438, Loss: 2.0623\n    Batch 250/438, Loss: 0.5519\n    Batch 260/438, Loss: 0.6766\n    Batch 270/438, Loss: 0.6337\n    Batch 280/438, Loss: 0.7037\n    Batch 290/438, Loss: 0.5029\n    Batch 300/438, Loss: 0.6419\n    Batch 310/438, Loss: 1.0070\n    Batch 320/438, Loss: 0.4652\n    Batch 330/438, Loss: 0.6649\n    Batch 340/438, Loss: 0.6169\n    Batch 350/438, Loss: 0.6564\n    Batch 360/438, Loss: 0.4934\n    Batch 370/438, Loss: 0.3890\n    Batch 380/438, Loss: 0.4405\n    Batch 390/438, Loss: 1.0599\n    Batch 400/438, Loss: 0.4003\n    Batch 410/438, Loss: 0.5032\n    Batch 420/438, Loss: 1.1909\n    Batch 430/438, Loss: 0.3672\n    Batch 438/438, Loss: 0.3966\n    GWO Eval 13: LR=0.000713, Dropout=0.34, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 1.0123\n    Batch 20/219, Loss: 0.8584\n    Batch 30/219, Loss: 0.8233\n    Batch 40/219, Loss: 0.7852\n    Batch 50/219, Loss: 1.3492\n    Batch 60/219, Loss: 0.7792\n    Batch 70/219, Loss: 0.8725\n    Batch 80/219, Loss: 0.7291\n    Batch 90/219, Loss: 0.8784\n    Batch 100/219, Loss: 1.4996\n    Batch 110/219, Loss: 0.7994\n    Batch 120/219, Loss: 0.6469\n    Batch 130/219, Loss: 0.6989\n    Batch 140/219, Loss: 0.8599\n    Batch 150/219, Loss: 0.8218\n    Batch 160/219, Loss: 0.5646\n    Batch 170/219, Loss: 0.5663\n    Batch 180/219, Loss: 0.6744\n    Batch 190/219, Loss: 0.4399\n    Batch 200/219, Loss: 0.5865\n    Batch 210/219, Loss: 0.8705\n    Batch 219/219, Loss: 1.1329\n    GWO Eval 14: LR=0.003000, Dropout=0.29, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.8677\n    Batch 20/219, Loss: 0.9819\n    Batch 30/219, Loss: 0.5480\n    Batch 40/219, Loss: 0.5714\n    Batch 50/219, Loss: 0.5458\n    Batch 60/219, Loss: 0.4443\n    Batch 70/219, Loss: 0.7876\n    Batch 80/219, Loss: 0.5103\n    Batch 90/219, Loss: 0.8141\n    Batch 100/219, Loss: 0.6329\n    Batch 110/219, Loss: 0.8328\n    Batch 120/219, Loss: 0.3850\n    Batch 130/219, Loss: 0.3422\n    Batch 140/219, Loss: 0.3617\n    Batch 150/219, Loss: 0.2875\n    Batch 160/219, Loss: 0.4921\n    Batch 170/219, Loss: 0.5332\n    Batch 180/219, Loss: 0.2467\n    Batch 190/219, Loss: 0.7153\n    Batch 200/219, Loss: 0.3675\n    Batch 210/219, Loss: 0.2364\n    Batch 219/219, Loss: 0.7452\n    GWO Eval 15: LR=0.000991, Dropout=0.57, BS=16\n    First batch loaded, starting training...\n    Batch 10/438, Loss: 1.0504\n    Batch 20/438, Loss: 0.8919\n    Batch 30/438, Loss: 1.1526\n    Batch 40/438, Loss: 1.1364\n    Batch 50/438, Loss: 0.6717\n    Batch 60/438, Loss: 0.6446\n    Batch 70/438, Loss: 0.9070\n    Batch 80/438, Loss: 1.0547\n    Batch 90/438, Loss: 1.1173\n    Batch 100/438, Loss: 0.5882\n    Batch 110/438, Loss: 0.5453\n    Batch 120/438, Loss: 1.0472\n    Batch 130/438, Loss: 0.6341\n    Batch 140/438, Loss: 0.8492\n    Batch 150/438, Loss: 0.5546\n    Batch 160/438, Loss: 0.5132\n    Batch 170/438, Loss: 0.4462\n    Batch 180/438, Loss: 0.7130\n    Batch 190/438, Loss: 0.3860\n    Batch 200/438, Loss: 1.1639\n    Batch 210/438, Loss: 0.9303\n    Batch 220/438, Loss: 0.5453\n    Batch 230/438, Loss: 0.7409\n    Batch 240/438, Loss: 0.8257\n    Batch 250/438, Loss: 0.8175\n    Batch 260/438, Loss: 0.6294\n    Batch 270/438, Loss: 0.7388\n    Batch 280/438, Loss: 0.5675\n    Batch 290/438, Loss: 0.9193\n    Batch 300/438, Loss: 0.8500\n    Batch 310/438, Loss: 0.8298\n    Batch 320/438, Loss: 0.6161\n    Batch 330/438, Loss: 1.9722\n    Batch 340/438, Loss: 0.6947\n    Batch 350/438, Loss: 1.0624\n    Batch 360/438, Loss: 0.8915\n    Batch 370/438, Loss: 0.5679\n    Batch 380/438, Loss: 0.4376\n    Batch 390/438, Loss: 0.6779\n    Batch 400/438, Loss: 0.8499\n    Batch 410/438, Loss: 0.6786\n    Batch 420/438, Loss: 0.6045\n    Batch 430/438, Loss: 1.4949\n    Batch 438/438, Loss: 1.4864\n    GWO Eval 16: LR=0.002680, Dropout=0.31, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 1.4268\n    Batch 20/219, Loss: 0.8066\n    Batch 30/219, Loss: 0.7651\n    Batch 40/219, Loss: 0.5708\n    Batch 50/219, Loss: 0.6226\n    Batch 60/219, Loss: 0.7686\n    Batch 70/219, Loss: 0.4478\n    Batch 80/219, Loss: 0.4673\n    Batch 90/219, Loss: 0.4250\n    Batch 100/219, Loss: 0.5431\n    Batch 110/219, Loss: 0.3724\n    Batch 120/219, Loss: 0.3599\n    Batch 130/219, Loss: 0.3269\n    Batch 140/219, Loss: 0.3922\n    Batch 150/219, Loss: 0.6909\n    Batch 160/219, Loss: 0.4721\n    Batch 170/219, Loss: 0.2549\n    Batch 180/219, Loss: 0.2658\n    Batch 190/219, Loss: 0.4360\n    Batch 200/219, Loss: 0.2991\n    Batch 210/219, Loss: 0.3212\n    Batch 219/219, Loss: 0.5235\n    GWO Eval 17: LR=0.003000, Dropout=0.39, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.7303\n    Batch 20/219, Loss: 0.9198\n    Batch 30/219, Loss: 0.7245\n    Batch 40/219, Loss: 0.9553\n    Batch 50/219, Loss: 1.0623\n    Batch 60/219, Loss: 0.8701\n    Batch 70/219, Loss: 0.7702\n    Batch 80/219, Loss: 0.7986\n    Batch 90/219, Loss: 0.8622\n    Batch 100/219, Loss: 0.9402\n    Batch 110/219, Loss: 0.7268\n    Batch 120/219, Loss: 0.7524\n    Batch 130/219, Loss: 0.5226\n    Batch 140/219, Loss: 0.5608\n    Batch 150/219, Loss: 0.6340\n    Batch 160/219, Loss: 0.6139\n    Batch 170/219, Loss: 0.8544\n    Batch 180/219, Loss: 0.5864\n    Batch 190/219, Loss: 0.6731\n    Batch 200/219, Loss: 0.5175\n    Batch 210/219, Loss: 0.5867\n    Batch 219/219, Loss: 0.5579\n    GWO Eval 18: LR=0.000715, Dropout=0.45, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.7642\n    Batch 20/219, Loss: 0.5585\n    Batch 30/219, Loss: 0.6109\n    Batch 40/219, Loss: 0.4755\n    Batch 50/219, Loss: 0.5032\n    Batch 60/219, Loss: 0.2903\n    Batch 70/219, Loss: 0.3178\n    Batch 80/219, Loss: 0.3238\n    Batch 90/219, Loss: 0.4138\n    Batch 100/219, Loss: 0.4257\n    Batch 110/219, Loss: 0.2830\n    Batch 120/219, Loss: 0.4963\n    Batch 130/219, Loss: 0.2407\n    Batch 140/219, Loss: 0.3920\n    Batch 150/219, Loss: 0.3992\n    Batch 160/219, Loss: 0.3089\n    Batch 170/219, Loss: 0.2660\n    Batch 180/219, Loss: 0.4200\n    Batch 190/219, Loss: 0.1755\n    Batch 200/219, Loss: 0.3923\n    Batch 210/219, Loss: 0.1436\n    Batch 219/219, Loss: 0.1995\n    GWO Eval 19: LR=0.001611, Dropout=0.43, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.6785\n    Batch 20/219, Loss: 0.6999\n    Batch 30/219, Loss: 0.8056\n    Batch 40/219, Loss: 0.6281\n    Batch 50/219, Loss: 0.6723\n    Batch 60/219, Loss: 0.5782\n    Batch 70/219, Loss: 0.7076\n    Batch 80/219, Loss: 0.8964\n    Batch 90/219, Loss: 0.5901\n    Batch 100/219, Loss: 0.7211\n    Batch 110/219, Loss: 0.5436\n    Batch 120/219, Loss: 0.7421\n    Batch 130/219, Loss: 0.9730\n    Batch 140/219, Loss: 0.5703\n    Batch 150/219, Loss: 0.7118\n    Batch 160/219, Loss: 0.5651\n    Batch 170/219, Loss: 0.7257\n    Batch 180/219, Loss: 0.6404\n    Batch 190/219, Loss: 0.5603\n    Batch 200/219, Loss: 0.5090\n    Batch 210/219, Loss: 0.6379\n    Batch 219/219, Loss: 0.6833\n    GWO Eval 20: LR=0.003000, Dropout=0.20, BS=16\n    First batch loaded, starting training...\n    Batch 10/438, Loss: 0.7832\n    Batch 20/438, Loss: 1.0339\n    Batch 30/438, Loss: 0.5028\n    Batch 40/438, Loss: 0.8441\n    Batch 50/438, Loss: 0.6525\n    Batch 60/438, Loss: 0.8527\n    Batch 70/438, Loss: 0.8884\n    Batch 80/438, Loss: 0.5895\n    Batch 90/438, Loss: 0.6006\n    Batch 100/438, Loss: 0.4589\n    Batch 110/438, Loss: 0.6772\n    Batch 120/438, Loss: 0.5408\n    Batch 130/438, Loss: 0.4972\n    Batch 140/438, Loss: 0.5285\n    Batch 150/438, Loss: 0.5757\n    Batch 160/438, Loss: 0.6210\n    Batch 170/438, Loss: 0.3837\n    Batch 180/438, Loss: 0.8466\n    Batch 190/438, Loss: 0.3481\n    Batch 200/438, Loss: 0.7013\n    Batch 210/438, Loss: 0.7193\n    Batch 220/438, Loss: 1.3634\n    Batch 230/438, Loss: 1.5966\n    Batch 240/438, Loss: 0.5229\n    Batch 250/438, Loss: 0.5558\n    Batch 260/438, Loss: 0.6053\n    Batch 270/438, Loss: 0.4817\n    Batch 280/438, Loss: 0.5075\n    Batch 290/438, Loss: 0.8459\n    Batch 300/438, Loss: 1.3765\n    Batch 310/438, Loss: 0.4062\n    Batch 320/438, Loss: 0.7226\n    Batch 330/438, Loss: 0.4675\n    Batch 340/438, Loss: 0.4914\n    Batch 350/438, Loss: 1.0168\n    Batch 360/438, Loss: 0.5546\n    Batch 370/438, Loss: 0.7114\n    Batch 380/438, Loss: 0.5796\n    Batch 390/438, Loss: 0.3902\n    Batch 400/438, Loss: 0.8566\n    Batch 410/438, Loss: 1.1966\n    Batch 420/438, Loss: 0.5740\n    Batch 430/438, Loss: 0.9098\n    Batch 438/438, Loss: 0.3338\n    GWO Eval 21: LR=0.000147, Dropout=0.43, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 1.0908\n    Batch 20/219, Loss: 1.2477\n    Batch 30/219, Loss: 0.8707\n    Batch 40/219, Loss: 0.7435\n    Batch 50/219, Loss: 0.8810\n    Batch 60/219, Loss: 0.8314\n    Batch 70/219, Loss: 0.8592\n    Batch 80/219, Loss: 0.6906\n    Batch 90/219, Loss: 0.9443\n    Batch 100/219, Loss: 0.8883\n    Batch 110/219, Loss: 0.8952\n    Batch 120/219, Loss: 0.8442\n    Batch 130/219, Loss: 1.0116\n    Batch 140/219, Loss: 0.6791\n    Batch 150/219, Loss: 0.7879\n    Batch 160/219, Loss: 0.7914\n    Batch 170/219, Loss: 0.7660\n    Batch 180/219, Loss: 0.9614\n    Batch 190/219, Loss: 0.6978\n    Batch 200/219, Loss: 1.0263\n    Batch 210/219, Loss: 0.7817\n    Batch 219/219, Loss: 0.5619\n    GWO Eval 22: LR=0.001142, Dropout=0.19, BS=16\n    First batch loaded, starting training...\n    Batch 10/438, Loss: 0.5706\n    Batch 20/438, Loss: 1.0221\n    Batch 30/438, Loss: 0.7131\n    Batch 40/438, Loss: 0.8366\n    Batch 50/438, Loss: 2.0142\n    Batch 60/438, Loss: 0.7751\n    Batch 70/438, Loss: 0.5687\n    Batch 80/438, Loss: 0.7567\n    Batch 90/438, Loss: 0.6209\n    Batch 100/438, Loss: 0.5092\n    Batch 110/438, Loss: 0.7351\n    Batch 120/438, Loss: 1.0281\n    Batch 130/438, Loss: 0.6124\n    Batch 140/438, Loss: 0.5381\n    Batch 150/438, Loss: 0.8884\n    Batch 160/438, Loss: 0.7447\n    Batch 170/438, Loss: 1.7102\n    Batch 180/438, Loss: 0.8441\n    Batch 190/438, Loss: 0.6876\n    Batch 200/438, Loss: 0.7129\n    Batch 210/438, Loss: 0.6216\n    Batch 220/438, Loss: 0.4058\n    Batch 230/438, Loss: 1.2114\n    Batch 240/438, Loss: 0.3281\n    Batch 250/438, Loss: 0.5960\n    Batch 260/438, Loss: 0.5454\n    Batch 270/438, Loss: 0.4414\n    Batch 280/438, Loss: 0.9049\n    Batch 290/438, Loss: 0.5988\n    Batch 300/438, Loss: 0.5436\n    Batch 310/438, Loss: 0.7381\n    Batch 320/438, Loss: 1.2076\n    Batch 330/438, Loss: 0.5006\n    Batch 340/438, Loss: 0.4435\n    Batch 350/438, Loss: 0.7007\n    Batch 360/438, Loss: 0.7980\n    Batch 370/438, Loss: 0.4947\n    Batch 380/438, Loss: 0.6885\n    Batch 390/438, Loss: 0.8789\n    Batch 400/438, Loss: 0.6495\n    Batch 410/438, Loss: 0.5810\n    Batch 420/438, Loss: 0.2363\n    Batch 430/438, Loss: 0.4742\n    Batch 438/438, Loss: 0.2521\n    GWO Eval 23: LR=0.000121, Dropout=0.30, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 1.1756\n    Batch 20/219, Loss: 0.9149\n    Batch 30/219, Loss: 0.8782\n    Batch 40/219, Loss: 0.6139\n    Batch 50/219, Loss: 0.6170\n    Batch 60/219, Loss: 0.4557\n    Batch 70/219, Loss: 0.7668\n    Batch 80/219, Loss: 0.5421\n    Batch 90/219, Loss: 0.5389\n    Batch 100/219, Loss: 0.3996\n    Batch 110/219, Loss: 0.4909\n    Batch 120/219, Loss: 0.3985\n    Batch 130/219, Loss: 0.4508\n    Batch 140/219, Loss: 0.3216\n    Batch 150/219, Loss: 0.3240\n    Batch 160/219, Loss: 0.2987\n    Batch 170/219, Loss: 0.3932\n    Batch 180/219, Loss: 0.4096\n    Batch 190/219, Loss: 0.3439\n    Batch 200/219, Loss: 0.2445\n    Batch 210/219, Loss: 0.2557\n    Batch 219/219, Loss: 0.3637\n    GWO Eval 24: LR=0.003000, Dropout=0.36, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.7083\n    Batch 20/219, Loss: 0.9291\n    Batch 30/219, Loss: 0.7677\n    Batch 40/219, Loss: 0.6894\n    Batch 50/219, Loss: 0.7362\n    Batch 60/219, Loss: 0.4504\n    Batch 70/219, Loss: 0.6133\n    Batch 80/219, Loss: 0.5228\n    Batch 90/219, Loss: 0.5279\n    Batch 100/219, Loss: 0.5532\n    Batch 110/219, Loss: 0.5841\n    Batch 120/219, Loss: 0.7307\n    Batch 130/219, Loss: 0.5114\n    Batch 140/219, Loss: 0.6398\n    Batch 150/219, Loss: 0.6602\n    Batch 160/219, Loss: 0.5836\n    Batch 170/219, Loss: 0.9026\n    Batch 180/219, Loss: 0.5125\n    Batch 190/219, Loss: 0.6955\n    Batch 200/219, Loss: 0.6096\n    Batch 210/219, Loss: 0.7712\n    Batch 219/219, Loss: 0.3950\n  GWO Iter 1/4: Best = 0.7850\n    GWO Eval 25: LR=0.000046, Dropout=0.26, BS=16\n    First batch loaded, starting training...\n    Batch 10/438, Loss: 1.4018\n    Batch 20/438, Loss: 1.1765\n    Batch 30/438, Loss: 1.4137\n    Batch 40/438, Loss: 1.6062\n    Batch 50/438, Loss: 1.1981\n    Batch 60/438, Loss: 1.0671\n    Batch 70/438, Loss: 1.1048\n    Batch 80/438, Loss: 0.8360\n    Batch 90/438, Loss: 0.9473\n    Batch 100/438, Loss: 0.7315\n    Batch 110/438, Loss: 1.0173\n    Batch 120/438, Loss: 0.9573\n    Batch 130/438, Loss: 0.7456\n    Batch 140/438, Loss: 0.8396\n    Batch 150/438, Loss: 0.8006\n    Batch 160/438, Loss: 0.7750\n    Batch 170/438, Loss: 0.8799\n    Batch 180/438, Loss: 0.8072\n    Batch 190/438, Loss: 0.8141\n    Batch 200/438, Loss: 1.1069\n    Batch 210/438, Loss: 1.0691\n    Batch 220/438, Loss: 0.8993\n    Batch 230/438, Loss: 0.6946\n    Batch 240/438, Loss: 0.9901\n    Batch 250/438, Loss: 1.0972\n    Batch 260/438, Loss: 0.8695\n    Batch 270/438, Loss: 1.0290\n    Batch 280/438, Loss: 0.7971\n    Batch 290/438, Loss: 1.2438\n    Batch 300/438, Loss: 1.1569\n    Batch 310/438, Loss: 1.0286\n    Batch 320/438, Loss: 0.8618\n    Batch 330/438, Loss: 0.8336\n    Batch 340/438, Loss: 1.0502\n    Batch 350/438, Loss: 0.6306\n    Batch 360/438, Loss: 1.3456\n    Batch 370/438, Loss: 0.6583\n    Batch 380/438, Loss: 0.8161\n    Batch 390/438, Loss: 0.9126\n    Batch 400/438, Loss: 0.7495\n    Batch 410/438, Loss: 0.6393\n    Batch 420/438, Loss: 0.6287\n    Batch 430/438, Loss: 0.7912\n    Batch 438/438, Loss: 1.0808\n    GWO Eval 26: LR=0.003000, Dropout=0.39, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.9515\n    Batch 20/219, Loss: 0.8497\n    Batch 30/219, Loss: 0.6427\n    Batch 40/219, Loss: 0.6651\n    Batch 50/219, Loss: 0.5893\n    Batch 60/219, Loss: 0.4417\n    Batch 70/219, Loss: 0.4071\n    Batch 80/219, Loss: 0.4875\n    Batch 90/219, Loss: 0.3455\n    Batch 100/219, Loss: 0.5030\n    Batch 110/219, Loss: 0.4254\n    Batch 120/219, Loss: 0.4165\n    Batch 130/219, Loss: 0.3347\n    Batch 140/219, Loss: 0.4420\n    Batch 150/219, Loss: 0.4485\n    Batch 160/219, Loss: 0.6462\n    Batch 170/219, Loss: 0.3804\n    Batch 180/219, Loss: 0.6219\n    Batch 190/219, Loss: 0.2682\n    Batch 200/219, Loss: 0.4607\n    Batch 210/219, Loss: 0.7449\n    Batch 219/219, Loss: 0.3825\n    GWO Eval 27: LR=0.000299, Dropout=0.51, BS=16\n    First batch loaded, starting training...\n    Batch 10/438, Loss: 1.2224\n    Batch 20/438, Loss: 1.0701\n    Batch 30/438, Loss: 1.2787\n    Batch 40/438, Loss: 0.9154\n    Batch 50/438, Loss: 1.1969\n    Batch 60/438, Loss: 1.0296\n    Batch 70/438, Loss: 0.8068\n    Batch 80/438, Loss: 0.7928\n    Batch 90/438, Loss: 1.1744\n    Batch 100/438, Loss: 0.9112\n    Batch 110/438, Loss: 1.0672\n    Batch 120/438, Loss: 1.6358\n    Batch 130/438, Loss: 0.7304\n    Batch 140/438, Loss: 1.9228\n    Batch 150/438, Loss: 0.8816\n    Batch 160/438, Loss: 0.7666\n    Batch 170/438, Loss: 0.6221\n    Batch 180/438, Loss: 0.9546\n    Batch 190/438, Loss: 0.9290\n    Batch 200/438, Loss: 0.7052\n    Batch 210/438, Loss: 0.5781\n    Batch 220/438, Loss: 0.7843\n    Batch 230/438, Loss: 1.2833\n    Batch 240/438, Loss: 0.9714\n    Batch 250/438, Loss: 0.4760\n    Batch 260/438, Loss: 0.4237\n    Batch 270/438, Loss: 0.6727\n    Batch 280/438, Loss: 0.8496\n    Batch 290/438, Loss: 0.5713\n    Batch 300/438, Loss: 0.6378\n    Batch 310/438, Loss: 0.6368\n    Batch 320/438, Loss: 0.7899\n    Batch 330/438, Loss: 0.5912\n    Batch 340/438, Loss: 0.8445\n    Batch 350/438, Loss: 1.5648\n    Batch 360/438, Loss: 0.6360\n    Batch 370/438, Loss: 0.6499\n    Batch 380/438, Loss: 0.7407\n    Batch 390/438, Loss: 1.0875\n    Batch 400/438, Loss: 0.9987\n    Batch 410/438, Loss: 0.6515\n    Batch 420/438, Loss: 1.1616\n    Batch 430/438, Loss: 0.5259\n    Batch 438/438, Loss: 1.1189\n    GWO Eval 28: LR=0.000497, Dropout=0.55, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.8498\n    Batch 20/219, Loss: 0.9785\n    Batch 30/219, Loss: 1.2077\n    Batch 40/219, Loss: 0.4529\n    Batch 50/219, Loss: 0.4787\n    Batch 60/219, Loss: 0.3888\n    Batch 70/219, Loss: 0.4378\n    Batch 80/219, Loss: 0.4257\n    Batch 90/219, Loss: 0.3778\n    Batch 100/219, Loss: 0.3347\n    Batch 110/219, Loss: 1.2747\n    Batch 120/219, Loss: 0.2296\n    Batch 130/219, Loss: 0.3988\n    Batch 140/219, Loss: 0.1848\n    Batch 150/219, Loss: 0.6933\n    Batch 160/219, Loss: 0.3341\n    Batch 170/219, Loss: 0.2411\n    Batch 180/219, Loss: 0.2511\n    Batch 190/219, Loss: 0.2960\n    Batch 200/219, Loss: 0.3567\n    Batch 210/219, Loss: 0.3069\n    Batch 219/219, Loss: 0.5333\n    GWO Eval 29: LR=0.000035, Dropout=0.50, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 1.3387\n    Batch 20/219, Loss: 1.2908\n    Batch 30/219, Loss: 1.2773\n    Batch 40/219, Loss: 1.2441\n    Batch 50/219, Loss: 1.1281\n    Batch 60/219, Loss: 1.1902\n    Batch 70/219, Loss: 1.0432\n    Batch 80/219, Loss: 1.0591\n    Batch 90/219, Loss: 0.9040\n    Batch 100/219, Loss: 0.8784\n    Batch 110/219, Loss: 0.9269\n    Batch 120/219, Loss: 1.0392\n    Batch 130/219, Loss: 0.9507\n    Batch 140/219, Loss: 1.2740\n    Batch 150/219, Loss: 1.0653\n    Batch 160/219, Loss: 1.5177\n    Batch 170/219, Loss: 1.1561\n    Batch 180/219, Loss: 1.3367\n    Batch 190/219, Loss: 0.8194\n    Batch 200/219, Loss: 0.9666\n    Batch 210/219, Loss: 1.1037\n    Batch 219/219, Loss: 1.0209\n    GWO Eval 30: LR=0.000677, Dropout=0.60, BS=16\n    First batch loaded, starting training...\n    Batch 10/438, Loss: 0.6857\n    Batch 20/438, Loss: 0.6593\n    Batch 30/438, Loss: 1.5941\n    Batch 40/438, Loss: 0.5616\n    Batch 50/438, Loss: 0.4116\n    Batch 60/438, Loss: 0.3186\n    Batch 70/438, Loss: 0.7985\n    Batch 80/438, Loss: 0.4063\n    Batch 90/438, Loss: 1.4688\n    Batch 100/438, Loss: 0.4542\n    Batch 110/438, Loss: 1.0335\n    Batch 120/438, Loss: 0.3440\n    Batch 130/438, Loss: 0.6243\n    Batch 140/438, Loss: 0.7825\n    Batch 150/438, Loss: 0.3280\n    Batch 160/438, Loss: 0.2599\n    Batch 170/438, Loss: 0.5852\n    Batch 180/438, Loss: 0.5703\n    Batch 190/438, Loss: 0.7745\n    Batch 200/438, Loss: 0.3092\n    Batch 210/438, Loss: 0.4297\n    Batch 220/438, Loss: 0.6592\n    Batch 230/438, Loss: 0.5217\n    Batch 240/438, Loss: 0.3741\n    Batch 250/438, Loss: 0.2100\n    Batch 260/438, Loss: 0.3303\n    Batch 270/438, Loss: 0.2287\n    Batch 280/438, Loss: 0.2752\n    Batch 290/438, Loss: 0.2567\n    Batch 300/438, Loss: 0.3519\n    Batch 310/438, Loss: 0.3150\n    Batch 320/438, Loss: 0.7006\n    Batch 330/438, Loss: 0.4402\n    Batch 340/438, Loss: 0.2156\n    Batch 350/438, Loss: 0.3617\n    Batch 360/438, Loss: 0.1496\n    Batch 370/438, Loss: 0.5127\n    Batch 380/438, Loss: 0.2532\n    Batch 390/438, Loss: 0.3737\n    Batch 400/438, Loss: 0.3240\n    Batch 410/438, Loss: 0.2405\n    Batch 420/438, Loss: 0.3652\n    Batch 430/438, Loss: 0.2660\n    Batch 438/438, Loss: 0.1620\n    GWO Eval 31: LR=0.003000, Dropout=0.54, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.9616\n    Batch 20/219, Loss: 1.1547\n    Batch 30/219, Loss: 0.7164\n    Batch 40/219, Loss: 1.0639\n    Batch 50/219, Loss: 0.4421\n    Batch 60/219, Loss: 0.4613\n    Batch 70/219, Loss: 0.7025\n    Batch 80/219, Loss: 0.5626\n    Batch 90/219, Loss: 0.2789\n    Batch 100/219, Loss: 0.3429\n    Batch 110/219, Loss: 0.6059\n    Batch 120/219, Loss: 0.4029\n    Batch 130/219, Loss: 0.3249\n    Batch 140/219, Loss: 0.3727\n    Batch 150/219, Loss: 1.0955\n    Batch 160/219, Loss: 0.3278\n    Batch 170/219, Loss: 0.4326\n    Batch 180/219, Loss: 0.5603\n    Batch 190/219, Loss: 0.2970\n    Batch 200/219, Loss: 0.2675\n    Batch 210/219, Loss: 0.3727\n    Batch 219/219, Loss: 0.6562\n    GWO Eval 32: LR=0.003000, Dropout=0.37, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.9096\n    Batch 20/219, Loss: 0.7792\n    Batch 30/219, Loss: 0.7180\n    Batch 40/219, Loss: 0.5733\n    Batch 50/219, Loss: 0.6738\n    Batch 60/219, Loss: 0.5536\n    Batch 70/219, Loss: 0.7115\n    Batch 80/219, Loss: 0.6027\n    Batch 90/219, Loss: 1.2646\n    Batch 100/219, Loss: 0.6461\n    Batch 110/219, Loss: 0.7458\n    Batch 120/219, Loss: 0.6106\n    Batch 130/219, Loss: 0.7243\n    Batch 140/219, Loss: 0.8583\n    Batch 150/219, Loss: 0.7054\n    Batch 160/219, Loss: 0.6113\n    Batch 170/219, Loss: 0.7629\n    Batch 180/219, Loss: 0.5513\n    Batch 190/219, Loss: 0.7024\n    Batch 200/219, Loss: 0.5269\n    Batch 210/219, Loss: 0.7713\n    Batch 219/219, Loss: 0.8533\n    GWO Eval 33: LR=0.003000, Dropout=0.12, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.6992\n    Batch 20/219, Loss: 0.9293\n    Batch 30/219, Loss: 1.0027\n    Batch 40/219, Loss: 0.5968\n    Batch 50/219, Loss: 0.4343\n    Batch 60/219, Loss: 0.5016\n    Batch 70/219, Loss: 0.5001\n    Batch 80/219, Loss: 0.5624\n    Batch 90/219, Loss: 0.2614\n    Batch 100/219, Loss: 0.4141\n    Batch 110/219, Loss: 0.3828\n    Batch 120/219, Loss: 0.3639\n    Batch 130/219, Loss: 0.3534\n    Batch 140/219, Loss: 0.8461\n    Batch 150/219, Loss: 0.2112\n    Batch 160/219, Loss: 0.5993\n    Batch 170/219, Loss: 0.2088\n    Batch 180/219, Loss: 0.4483\n    Batch 190/219, Loss: 0.3626\n    Batch 200/219, Loss: 0.4663\n    Batch 210/219, Loss: 0.5832\n    Batch 219/219, Loss: 0.2813\n    GWO Eval 34: LR=0.000158, Dropout=0.60, BS=16\n    First batch loaded, starting training...\n    Batch 10/438, Loss: 1.2583\n    Batch 20/438, Loss: 0.9862\n    Batch 30/438, Loss: 1.2926\n    Batch 40/438, Loss: 0.6108\n    Batch 50/438, Loss: 0.4703\n    Batch 60/438, Loss: 0.5700\n    Batch 70/438, Loss: 0.6592\n    Batch 80/438, Loss: 0.6070\n    Batch 90/438, Loss: 0.6766\n    Batch 100/438, Loss: 0.4664\n    Batch 110/438, Loss: 0.5540\n    Batch 120/438, Loss: 0.5000\n    Batch 130/438, Loss: 0.4470\n    Batch 140/438, Loss: 0.4641\n    Batch 150/438, Loss: 0.4527\n    Batch 160/438, Loss: 0.4367\n    Batch 170/438, Loss: 0.4167\n    Batch 180/438, Loss: 0.3105\n    Batch 190/438, Loss: 0.3222\n    Batch 200/438, Loss: 0.4046\n    Batch 210/438, Loss: 0.4529\n    Batch 220/438, Loss: 0.3665\n    Batch 230/438, Loss: 0.2652\n    Batch 240/438, Loss: 0.6066\n    Batch 250/438, Loss: 0.5390\n    Batch 260/438, Loss: 0.2550\n    Batch 270/438, Loss: 0.9635\n    Batch 280/438, Loss: 0.4216\n    Batch 290/438, Loss: 0.6295\n    Batch 300/438, Loss: 0.3180\n    Batch 310/438, Loss: 0.7904\n    Batch 320/438, Loss: 0.2939\n    Batch 330/438, Loss: 0.4617\n    Batch 340/438, Loss: 0.5542\n    Batch 350/438, Loss: 0.2948\n    Batch 360/438, Loss: 0.2171\n    Batch 370/438, Loss: 0.5896\n    Batch 380/438, Loss: 0.2608\n    Batch 390/438, Loss: 0.3781\n    Batch 400/438, Loss: 0.1441\n    Batch 410/438, Loss: 0.1864\n    Batch 420/438, Loss: 0.6198\n    Batch 430/438, Loss: 0.2285\n    Batch 438/438, Loss: 0.4521\n    GWO Eval 35: LR=0.001197, Dropout=0.48, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 1.1459\n    Batch 20/219, Loss: 0.3939\n    Batch 30/219, Loss: 0.4701\n    Batch 40/219, Loss: 0.3533\n    Batch 50/219, Loss: 0.5656\n    Batch 60/219, Loss: 0.6537\n    Batch 70/219, Loss: 0.4001\n    Batch 80/219, Loss: 0.2771\n    Batch 90/219, Loss: 0.3507\n    Batch 100/219, Loss: 0.4534\n    Batch 110/219, Loss: 0.3844\n    Batch 120/219, Loss: 0.2816\n    Batch 130/219, Loss: 0.3600\n    Batch 140/219, Loss: 0.3278\n    Batch 150/219, Loss: 0.3179\n    Batch 160/219, Loss: 0.4585\n    Batch 170/219, Loss: 0.3078\n    Batch 180/219, Loss: 0.1857\n    Batch 190/219, Loss: 0.2856\n    Batch 200/219, Loss: 0.4168\n    Batch 210/219, Loss: 0.3034\n    Batch 219/219, Loss: 0.3500\n    GWO Eval 36: LR=0.000333, Dropout=0.45, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.7881\n    Batch 20/219, Loss: 0.5722\n    Batch 30/219, Loss: 0.5350\n    Batch 40/219, Loss: 0.9554\n    Batch 50/219, Loss: 0.5549\n    Batch 60/219, Loss: 0.4542\n    Batch 70/219, Loss: 0.4687\n    Batch 80/219, Loss: 1.2038\n    Batch 90/219, Loss: 0.2884\n    Batch 100/219, Loss: 0.2715\n    Batch 110/219, Loss: 0.2453\n    Batch 120/219, Loss: 0.7257\n    Batch 130/219, Loss: 0.2320\n    Batch 140/219, Loss: 0.2265\n    Batch 150/219, Loss: 0.2056\n    Batch 160/219, Loss: 0.1883\n    Batch 170/219, Loss: 0.3880\n    Batch 180/219, Loss: 0.1639\n    Batch 190/219, Loss: 0.1386\n    Batch 200/219, Loss: 0.2031\n    Batch 210/219, Loss: 0.2132\n    Batch 219/219, Loss: 0.1814\n  GWO Iter 2/4: Best = 0.7929\n    GWO Eval 37: LR=0.000825, Dropout=0.48, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.8814\n    Batch 20/219, Loss: 0.6722\n    Batch 30/219, Loss: 0.7071\n    Batch 40/219, Loss: 0.4825\n    Batch 50/219, Loss: 0.5580\n    Batch 60/219, Loss: 0.6464\n    Batch 70/219, Loss: 0.3922\n    Batch 80/219, Loss: 0.2237\n    Batch 90/219, Loss: 0.2881\n    Batch 100/219, Loss: 0.1860\n    Batch 110/219, Loss: 0.3701\n    Batch 120/219, Loss: 0.2071\n    Batch 130/219, Loss: 0.5058\n    Batch 140/219, Loss: 0.2351\n    Batch 150/219, Loss: 0.2886\n    Batch 160/219, Loss: 0.2656\n    Batch 170/219, Loss: 0.6482\n    Batch 180/219, Loss: 0.2344\n    Batch 190/219, Loss: 0.2573\n    Batch 200/219, Loss: 0.5782\n    Batch 210/219, Loss: 0.2747\n    Batch 219/219, Loss: 0.1767\n    GWO Eval 38: LR=0.000604, Dropout=0.60, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 1.0376\n    Batch 20/219, Loss: 0.3586\n    Batch 30/219, Loss: 0.4125\n    Batch 40/219, Loss: 0.7836\n    Batch 50/219, Loss: 0.4504\n    Batch 60/219, Loss: 0.2902\n    Batch 70/219, Loss: 0.4143\n    Batch 80/219, Loss: 0.2955\n    Batch 90/219, Loss: 0.3035\n    Batch 100/219, Loss: 0.1689\n    Batch 110/219, Loss: 0.4652\n    Batch 120/219, Loss: 0.3870\n    Batch 130/219, Loss: 0.2029\n    Batch 140/219, Loss: 0.1976\n    Batch 150/219, Loss: 0.3305\n    Batch 160/219, Loss: 0.3229\n    Batch 170/219, Loss: 0.1919\n    Batch 180/219, Loss: 0.2253\n    Batch 190/219, Loss: 0.3143\n    Batch 200/219, Loss: 0.3300\n    Batch 210/219, Loss: 0.3251\n    Batch 219/219, Loss: 0.2566\n    GWO Eval 39: LR=0.000038, Dropout=0.50, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 1.2968\n    Batch 20/219, Loss: 1.1844\n    Batch 30/219, Loss: 1.0880\n    Batch 40/219, Loss: 1.0788\n    Batch 50/219, Loss: 0.8342\n    Batch 60/219, Loss: 1.1190\n    Batch 70/219, Loss: 0.7206\n    Batch 80/219, Loss: 0.6307\n    Batch 90/219, Loss: 0.6845\n    Batch 100/219, Loss: 0.6703\n    Batch 110/219, Loss: 1.0594\n    Batch 120/219, Loss: 0.7427\n    Batch 130/219, Loss: 1.0433\n    Batch 140/219, Loss: 0.8642\n    Batch 150/219, Loss: 0.5187\n    Batch 160/219, Loss: 1.0411\n    Batch 170/219, Loss: 1.0152\n    Batch 180/219, Loss: 0.4474\n    Batch 190/219, Loss: 0.4283\n    Batch 200/219, Loss: 0.3716\n    Batch 210/219, Loss: 0.7060\n    Batch 219/219, Loss: 0.5795\n    GWO Eval 40: LR=0.000926, Dropout=0.60, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.6851\n    Batch 20/219, Loss: 0.6414\n    Batch 30/219, Loss: 0.4412\n    Batch 40/219, Loss: 0.4511\n    Batch 50/219, Loss: 0.3449\n    Batch 60/219, Loss: 0.3904\n    Batch 70/219, Loss: 0.4991\n    Batch 80/219, Loss: 0.4669\n    Batch 90/219, Loss: 0.3658\n    Batch 100/219, Loss: 0.3642\n    Batch 110/219, Loss: 0.4734\n    Batch 120/219, Loss: 0.2539\n    Batch 130/219, Loss: 0.4701\n    Batch 140/219, Loss: 0.2197\n    Batch 150/219, Loss: 0.2897\n    Batch 160/219, Loss: 0.2583\n    Batch 170/219, Loss: 0.2292\n    Batch 180/219, Loss: 0.1832\n    Batch 190/219, Loss: 0.3423\n    Batch 200/219, Loss: 0.3824\n    Batch 210/219, Loss: 0.4156\n    Batch 219/219, Loss: 0.2601\n    GWO Eval 41: LR=0.000691, Dropout=0.46, BS=16\n    First batch loaded, starting training...\n    Batch 10/438, Loss: 0.7036\n    Batch 20/438, Loss: 0.5852\n    Batch 30/438, Loss: 0.6910\n    Batch 40/438, Loss: 1.0376\n    Batch 50/438, Loss: 1.4922\n    Batch 60/438, Loss: 0.5413\n    Batch 70/438, Loss: 0.5724\n    Batch 80/438, Loss: 0.5799\n    Batch 90/438, Loss: 0.2843\n    Batch 100/438, Loss: 0.5922\n    Batch 110/438, Loss: 0.5116\n    Batch 120/438, Loss: 0.9723\n    Batch 130/438, Loss: 0.4516\n    Batch 140/438, Loss: 0.5367\n    Batch 150/438, Loss: 0.4196\n    Batch 160/438, Loss: 0.4213\n    Batch 170/438, Loss: 0.5001\n    Batch 180/438, Loss: 0.1988\n    Batch 190/438, Loss: 0.2628\n    Batch 200/438, Loss: 0.3514\n    Batch 210/438, Loss: 0.2687\n    Batch 220/438, Loss: 0.1368\n    Batch 230/438, Loss: 0.6585\n    Batch 240/438, Loss: 0.1607\n    Batch 250/438, Loss: 0.2884\n    Batch 260/438, Loss: 0.6673\n    Batch 270/438, Loss: 0.2324\n    Batch 280/438, Loss: 0.1746\n    Batch 290/438, Loss: 0.2081\n    Batch 300/438, Loss: 0.3271\n    Batch 310/438, Loss: 0.2337\n    Batch 320/438, Loss: 0.3802\n    Batch 330/438, Loss: 1.3123\n    Batch 340/438, Loss: 0.2199\n    Batch 350/438, Loss: 0.2196\n    Batch 360/438, Loss: 0.7908\n    Batch 370/438, Loss: 0.1414\n    Batch 380/438, Loss: 0.4638\n    Batch 390/438, Loss: 0.4471\n    Batch 400/438, Loss: 0.3295\n    Batch 410/438, Loss: 0.5989\n    Batch 420/438, Loss: 0.2186\n    Batch 430/438, Loss: 0.2029\n    Batch 438/438, Loss: 0.2694\n    GWO Eval 42: LR=0.000270, Dropout=0.59, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 1.1697\n    Batch 20/219, Loss: 0.7497\n    Batch 30/219, Loss: 0.4273\n    Batch 40/219, Loss: 0.7185\n    Batch 50/219, Loss: 0.8564\n    Batch 60/219, Loss: 0.3207\n    Batch 70/219, Loss: 0.5899\n    Batch 80/219, Loss: 0.3392\n    Batch 90/219, Loss: 0.3095\n    Batch 100/219, Loss: 0.3180\n    Batch 110/219, Loss: 0.3723\n    Batch 120/219, Loss: 0.3364\n    Batch 130/219, Loss: 0.3890\n    Batch 140/219, Loss: 0.4374\n    Batch 150/219, Loss: 0.2371\n    Batch 160/219, Loss: 0.4920\n    Batch 170/219, Loss: 0.4705\n    Batch 180/219, Loss: 0.3324\n    Batch 190/219, Loss: 0.2266\n    Batch 200/219, Loss: 0.3653\n    Batch 210/219, Loss: 0.4212\n    Batch 219/219, Loss: 0.2138\n    GWO Eval 43: LR=0.003000, Dropout=0.44, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.8358\n    Batch 20/219, Loss: 0.7352\n    Batch 30/219, Loss: 0.6024\n    Batch 40/219, Loss: 0.6505\n    Batch 50/219, Loss: 0.7723\n    Batch 60/219, Loss: 0.4024\n    Batch 70/219, Loss: 0.5392\n    Batch 80/219, Loss: 0.4144\n    Batch 90/219, Loss: 0.3615\n    Batch 100/219, Loss: 0.2516\n    Batch 110/219, Loss: 0.4120\n    Batch 120/219, Loss: 0.5371\n    Batch 130/219, Loss: 0.2893\n    Batch 140/219, Loss: 0.3626\n    Batch 150/219, Loss: 0.4248\n    Batch 160/219, Loss: 0.4340\n    Batch 170/219, Loss: 0.2413\n    Batch 180/219, Loss: 0.6441\n    Batch 190/219, Loss: 0.5701\n    Batch 200/219, Loss: 0.2912\n    Batch 210/219, Loss: 0.2919\n    Batch 219/219, Loss: 0.6294\n    GWO Eval 44: LR=0.000355, Dropout=0.55, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.9456\n    Batch 20/219, Loss: 0.9006\n    Batch 30/219, Loss: 0.7651\n    Batch 40/219, Loss: 0.5436\n    Batch 50/219, Loss: 0.4127\n    Batch 60/219, Loss: 0.4868\n    Batch 70/219, Loss: 0.8330\n    Batch 80/219, Loss: 0.3862\n    Batch 90/219, Loss: 0.2607\n    Batch 100/219, Loss: 0.2331\n    Batch 110/219, Loss: 0.4567\n    Batch 120/219, Loss: 0.2176\n    Batch 130/219, Loss: 0.1584\n    Batch 140/219, Loss: 0.4740\n    Batch 150/219, Loss: 0.2796\n    Batch 160/219, Loss: 0.2667\n    Batch 170/219, Loss: 0.3916\n    Batch 180/219, Loss: 0.4033\n    Batch 190/219, Loss: 0.1970\n    Batch 200/219, Loss: 0.3983\n    Batch 210/219, Loss: 0.2849\n    Batch 219/219, Loss: 0.2722\n    GWO Eval 45: LR=0.002293, Dropout=0.42, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.9263\n    Batch 20/219, Loss: 1.0125\n    Batch 30/219, Loss: 0.5712\n    Batch 40/219, Loss: 0.4611\n    Batch 50/219, Loss: 0.5889\n    Batch 60/219, Loss: 0.4996\n    Batch 70/219, Loss: 0.5318\n    Batch 80/219, Loss: 0.4043\n    Batch 90/219, Loss: 0.4407\n    Batch 100/219, Loss: 0.6324\n    Batch 110/219, Loss: 0.3195\n    Batch 120/219, Loss: 0.7007\n    Batch 130/219, Loss: 0.3308\n    Batch 140/219, Loss: 0.4029\n    Batch 150/219, Loss: 0.4590\n    Batch 160/219, Loss: 0.1837\n    Batch 170/219, Loss: 0.2638\n    Batch 180/219, Loss: 0.5082\n    Batch 190/219, Loss: 0.3237\n    Batch 200/219, Loss: 0.5349\n    Batch 210/219, Loss: 0.6186\n    Batch 219/219, Loss: 0.3472\n    GWO Eval 46: LR=0.000448, Dropout=0.29, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.7757\n    Batch 20/219, Loss: 0.7468\n    Batch 30/219, Loss: 0.7379\n    Batch 40/219, Loss: 0.8087\n    Batch 50/219, Loss: 0.3225\n    Batch 60/219, Loss: 0.3253\n    Batch 70/219, Loss: 0.3258\n    Batch 80/219, Loss: 0.3382\n    Batch 90/219, Loss: 0.3498\n    Batch 100/219, Loss: 0.5952\n    Batch 110/219, Loss: 0.2716\n    Batch 120/219, Loss: 0.3126\n    Batch 130/219, Loss: 0.2408\n    Batch 140/219, Loss: 0.1841\n    Batch 150/219, Loss: 0.2588\n    Batch 160/219, Loss: 0.1790\n    Batch 170/219, Loss: 0.1810\n    Batch 180/219, Loss: 0.2565\n    Batch 190/219, Loss: 0.3188\n    Batch 200/219, Loss: 0.3077\n    Batch 210/219, Loss: 0.2349\n    Batch 219/219, Loss: 0.7496\n    GWO Eval 47: LR=0.000864, Dropout=0.45, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.9811\n    Batch 20/219, Loss: 0.5730\n    Batch 30/219, Loss: 0.6290\n    Batch 40/219, Loss: 1.2076\n    Batch 50/219, Loss: 0.3263\n    Batch 60/219, Loss: 0.4923\n    Batch 70/219, Loss: 0.2435\n    Batch 80/219, Loss: 0.3022\n    Batch 90/219, Loss: 0.4301\n    Batch 100/219, Loss: 0.3714\n    Batch 110/219, Loss: 0.3812\n    Batch 120/219, Loss: 0.2291\n    Batch 130/219, Loss: 0.4143\n    Batch 140/219, Loss: 0.1805\n    Batch 150/219, Loss: 0.3250\n    Batch 160/219, Loss: 0.2643\n    Batch 170/219, Loss: 0.2060\n    Batch 180/219, Loss: 0.5282\n    Batch 190/219, Loss: 0.2927\n    Batch 200/219, Loss: 0.3140\n    Batch 210/219, Loss: 0.2835\n    Batch 219/219, Loss: 0.6064\n    GWO Eval 48: LR=0.000698, Dropout=0.60, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.6174\n    Batch 20/219, Loss: 0.9272\n    Batch 30/219, Loss: 0.4599\n    Batch 40/219, Loss: 0.5720\n    Batch 50/219, Loss: 0.3784\n    Batch 60/219, Loss: 0.2867\n    Batch 70/219, Loss: 0.6051\n    Batch 80/219, Loss: 0.4487\n    Batch 90/219, Loss: 0.5534\n    Batch 100/219, Loss: 0.2061\n    Batch 110/219, Loss: 0.3988\n    Batch 120/219, Loss: 0.7183\n    Batch 130/219, Loss: 0.3701\n    Batch 140/219, Loss: 0.2048\n    Batch 150/219, Loss: 0.1788\n    Batch 160/219, Loss: 0.2054\n    Batch 170/219, Loss: 0.1350\n    Batch 180/219, Loss: 0.3361\n    Batch 190/219, Loss: 0.2412\n    Batch 200/219, Loss: 0.2166\n    Batch 210/219, Loss: 0.1854\n    Batch 219/219, Loss: 0.2441\n  GWO Iter 3/4: Best = 0.8289\n    GWO Eval 49: LR=0.000895, Dropout=0.60, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 1.5139\n    Batch 20/219, Loss: 0.5926\n    Batch 30/219, Loss: 0.3917\n    Batch 40/219, Loss: 0.6299\n    Batch 50/219, Loss: 0.3911\n    Batch 60/219, Loss: 0.5221\n    Batch 70/219, Loss: 0.2969\n    Batch 80/219, Loss: 0.3562\n    Batch 90/219, Loss: 0.3633\n    Batch 100/219, Loss: 0.2258\n    Batch 110/219, Loss: 0.4701\n    Batch 120/219, Loss: 0.2653\n    Batch 130/219, Loss: 0.1952\n    Batch 140/219, Loss: 0.5089\n    Batch 150/219, Loss: 0.3664\n    Batch 160/219, Loss: 0.6843\n    Batch 170/219, Loss: 0.2261\n    Batch 180/219, Loss: 0.3423\n    Batch 190/219, Loss: 0.1446\n    Batch 200/219, Loss: 0.2885\n    Batch 210/219, Loss: 0.1415\n    Batch 219/219, Loss: 0.1605\n    GWO Eval 50: LR=0.000635, Dropout=0.60, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.6398\n    Batch 20/219, Loss: 0.6202\n    Batch 30/219, Loss: 0.6759\n    Batch 40/219, Loss: 0.5629\n    Batch 50/219, Loss: 0.4330\n    Batch 60/219, Loss: 0.5286\n    Batch 70/219, Loss: 0.3621\n    Batch 80/219, Loss: 0.2473\n    Batch 90/219, Loss: 0.3968\n    Batch 100/219, Loss: 0.4285\n    Batch 110/219, Loss: 0.2358\n    Batch 120/219, Loss: 0.3276\n    Batch 130/219, Loss: 0.2449\n    Batch 140/219, Loss: 0.1845\n    Batch 150/219, Loss: 0.3050\n    Batch 160/219, Loss: 0.1258\n    Batch 170/219, Loss: 0.4045\n    Batch 180/219, Loss: 0.3969\n    Batch 190/219, Loss: 0.1812\n    Batch 200/219, Loss: 0.2080\n    Batch 210/219, Loss: 0.2000\n    Batch 219/219, Loss: 0.4579\n    GWO Eval 51: LR=0.001650, Dropout=0.43, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.6796\n    Batch 20/219, Loss: 0.6562\n    Batch 30/219, Loss: 0.4676\n    Batch 40/219, Loss: 0.3827\n    Batch 50/219, Loss: 0.4934\n    Batch 60/219, Loss: 0.5689\n    Batch 70/219, Loss: 0.3958\n    Batch 80/219, Loss: 0.3048\n    Batch 90/219, Loss: 0.5465\n    Batch 100/219, Loss: 0.7256\n    Batch 110/219, Loss: 0.2819\n    Batch 120/219, Loss: 0.4846\n    Batch 130/219, Loss: 0.2587\n    Batch 140/219, Loss: 0.8715\n    Batch 150/219, Loss: 0.6116\n    Batch 160/219, Loss: 1.0147\n    Batch 170/219, Loss: 0.4677\n    Batch 180/219, Loss: 0.3539\n    Batch 190/219, Loss: 0.2586\n    Batch 200/219, Loss: 0.4437\n    Batch 210/219, Loss: 0.3231\n    Batch 219/219, Loss: 0.1573\n    GWO Eval 52: LR=0.000425, Dropout=0.57, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 1.2415\n    Batch 20/219, Loss: 0.6099\n    Batch 30/219, Loss: 0.6897\n    Batch 40/219, Loss: 0.5647\n    Batch 50/219, Loss: 0.4042\n    Batch 60/219, Loss: 0.4022\n    Batch 70/219, Loss: 0.3722\n    Batch 80/219, Loss: 1.1113\n    Batch 90/219, Loss: 0.5401\n    Batch 100/219, Loss: 0.3295\n    Batch 110/219, Loss: 0.2271\n    Batch 120/219, Loss: 0.2290\n    Batch 130/219, Loss: 0.4296\n    Batch 140/219, Loss: 0.2918\n    Batch 150/219, Loss: 0.2277\n    Batch 160/219, Loss: 0.1978\n    Batch 170/219, Loss: 0.2133\n    Batch 180/219, Loss: 0.4115\n    Batch 190/219, Loss: 0.5142\n    Batch 200/219, Loss: 0.1897\n    Batch 210/219, Loss: 0.1823\n    Batch 219/219, Loss: 0.1404\n    GWO Eval 53: LR=0.000431, Dropout=0.56, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 1.0742\n    Batch 20/219, Loss: 0.6542\n    Batch 30/219, Loss: 0.5901\n    Batch 40/219, Loss: 0.5000\n    Batch 50/219, Loss: 0.7655\n    Batch 60/219, Loss: 0.4627\n    Batch 70/219, Loss: 0.2969\n    Batch 80/219, Loss: 0.2061\n    Batch 90/219, Loss: 0.3512\n    Batch 100/219, Loss: 0.2520\n    Batch 110/219, Loss: 0.4322\n    Batch 120/219, Loss: 0.3786\n    Batch 130/219, Loss: 0.5068\n    Batch 140/219, Loss: 0.2153\n    Batch 150/219, Loss: 0.2884\n    Batch 160/219, Loss: 0.1827\n    Batch 170/219, Loss: 0.1989\n    Batch 180/219, Loss: 0.3011\n    Batch 190/219, Loss: 0.3146\n    Batch 200/219, Loss: 0.2367\n    Batch 210/219, Loss: 0.2541\n    Batch 219/219, Loss: 0.1994\n    GWO Eval 54: LR=0.000944, Dropout=0.51, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.8425\n    Batch 20/219, Loss: 0.6445\n    Batch 30/219, Loss: 0.6905\n    Batch 40/219, Loss: 0.4544\n    Batch 50/219, Loss: 0.4462\n    Batch 60/219, Loss: 0.3592\n    Batch 70/219, Loss: 0.5156\n    Batch 80/219, Loss: 0.4009\n    Batch 90/219, Loss: 0.3047\n    Batch 100/219, Loss: 0.3569\n    Batch 110/219, Loss: 0.1997\n    Batch 120/219, Loss: 0.3935\n    Batch 130/219, Loss: 0.2904\n    Batch 140/219, Loss: 0.3486\n    Batch 150/219, Loss: 0.4066\n    Batch 160/219, Loss: 0.2278\n    Batch 170/219, Loss: 0.2013\n    Batch 180/219, Loss: 0.1525\n    Batch 190/219, Loss: 0.1929\n    Batch 200/219, Loss: 0.6310\n    Batch 210/219, Loss: 0.3624\n    Batch 219/219, Loss: 1.0660\n    GWO Eval 55: LR=0.000380, Dropout=0.60, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.8696\n    Batch 20/219, Loss: 0.7926\n    Batch 30/219, Loss: 0.4652\n    Batch 40/219, Loss: 0.6418\n    Batch 50/219, Loss: 0.3718\n    Batch 60/219, Loss: 0.4743\n    Batch 70/219, Loss: 0.6817\n    Batch 80/219, Loss: 0.5082\n    Batch 90/219, Loss: 0.5820\n    Batch 100/219, Loss: 0.3134\n    Batch 110/219, Loss: 0.2992\n    Batch 120/219, Loss: 0.3431\n    Batch 130/219, Loss: 0.2553\n    Batch 140/219, Loss: 0.3040\n    Batch 150/219, Loss: 0.3691\n    Batch 160/219, Loss: 0.2624\n    Batch 170/219, Loss: 0.1747\n    Batch 180/219, Loss: 0.3396\n    Batch 190/219, Loss: 0.1929\n    Batch 200/219, Loss: 0.3194\n    Batch 210/219, Loss: 0.2783\n    Batch 219/219, Loss: 0.3799\n    GWO Eval 56: LR=0.000558, Dropout=0.60, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.9712\n    Batch 20/219, Loss: 0.5992\n    Batch 30/219, Loss: 0.5141\n    Batch 40/219, Loss: 0.6248\n    Batch 50/219, Loss: 0.4419\n    Batch 60/219, Loss: 0.8310\n    Batch 70/219, Loss: 0.2452\n    Batch 80/219, Loss: 0.3148\n    Batch 90/219, Loss: 0.4089\n    Batch 100/219, Loss: 0.2326\n    Batch 110/219, Loss: 0.6794\n    Batch 120/219, Loss: 0.4444\n    Batch 130/219, Loss: 0.4482\n    Batch 140/219, Loss: 0.2841\n    Batch 150/219, Loss: 0.2084\n    Batch 160/219, Loss: 0.2251\n    Batch 170/219, Loss: 0.2407\n    Batch 180/219, Loss: 0.3676\n    Batch 190/219, Loss: 0.2541\n    Batch 200/219, Loss: 0.4247\n    Batch 210/219, Loss: 0.3160\n    Batch 219/219, Loss: 0.3027\n    GWO Eval 57: LR=0.001144, Dropout=0.55, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.8190\n    Batch 20/219, Loss: 0.5887\n    Batch 30/219, Loss: 0.4818\n    Batch 40/219, Loss: 0.4465\n    Batch 50/219, Loss: 0.2630\n    Batch 60/219, Loss: 0.7471\n    Batch 70/219, Loss: 0.7865\n    Batch 80/219, Loss: 0.3305\n    Batch 90/219, Loss: 0.3754\n    Batch 100/219, Loss: 0.3324\n    Batch 110/219, Loss: 0.4346\n    Batch 120/219, Loss: 0.4447\n    Batch 130/219, Loss: 0.2037\n    Batch 140/219, Loss: 0.4523\n    Batch 150/219, Loss: 0.2965\n    Batch 160/219, Loss: 0.2974\n    Batch 170/219, Loss: 0.2070\n    Batch 180/219, Loss: 0.9427\n    Batch 190/219, Loss: 0.2848\n    Batch 200/219, Loss: 0.2583\n    Batch 210/219, Loss: 0.6352\n    Batch 219/219, Loss: 0.2550\n    GWO Eval 58: LR=0.001145, Dropout=0.60, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.5182\n    Batch 20/219, Loss: 0.9483\n    Batch 30/219, Loss: 0.5720\n    Batch 40/219, Loss: 0.5841\n    Batch 50/219, Loss: 0.3812\n    Batch 60/219, Loss: 0.6094\n    Batch 70/219, Loss: 0.2975\n    Batch 80/219, Loss: 0.3800\n    Batch 90/219, Loss: 0.5395\n    Batch 100/219, Loss: 0.3963\n    Batch 110/219, Loss: 0.2605\n    Batch 120/219, Loss: 0.3899\n    Batch 130/219, Loss: 0.2675\n    Batch 140/219, Loss: 0.3201\n    Batch 150/219, Loss: 0.4171\n    Batch 160/219, Loss: 0.2685\n    Batch 170/219, Loss: 0.3464\n    Batch 180/219, Loss: 0.3320\n    Batch 190/219, Loss: 0.4868\n    Batch 200/219, Loss: 0.2309\n    Batch 210/219, Loss: 0.2900\n    Batch 219/219, Loss: 0.1870\n    GWO Eval 59: LR=0.000890, Dropout=0.60, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.8173\n    Batch 20/219, Loss: 0.6533\n    Batch 30/219, Loss: 0.6793\n    Batch 40/219, Loss: 0.4641\n    Batch 50/219, Loss: 0.5140\n    Batch 60/219, Loss: 0.5598\n    Batch 70/219, Loss: 0.7146\n    Batch 80/219, Loss: 0.3811\n    Batch 90/219, Loss: 0.6269\n    Batch 100/219, Loss: 0.3474\n    Batch 110/219, Loss: 0.2659\n    Batch 120/219, Loss: 0.2635\n    Batch 130/219, Loss: 0.7342\n    Batch 140/219, Loss: 0.2394\n    Batch 150/219, Loss: 0.5028\n    Batch 160/219, Loss: 0.3913\n    Batch 170/219, Loss: 0.1656\n    Batch 180/219, Loss: 0.1876\n    Batch 190/219, Loss: 0.2958\n    Batch 200/219, Loss: 0.2456\n    Batch 210/219, Loss: 0.4387\n    Batch 219/219, Loss: 0.6612\n    GWO Eval 60: LR=0.000882, Dropout=0.60, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.7456\n    Batch 20/219, Loss: 0.5440\n    Batch 30/219, Loss: 0.3815\n    Batch 40/219, Loss: 0.6503\n    Batch 50/219, Loss: 0.4121\n    Batch 60/219, Loss: 0.4310\n    Batch 70/219, Loss: 0.5130\n    Batch 80/219, Loss: 0.2945\n    Batch 90/219, Loss: 0.3939\n    Batch 100/219, Loss: 1.0103\n    Batch 110/219, Loss: 0.2284\n    Batch 120/219, Loss: 0.2020\n    Batch 130/219, Loss: 0.3417\n    Batch 140/219, Loss: 0.4483\n    Batch 150/219, Loss: 0.5355\n    Batch 160/219, Loss: 0.1994\n    Batch 170/219, Loss: 0.2420\n    Batch 180/219, Loss: 0.2328\n    Batch 190/219, Loss: 0.1925\n    Batch 200/219, Loss: 0.6830\n    Batch 210/219, Loss: 0.2510\n    Batch 219/219, Loss: 0.2733\n  GWO Iter 4/4: Best = 0.8289\n\nGWO Result: Score=0.8289, Time=1697.9s\n  Best Hyperparams: {'lr': np.float64(0.0006978496554427226), 'weight_decay': np.float64(0.007269211868624135), 'dropout': np.float64(0.6), 'unfreeze_epoch': 0, 'batch_size': 32, 'augment_strength': np.float64(0.7909538975217718)}\n\n--- Running PSO ---\n    PSO Eval 1: LR=0.001170, Dropout=0.45, BS=16\n    First batch loaded, starting training...\n    Batch 10/438, Loss: 0.7467\n    Batch 20/438, Loss: 0.8894\n    Batch 30/438, Loss: 0.7213\n    Batch 40/438, Loss: 0.9789\n    Batch 50/438, Loss: 0.8874\n    Batch 60/438, Loss: 0.8657\n    Batch 70/438, Loss: 0.7319\n    Batch 80/438, Loss: 1.4295\n    Batch 90/438, Loss: 1.1982\n    Batch 100/438, Loss: 0.8940\n    Batch 110/438, Loss: 0.6687\n    Batch 120/438, Loss: 0.5163\n    Batch 130/438, Loss: 0.6398\n    Batch 140/438, Loss: 0.4656\n    Batch 150/438, Loss: 0.8059\n    Batch 160/438, Loss: 0.7373\n    Batch 170/438, Loss: 1.0203\n    Batch 180/438, Loss: 0.5714\n    Batch 190/438, Loss: 0.5862\n    Batch 200/438, Loss: 0.6636\n    Batch 210/438, Loss: 0.9437\n    Batch 220/438, Loss: 0.5496\n    Batch 230/438, Loss: 0.4530\n    Batch 240/438, Loss: 0.5507\n    Batch 250/438, Loss: 1.5051\n    Batch 260/438, Loss: 0.6721\n    Batch 270/438, Loss: 0.9409\n    Batch 280/438, Loss: 0.7479\n    Batch 290/438, Loss: 1.2764\n    Batch 300/438, Loss: 0.9891\n    Batch 310/438, Loss: 0.8157\n    Batch 320/438, Loss: 0.4921\n    Batch 330/438, Loss: 0.5230\n    Batch 340/438, Loss: 0.9469\n    Batch 350/438, Loss: 0.3870\n    Batch 360/438, Loss: 0.6750\n    Batch 370/438, Loss: 0.7231\n    Batch 380/438, Loss: 0.7255\n    Batch 390/438, Loss: 0.4933\n    Batch 400/438, Loss: 0.6704\n    Batch 410/438, Loss: 0.6590\n    Batch 420/438, Loss: 0.7283\n    Batch 430/438, Loss: 1.2114\n    Batch 438/438, Loss: 0.4002\n    PSO Eval 2: LR=0.000095, Dropout=0.31, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 1.2323\n    Batch 20/219, Loss: 1.3043\n    Batch 30/219, Loss: 0.8299\n    Batch 40/219, Loss: 0.8234\n    Batch 50/219, Loss: 1.0190\n    Batch 60/219, Loss: 0.8175\n    Batch 70/219, Loss: 0.6828\n    Batch 80/219, Loss: 0.9994\n    Batch 90/219, Loss: 0.8903\n    Batch 100/219, Loss: 0.8048\n    Batch 110/219, Loss: 0.7627\n    Batch 120/219, Loss: 1.0269\n    Batch 130/219, Loss: 0.9314\n    Batch 140/219, Loss: 0.8297\n    Batch 150/219, Loss: 0.7730\n    Batch 160/219, Loss: 1.0121\n    Batch 170/219, Loss: 0.9546\n    Batch 180/219, Loss: 0.8679\n    Batch 190/219, Loss: 0.8254\n    Batch 200/219, Loss: 0.9395\n    Batch 210/219, Loss: 0.7812\n    Batch 219/219, Loss: 0.6599\n    PSO Eval 3: LR=0.000115, Dropout=0.13, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 1.0848\n    Batch 20/219, Loss: 1.0693\n    Batch 30/219, Loss: 1.2244\n    Batch 40/219, Loss: 0.9219\n    Batch 50/219, Loss: 1.0285\n    Batch 60/219, Loss: 0.9086\n    Batch 70/219, Loss: 0.9143\n    Batch 80/219, Loss: 0.8845\n    Batch 90/219, Loss: 0.8732\n    Batch 100/219, Loss: 0.9570\n    Batch 110/219, Loss: 0.8523\n    Batch 120/219, Loss: 0.7524\n    Batch 130/219, Loss: 1.0576\n    Batch 140/219, Loss: 0.9788\n    Batch 150/219, Loss: 0.9878\n    Batch 160/219, Loss: 0.8553\n    Batch 170/219, Loss: 0.8923\n    Batch 180/219, Loss: 0.7888\n    Batch 190/219, Loss: 0.8249\n    Batch 200/219, Loss: 0.8006\n    Batch 210/219, Loss: 0.5598\n    Batch 219/219, Loss: 0.7101\n    PSO Eval 4: LR=0.000399, Dropout=0.38, BS=16\n    First batch loaded, starting training...\n    Batch 10/438, Loss: 0.9509\n    Batch 20/438, Loss: 0.7130\n    Batch 30/438, Loss: 0.9020\n    Batch 40/438, Loss: 0.5549\n    Batch 50/438, Loss: 1.0161\n    Batch 60/438, Loss: 0.5840\n    Batch 70/438, Loss: 0.3885\n    Batch 80/438, Loss: 0.2503\n    Batch 90/438, Loss: 0.7572\n    Batch 100/438, Loss: 0.3305\n    Batch 110/438, Loss: 0.5687\n    Batch 120/438, Loss: 0.5979\n    Batch 130/438, Loss: 0.3814\n    Batch 140/438, Loss: 0.3652\n    Batch 150/438, Loss: 0.3739\n    Batch 160/438, Loss: 0.1966\n    Batch 170/438, Loss: 0.4834\n    Batch 180/438, Loss: 0.3227\n    Batch 190/438, Loss: 1.1753\n    Batch 200/438, Loss: 0.5728\n    Batch 210/438, Loss: 0.2034\n    Batch 220/438, Loss: 0.2162\n    Batch 230/438, Loss: 0.2209\n    Batch 240/438, Loss: 0.8749\n    Batch 250/438, Loss: 0.5404\n    Batch 260/438, Loss: 0.2521\n    Batch 270/438, Loss: 0.2339\n    Batch 280/438, Loss: 0.6144\n    Batch 290/438, Loss: 0.2201\n    Batch 300/438, Loss: 0.2360\n    Batch 310/438, Loss: 0.2601\n    Batch 320/438, Loss: 0.1561\n    Batch 330/438, Loss: 0.3455\n    Batch 340/438, Loss: 0.3337\n    Batch 350/438, Loss: 0.2459\n    Batch 360/438, Loss: 0.2613\n    Batch 370/438, Loss: 0.2105\n    Batch 380/438, Loss: 0.2618\n    Batch 390/438, Loss: 0.1438\n    Batch 400/438, Loss: 0.1634\n    Batch 410/438, Loss: 0.2481\n    Batch 420/438, Loss: 0.2376\n    Batch 430/438, Loss: 0.4500\n    Batch 438/438, Loss: 0.2378\n    PSO Eval 5: LR=0.000044, Dropout=0.52, BS=16\n    First batch loaded, starting training...\n    Batch 10/438, Loss: 1.3655\n    Batch 20/438, Loss: 1.2776\n    Batch 30/438, Loss: 1.1381\n    Batch 40/438, Loss: 1.3296\n    Batch 50/438, Loss: 0.8280\n    Batch 60/438, Loss: 0.9616\n    Batch 70/438, Loss: 0.8500\n    Batch 80/438, Loss: 0.7835\n    Batch 90/438, Loss: 0.8249\n    Batch 100/438, Loss: 0.7085\n    Batch 110/438, Loss: 1.1712\n    Batch 120/438, Loss: 0.5690\n    Batch 130/438, Loss: 0.7279\n    Batch 140/438, Loss: 0.6755\n    Batch 150/438, Loss: 0.5611\n    Batch 160/438, Loss: 0.6398\n    Batch 170/438, Loss: 0.7306\n    Batch 180/438, Loss: 0.4945\n    Batch 190/438, Loss: 0.5313\n    Batch 200/438, Loss: 0.5264\n    Batch 210/438, Loss: 1.3244\n    Batch 220/438, Loss: 0.8432\n    Batch 230/438, Loss: 0.6474\n    Batch 240/438, Loss: 0.3876\n    Batch 250/438, Loss: 0.6187\n    Batch 260/438, Loss: 0.6140\n    Batch 270/438, Loss: 1.1685\n    Batch 280/438, Loss: 0.3709\n    Batch 290/438, Loss: 0.4315\n    Batch 300/438, Loss: 0.6266\n    Batch 310/438, Loss: 1.9797\n    Batch 320/438, Loss: 0.4104\n    Batch 330/438, Loss: 0.4759\n    Batch 340/438, Loss: 1.2100\n    Batch 350/438, Loss: 0.2567\n    Batch 360/438, Loss: 0.5479\n    Batch 370/438, Loss: 0.4479\n    Batch 380/438, Loss: 0.2885\n    Batch 390/438, Loss: 0.5320\n    Batch 400/438, Loss: 0.5820\n    Batch 410/438, Loss: 0.4494\n    Batch 420/438, Loss: 0.3870\n    Batch 430/438, Loss: 0.5493\n    Batch 438/438, Loss: 0.2628\n    PSO Eval 6: LR=0.000027, Dropout=0.12, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 1.3356\n    Batch 20/219, Loss: 1.2673\n    Batch 30/219, Loss: 1.1972\n    Batch 40/219, Loss: 1.1416\n    Batch 50/219, Loss: 1.1613\n    Batch 60/219, Loss: 1.1554\n    Batch 70/219, Loss: 1.1074\n    Batch 80/219, Loss: 0.9298\n    Batch 90/219, Loss: 1.1475\n    Batch 100/219, Loss: 1.0311\n    Batch 110/219, Loss: 1.0978\n    Batch 120/219, Loss: 1.2446\n    Batch 130/219, Loss: 0.8310\n    Batch 140/219, Loss: 1.2649\n    Batch 150/219, Loss: 0.9876\n    Batch 160/219, Loss: 0.7248\n    Batch 170/219, Loss: 0.7392\n    Batch 180/219, Loss: 0.7789\n    Batch 190/219, Loss: 1.2581\n    Batch 200/219, Loss: 1.0137\n    Batch 210/219, Loss: 0.8848\n    Batch 219/219, Loss: 1.1682\n    PSO Eval 7: LR=0.000987, Dropout=0.31, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.6008\n    Batch 20/219, Loss: 0.7687\n    Batch 30/219, Loss: 0.4038\n    Batch 40/219, Loss: 0.7627\n    Batch 50/219, Loss: 0.8733\n    Batch 60/219, Loss: 0.3234\n    Batch 70/219, Loss: 0.4330\n    Batch 80/219, Loss: 0.4337\n    Batch 90/219, Loss: 0.2148\n    Batch 100/219, Loss: 0.3590\n    Batch 110/219, Loss: 0.2499\n    Batch 120/219, Loss: 0.3386\n    Batch 130/219, Loss: 0.3642\n    Batch 140/219, Loss: 0.6078\n    Batch 150/219, Loss: 0.3026\n    Batch 160/219, Loss: 0.4150\n    Batch 170/219, Loss: 0.1437\n    Batch 180/219, Loss: 0.1718\n    Batch 190/219, Loss: 0.1639\n    Batch 200/219, Loss: 0.1844\n    Batch 210/219, Loss: 0.5666\n    Batch 219/219, Loss: 0.2101\n    PSO Eval 8: LR=0.001536, Dropout=0.50, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.9074\n    Batch 20/219, Loss: 0.8240\n    Batch 30/219, Loss: 0.6082\n    Batch 40/219, Loss: 0.8581\n    Batch 50/219, Loss: 0.6596\n    Batch 60/219, Loss: 0.7956\n    Batch 70/219, Loss: 0.7398\n    Batch 80/219, Loss: 0.7292\n    Batch 90/219, Loss: 0.6528\n    Batch 100/219, Loss: 0.8153\n    Batch 110/219, Loss: 0.8804\n    Batch 120/219, Loss: 0.7168\n    Batch 130/219, Loss: 0.7346\n    Batch 140/219, Loss: 0.6483\n    Batch 150/219, Loss: 0.7819\n    Batch 160/219, Loss: 1.0009\n    Batch 170/219, Loss: 1.1693\n    Batch 180/219, Loss: 0.5270\n    Batch 190/219, Loss: 0.8518\n    Batch 200/219, Loss: 0.4739\n    Batch 210/219, Loss: 0.6965\n    Batch 219/219, Loss: 0.4042\n    PSO Eval 9: LR=0.001528, Dropout=0.27, BS=8\n    First batch loaded, starting training...\n    Batch 10/875, Loss: 0.9981\n    Batch 20/875, Loss: 1.0026\n    Batch 30/875, Loss: 0.5496\n    Batch 40/875, Loss: 1.0212\n    Batch 50/875, Loss: 0.9862\n    Batch 60/875, Loss: 1.1280\n    Batch 70/875, Loss: 0.5844\n    Batch 80/875, Loss: 0.7106\n    Batch 90/875, Loss: 0.6329\n    Batch 100/875, Loss: 0.6744\n    Batch 110/875, Loss: 0.7428\n    Batch 120/875, Loss: 0.5178\n    Batch 130/875, Loss: 0.9907\n    Batch 140/875, Loss: 0.7889\n    Batch 150/875, Loss: 0.6929\n    Batch 160/875, Loss: 0.7589\n    Batch 170/875, Loss: 0.8049\n    Batch 180/875, Loss: 0.3877\n    Batch 190/875, Loss: 0.7833\n    Batch 200/875, Loss: 0.6389\n    Batch 210/875, Loss: 1.2052\n    Batch 220/875, Loss: 0.8071\n    Batch 230/875, Loss: 0.7274\n    Batch 240/875, Loss: 1.6003\n    Batch 250/875, Loss: 0.9826\n    Batch 260/875, Loss: 1.1829\n    Batch 270/875, Loss: 0.8679\n    Batch 280/875, Loss: 1.2776\n    Batch 290/875, Loss: 1.8084\n    Batch 300/875, Loss: 1.0392\n    Batch 310/875, Loss: 0.7452\n    Batch 320/875, Loss: 1.8547\n    Batch 330/875, Loss: 1.5733\n    Batch 340/875, Loss: 0.6680\n    Batch 350/875, Loss: 0.6011\n    Batch 360/875, Loss: 0.2091\n    Batch 370/875, Loss: 0.9061\n    Batch 380/875, Loss: 0.6355\n    Batch 390/875, Loss: 0.7531\n    Batch 400/875, Loss: 0.4345\n    Batch 410/875, Loss: 0.6476\n    Batch 420/875, Loss: 0.8040\n    Batch 430/875, Loss: 0.5230\n    Batch 440/875, Loss: 0.3940\n    Batch 450/875, Loss: 0.7449\n    Batch 460/875, Loss: 0.5082\n    Batch 470/875, Loss: 0.3398\n    Batch 480/875, Loss: 2.1080\n    Batch 490/875, Loss: 0.5416\n    Batch 500/875, Loss: 0.4348\n    Batch 510/875, Loss: 0.9313\n    Batch 520/875, Loss: 0.5622\n    Batch 530/875, Loss: 0.4039\n    Batch 540/875, Loss: 0.5424\n    Batch 550/875, Loss: 0.7129\n    Batch 560/875, Loss: 1.5198\n    Batch 570/875, Loss: 0.8719\n    Batch 580/875, Loss: 0.5764\n    Batch 590/875, Loss: 0.2936\n    Batch 600/875, Loss: 0.6201\n    Batch 610/875, Loss: 1.2734\n    Batch 620/875, Loss: 0.5912\n    Batch 630/875, Loss: 0.6031\n    Batch 640/875, Loss: 0.9750\n    Batch 650/875, Loss: 0.7124\n    Batch 660/875, Loss: 1.3751\n    Batch 670/875, Loss: 0.7823\n    Batch 680/875, Loss: 0.5185\n    Batch 690/875, Loss: 0.6862\n    Batch 700/875, Loss: 0.8493\n    Batch 710/875, Loss: 0.7682\n    Batch 720/875, Loss: 0.7187\n    Batch 730/875, Loss: 0.7963\n    Batch 740/875, Loss: 0.3722\n    Batch 750/875, Loss: 0.7545\n    Batch 760/875, Loss: 0.7546\n    Batch 770/875, Loss: 1.0753\n    Batch 780/875, Loss: 0.7648\n    Batch 790/875, Loss: 0.4035\n    Batch 800/875, Loss: 0.5446\n    Batch 810/875, Loss: 0.9997\n    Batch 820/875, Loss: 0.5580\n    Batch 830/875, Loss: 0.5200\n    Batch 840/875, Loss: 0.4385\n    Batch 850/875, Loss: 0.4275\n    Batch 860/875, Loss: 1.2224\n    Batch 870/875, Loss: 0.5045\n    Batch 875/875, Loss: 0.8115\n    PSO Eval 10: LR=0.002743, Dropout=0.54, BS=8\n    First batch loaded, starting training...\n    Batch 10/875, Loss: 1.7316\n    Batch 20/875, Loss: 1.4460\n    Batch 30/875, Loss: 1.3061\n    Batch 40/875, Loss: 1.1724\n    Batch 50/875, Loss: 1.0537\n    Batch 60/875, Loss: 0.9689\n    Batch 70/875, Loss: 0.3411\n    Batch 80/875, Loss: 0.8624\n    Batch 90/875, Loss: 1.0330\n    Batch 100/875, Loss: 1.4730\n    Batch 110/875, Loss: 1.9216\n    Batch 120/875, Loss: 1.3532\n    Batch 130/875, Loss: 0.9665\n    Batch 140/875, Loss: 0.4877\n    Batch 150/875, Loss: 1.1544\n    Batch 160/875, Loss: 0.3049\n    Batch 170/875, Loss: 0.9147\n    Batch 180/875, Loss: 0.4153\n    Batch 190/875, Loss: 0.9089\n    Batch 200/875, Loss: 0.8473\n    Batch 210/875, Loss: 1.1068\n    Batch 220/875, Loss: 0.6806\n    Batch 230/875, Loss: 0.6519\n    Batch 240/875, Loss: 1.2429\n    Batch 250/875, Loss: 1.3343\n    Batch 260/875, Loss: 1.9524\n    Batch 270/875, Loss: 0.7895\n    Batch 280/875, Loss: 2.0477\n    Batch 290/875, Loss: 0.4332\n    Batch 300/875, Loss: 2.0096\n    Batch 310/875, Loss: 0.7520\n    Batch 320/875, Loss: 0.6735\n    Batch 330/875, Loss: 1.4980\n    Batch 340/875, Loss: 0.9913\n    Batch 350/875, Loss: 0.5688\n    Batch 360/875, Loss: 0.4464\n    Batch 370/875, Loss: 0.4907\n    Batch 380/875, Loss: 0.7250\n    Batch 390/875, Loss: 0.6729\n    Batch 400/875, Loss: 0.6228\n    Batch 410/875, Loss: 0.6341\n    Batch 420/875, Loss: 1.5513\n    Batch 430/875, Loss: 0.5799\n    Batch 440/875, Loss: 0.6906\n    Batch 450/875, Loss: 0.4382\n    Batch 460/875, Loss: 0.6479\n    Batch 470/875, Loss: 1.0544\n    Batch 480/875, Loss: 0.7563\n    Batch 490/875, Loss: 2.6421\n    Batch 500/875, Loss: 0.4737\n    Batch 510/875, Loss: 0.5085\n    Batch 520/875, Loss: 0.4051\n    Batch 530/875, Loss: 0.3443\n    Batch 540/875, Loss: 1.2376\n    Batch 550/875, Loss: 0.7319\n    Batch 560/875, Loss: 0.6852\n    Batch 570/875, Loss: 0.9574\n    Batch 580/875, Loss: 1.5975\n    Batch 590/875, Loss: 1.1268\n    Batch 600/875, Loss: 1.4634\n    Batch 610/875, Loss: 0.3950\n    Batch 620/875, Loss: 0.4703\n    Batch 630/875, Loss: 2.4243\n    Batch 640/875, Loss: 0.5738\n    Batch 650/875, Loss: 0.5496\n    Batch 660/875, Loss: 0.8181\n    Batch 670/875, Loss: 0.3052\n    Batch 680/875, Loss: 0.7713\n    Batch 690/875, Loss: 0.3932\n    Batch 700/875, Loss: 0.9589\n    Batch 710/875, Loss: 0.8068\n    Batch 720/875, Loss: 0.8671\n    Batch 730/875, Loss: 1.2191\n    Batch 740/875, Loss: 1.1102\n    Batch 750/875, Loss: 1.1497\n    Batch 760/875, Loss: 0.8841\n    Batch 770/875, Loss: 0.9698\n    Batch 780/875, Loss: 0.6631\n    Batch 790/875, Loss: 0.7778\n    Batch 800/875, Loss: 1.7902\n    Batch 810/875, Loss: 0.6861\n    Batch 820/875, Loss: 2.3263\n    Batch 830/875, Loss: 1.5400\n    Batch 840/875, Loss: 0.6182\n    Batch 850/875, Loss: 0.5823\n    Batch 860/875, Loss: 0.5237\n    Batch 870/875, Loss: 0.4661\n    Batch 875/875, Loss: 0.7889\n    PSO Eval 11: LR=0.000058, Dropout=0.20, BS=8\n    First batch loaded, starting training...\n    Batch 10/875, Loss: 1.3555\n    Batch 20/875, Loss: 1.2096\n    Batch 30/875, Loss: 1.2578\n    Batch 40/875, Loss: 0.9189\n    Batch 50/875, Loss: 0.8860\n    Batch 60/875, Loss: 0.8036\n    Batch 70/875, Loss: 0.9831\n    Batch 80/875, Loss: 0.6951\n    Batch 90/875, Loss: 0.8780\n    Batch 100/875, Loss: 1.3187\n    Batch 110/875, Loss: 1.2546\n    Batch 120/875, Loss: 1.2046\n    Batch 130/875, Loss: 1.1777\n    Batch 140/875, Loss: 1.4073\n    Batch 150/875, Loss: 0.7466\n    Batch 160/875, Loss: 0.8221\n    Batch 170/875, Loss: 1.1763\n    Batch 180/875, Loss: 1.9426\n    Batch 190/875, Loss: 0.7254\n    Batch 200/875, Loss: 1.1138\n    Batch 210/875, Loss: 1.2348\n    Batch 220/875, Loss: 1.2481\n    Batch 230/875, Loss: 1.1255\n    Batch 240/875, Loss: 0.8459\n    Batch 250/875, Loss: 0.6438\n    Batch 260/875, Loss: 1.3038\n    Batch 270/875, Loss: 2.1800\n    Batch 280/875, Loss: 1.0675\n    Batch 290/875, Loss: 0.7581\n    Batch 300/875, Loss: 1.4306\n    Batch 310/875, Loss: 1.0715\n    Batch 320/875, Loss: 0.6669\n    Batch 330/875, Loss: 2.0207\n    Batch 340/875, Loss: 0.8371\n    Batch 350/875, Loss: 0.4974\n    Batch 360/875, Loss: 1.0350\n    Batch 370/875, Loss: 0.9115\n    Batch 380/875, Loss: 0.6905\n    Batch 390/875, Loss: 2.0009\n    Batch 400/875, Loss: 2.0831\n    Batch 410/875, Loss: 0.7396\n    Batch 420/875, Loss: 0.9202\n    Batch 430/875, Loss: 0.9314\n    Batch 440/875, Loss: 1.2334\n    Batch 450/875, Loss: 0.7567\n    Batch 460/875, Loss: 1.2232\n    Batch 470/875, Loss: 1.0784\n    Batch 480/875, Loss: 0.5409\n    Batch 490/875, Loss: 0.8802\n    Batch 500/875, Loss: 1.1450\n    Batch 510/875, Loss: 1.8206\n    Batch 520/875, Loss: 1.1999\n    Batch 530/875, Loss: 0.8793\n    Batch 540/875, Loss: 1.0554\n    Batch 550/875, Loss: 1.0390\n    Batch 560/875, Loss: 1.2337\n    Batch 570/875, Loss: 0.6070\n    Batch 580/875, Loss: 1.0161\n    Batch 590/875, Loss: 0.6583\n    Batch 600/875, Loss: 0.9979\n    Batch 610/875, Loss: 0.5168\n    Batch 620/875, Loss: 1.2770\n    Batch 630/875, Loss: 1.1235\n    Batch 640/875, Loss: 1.2535\n    Batch 650/875, Loss: 0.6487\n    Batch 660/875, Loss: 1.1024\n    Batch 670/875, Loss: 0.5120\n    Batch 680/875, Loss: 1.7443\n    Batch 690/875, Loss: 0.6361\n    Batch 700/875, Loss: 0.5613\n    Batch 710/875, Loss: 0.5608\n    Batch 720/875, Loss: 0.8892\n    Batch 730/875, Loss: 0.9349\n    Batch 740/875, Loss: 0.7670\n    Batch 750/875, Loss: 0.6109\n    Batch 760/875, Loss: 1.6733\n    Batch 770/875, Loss: 0.9455\n    Batch 780/875, Loss: 1.0224\n    Batch 790/875, Loss: 1.5551\n    Batch 800/875, Loss: 0.8229\n    Batch 810/875, Loss: 0.5433\n    Batch 820/875, Loss: 1.4068\n    Batch 830/875, Loss: 0.5256\n    Batch 840/875, Loss: 1.0286\n    Batch 850/875, Loss: 0.6694\n    Batch 860/875, Loss: 0.9150\n    Batch 870/875, Loss: 0.4972\n    Batch 875/875, Loss: 0.8352\n    PSO Eval 12: LR=0.000046, Dropout=0.54, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 1.3454\n    Batch 20/219, Loss: 1.1924\n    Batch 30/219, Loss: 1.3604\n    Batch 40/219, Loss: 0.9673\n    Batch 50/219, Loss: 0.7655\n    Batch 60/219, Loss: 0.9119\n    Batch 70/219, Loss: 0.6635\n    Batch 80/219, Loss: 0.9783\n    Batch 90/219, Loss: 0.4913\n    Batch 100/219, Loss: 0.7620\n    Batch 110/219, Loss: 0.8587\n    Batch 120/219, Loss: 0.5914\n    Batch 130/219, Loss: 0.6031\n    Batch 140/219, Loss: 0.6256\n    Batch 150/219, Loss: 0.6725\n    Batch 160/219, Loss: 0.5471\n    Batch 170/219, Loss: 0.5079\n    Batch 180/219, Loss: 1.1449\n    Batch 190/219, Loss: 0.6572\n    Batch 200/219, Loss: 0.3543\n    Batch 210/219, Loss: 0.3264\n    Batch 219/219, Loss: 0.7136\n    PSO Eval 13: LR=0.000978, Dropout=0.31, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.6171\n    Batch 20/219, Loss: 0.3959\n    Batch 30/219, Loss: 0.7018\n    Batch 40/219, Loss: 0.9318\n    Batch 50/219, Loss: 0.4699\n    Batch 60/219, Loss: 0.7878\n    Batch 70/219, Loss: 0.1907\n    Batch 80/219, Loss: 0.3433\n    Batch 90/219, Loss: 0.3509\n    Batch 100/219, Loss: 0.3765\n    Batch 110/219, Loss: 0.7112\n    Batch 120/219, Loss: 0.6626\n    Batch 130/219, Loss: 0.3185\n    Batch 140/219, Loss: 0.2590\n    Batch 150/219, Loss: 0.1923\n    Batch 160/219, Loss: 0.1627\n    Batch 170/219, Loss: 0.1867\n    Batch 180/219, Loss: 0.3325\n    Batch 190/219, Loss: 0.2899\n    Batch 200/219, Loss: 1.6096\n    Batch 210/219, Loss: 0.4146\n    Batch 219/219, Loss: 0.4798\n    PSO Eval 14: LR=0.001114, Dropout=0.34, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 1.2051\n    Batch 20/219, Loss: 0.5527\n    Batch 30/219, Loss: 0.4902\n    Batch 40/219, Loss: 0.4573\n    Batch 50/219, Loss: 0.3820\n    Batch 60/219, Loss: 0.3251\n    Batch 70/219, Loss: 0.5548\n    Batch 80/219, Loss: 0.4733\n    Batch 90/219, Loss: 0.4879\n    Batch 100/219, Loss: 0.2773\n    Batch 110/219, Loss: 0.3519\n    Batch 120/219, Loss: 0.3492\n    Batch 130/219, Loss: 0.3909\n    Batch 140/219, Loss: 0.2932\n    Batch 150/219, Loss: 0.2260\n    Batch 160/219, Loss: 0.3704\n    Batch 170/219, Loss: 0.3649\n    Batch 180/219, Loss: 0.5348\n    Batch 190/219, Loss: 0.4433\n    Batch 200/219, Loss: 0.3864\n    Batch 210/219, Loss: 0.2818\n    Batch 219/219, Loss: 0.2003\n    PSO Eval 15: LR=0.000410, Dropout=0.25, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.8792\n    Batch 20/219, Loss: 0.9534\n    Batch 30/219, Loss: 0.9516\n    Batch 40/219, Loss: 1.3353\n    Batch 50/219, Loss: 0.7188\n    Batch 60/219, Loss: 0.5719\n    Batch 70/219, Loss: 0.7922\n    Batch 80/219, Loss: 0.6996\n    Batch 90/219, Loss: 0.7957\n    Batch 100/219, Loss: 0.6782\n    Batch 110/219, Loss: 0.5917\n    Batch 120/219, Loss: 0.6660\n    Batch 130/219, Loss: 0.5914\n    Batch 140/219, Loss: 0.8002\n    Batch 150/219, Loss: 0.3964\n    Batch 160/219, Loss: 0.7805\n    Batch 170/219, Loss: 0.7564\n    Batch 180/219, Loss: 0.9005\n    Batch 190/219, Loss: 0.5260\n    Batch 200/219, Loss: 0.5257\n    Batch 210/219, Loss: 0.6251\n    Batch 219/219, Loss: 0.6579\n    PSO Eval 16: LR=0.000871, Dropout=0.37, BS=16\n    First batch loaded, starting training...\n    Batch 10/438, Loss: 0.9297\n    Batch 20/438, Loss: 1.2453\n    Batch 30/438, Loss: 0.5509\n    Batch 40/438, Loss: 0.9837\n    Batch 50/438, Loss: 0.7307\n    Batch 60/438, Loss: 0.3344\n    Batch 70/438, Loss: 0.4574\n    Batch 80/438, Loss: 0.3202\n    Batch 90/438, Loss: 0.4441\n    Batch 100/438, Loss: 1.1141\n    Batch 110/438, Loss: 0.6745\n    Batch 120/438, Loss: 0.2212\n    Batch 130/438, Loss: 0.6688\n    Batch 140/438, Loss: 1.4519\n    Batch 150/438, Loss: 0.5509\n    Batch 160/438, Loss: 0.1956\n    Batch 170/438, Loss: 1.0622\n    Batch 180/438, Loss: 0.2682\n    Batch 190/438, Loss: 0.2747\n    Batch 200/438, Loss: 0.2193\n    Batch 210/438, Loss: 0.2150\n    Batch 220/438, Loss: 0.1731\n    Batch 230/438, Loss: 0.1868\n    Batch 240/438, Loss: 0.2458\n    Batch 250/438, Loss: 0.3986\n    Batch 260/438, Loss: 0.8442\n    Batch 270/438, Loss: 0.4366\n    Batch 280/438, Loss: 0.1670\n    Batch 290/438, Loss: 0.2954\n    Batch 300/438, Loss: 0.2816\n    Batch 310/438, Loss: 0.3016\n    Batch 320/438, Loss: 0.1848\n    Batch 330/438, Loss: 0.1362\n    Batch 340/438, Loss: 0.1661\n    Batch 350/438, Loss: 0.1419\n    Batch 360/438, Loss: 0.2208\n    Batch 370/438, Loss: 0.3845\n    Batch 380/438, Loss: 0.9492\n    Batch 390/438, Loss: 0.2211\n    Batch 400/438, Loss: 0.4334\n    Batch 410/438, Loss: 0.1126\n    Batch 420/438, Loss: 0.3956\n    Batch 430/438, Loss: 0.3003\n    Batch 438/438, Loss: 0.1140\n    PSO Eval 17: LR=0.000756, Dropout=0.37, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.8561\n    Batch 20/219, Loss: 0.6371\n    Batch 30/219, Loss: 0.4186\n    Batch 40/219, Loss: 0.4375\n    Batch 50/219, Loss: 0.4403\n    Batch 60/219, Loss: 0.4055\n    Batch 70/219, Loss: 0.3439\n    Batch 80/219, Loss: 0.4781\n    Batch 90/219, Loss: 0.4322\n    Batch 100/219, Loss: 0.3145\n    Batch 110/219, Loss: 0.3796\n    Batch 120/219, Loss: 0.3884\n    Batch 130/219, Loss: 0.3811\n    Batch 140/219, Loss: 0.1876\n    Batch 150/219, Loss: 0.1947\n    Batch 160/219, Loss: 0.2791\n    Batch 170/219, Loss: 0.3151\n    Batch 180/219, Loss: 0.4043\n    Batch 190/219, Loss: 0.1732\n    Batch 200/219, Loss: 0.2822\n    Batch 210/219, Loss: 0.1796\n    Batch 219/219, Loss: 0.1363\n    PSO Eval 18: LR=0.000464, Dropout=0.29, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.8418\n    Batch 20/219, Loss: 1.3353\n    Batch 30/219, Loss: 0.8998\n    Batch 40/219, Loss: 0.4770\n    Batch 50/219, Loss: 0.2926\n    Batch 60/219, Loss: 0.9610\n    Batch 70/219, Loss: 0.4084\n    Batch 80/219, Loss: 0.5525\n    Batch 90/219, Loss: 0.3259\n    Batch 100/219, Loss: 0.3659\n    Batch 110/219, Loss: 0.3456\n    Batch 120/219, Loss: 0.2016\n    Batch 130/219, Loss: 0.1958\n    Batch 140/219, Loss: 0.1785\n    Batch 150/219, Loss: 0.2622\n    Batch 160/219, Loss: 0.3786\n    Batch 170/219, Loss: 0.1813\n    Batch 180/219, Loss: 0.2075\n    Batch 190/219, Loss: 0.1476\n    Batch 200/219, Loss: 0.2511\n    Batch 210/219, Loss: 0.6374\n    Batch 219/219, Loss: 0.1455\n    PSO Eval 19: LR=0.001199, Dropout=0.34, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.9304\n    Batch 20/219, Loss: 0.8884\n    Batch 30/219, Loss: 0.4609\n    Batch 40/219, Loss: 0.3175\n    Batch 50/219, Loss: 0.4603\n    Batch 60/219, Loss: 0.3612\n    Batch 70/219, Loss: 0.3102\n    Batch 80/219, Loss: 0.3456\n    Batch 90/219, Loss: 0.2677\n    Batch 100/219, Loss: 0.2977\n    Batch 110/219, Loss: 0.4310\n    Batch 120/219, Loss: 0.3253\n    Batch 130/219, Loss: 0.3091\n    Batch 140/219, Loss: 0.3046\n    Batch 150/219, Loss: 0.4066\n    Batch 160/219, Loss: 0.4457\n    Batch 170/219, Loss: 0.3082\n    Batch 180/219, Loss: 0.2982\n    Batch 190/219, Loss: 0.4096\n    Batch 200/219, Loss: 0.1362\n    Batch 210/219, Loss: 0.4741\n    Batch 219/219, Loss: 0.0905\n    PSO Eval 20: LR=0.001807, Dropout=0.48, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.7690\n    Batch 20/219, Loss: 0.8630\n    Batch 30/219, Loss: 0.8783\n    Batch 40/219, Loss: 0.5456\n    Batch 50/219, Loss: 0.7982\n    Batch 60/219, Loss: 0.7756\n    Batch 70/219, Loss: 0.8851\n    Batch 80/219, Loss: 0.4735\n    Batch 90/219, Loss: 0.7308\n    Batch 100/219, Loss: 0.7217\n    Batch 110/219, Loss: 0.7567\n    Batch 120/219, Loss: 0.8499\n    Batch 130/219, Loss: 0.6073\n    Batch 140/219, Loss: 0.7351\n    Batch 150/219, Loss: 0.4018\n    Batch 160/219, Loss: 0.5614\n    Batch 170/219, Loss: 0.7541\n    Batch 180/219, Loss: 0.8127\n    Batch 190/219, Loss: 0.8925\n    Batch 200/219, Loss: 0.7831\n    Batch 210/219, Loss: 0.7343\n    Batch 219/219, Loss: 0.8611\n    PSO Eval 21: LR=0.001778, Dropout=0.31, BS=8\n    First batch loaded, starting training...\n    Batch 10/875, Loss: 1.1422\n    Batch 20/875, Loss: 1.1978\n    Batch 30/875, Loss: 0.7793\n    Batch 40/875, Loss: 0.6795\n    Batch 50/875, Loss: 2.4408\n    Batch 60/875, Loss: 0.6620\n    Batch 70/875, Loss: 0.7106\n    Batch 80/875, Loss: 0.8802\n    Batch 90/875, Loss: 1.1574\n    Batch 100/875, Loss: 0.5696\n    Batch 110/875, Loss: 1.2011\n    Batch 120/875, Loss: 1.6741\n    Batch 130/875, Loss: 0.9033\n    Batch 140/875, Loss: 1.2778\n    Batch 150/875, Loss: 1.2518\n    Batch 160/875, Loss: 0.6327\n    Batch 170/875, Loss: 1.8435\n    Batch 180/875, Loss: 0.9295\n    Batch 190/875, Loss: 0.4895\n    Batch 200/875, Loss: 0.4858\n    Batch 210/875, Loss: 0.4347\n    Batch 220/875, Loss: 2.3982\n    Batch 230/875, Loss: 0.7973\n    Batch 240/875, Loss: 1.0147\n    Batch 250/875, Loss: 0.4286\n    Batch 260/875, Loss: 0.5881\n    Batch 270/875, Loss: 0.4867\n    Batch 280/875, Loss: 1.1568\n    Batch 290/875, Loss: 0.5257\n    Batch 300/875, Loss: 1.6355\n    Batch 310/875, Loss: 0.7308\n    Batch 320/875, Loss: 0.5370\n    Batch 330/875, Loss: 0.8025\n    Batch 340/875, Loss: 0.7562\n    Batch 350/875, Loss: 0.4778\n    Batch 360/875, Loss: 0.7343\n    Batch 370/875, Loss: 0.6631\n    Batch 380/875, Loss: 0.5706\n    Batch 390/875, Loss: 0.6861\n    Batch 400/875, Loss: 2.9269\n    Batch 410/875, Loss: 0.4706\n    Batch 420/875, Loss: 0.9700\n    Batch 430/875, Loss: 1.4244\n    Batch 440/875, Loss: 0.9437\n    Batch 450/875, Loss: 0.6348\n    Batch 460/875, Loss: 0.8608\n    Batch 470/875, Loss: 0.4123\n    Batch 480/875, Loss: 0.7724\n    Batch 490/875, Loss: 0.7629\n    Batch 500/875, Loss: 0.2590\n    Batch 510/875, Loss: 0.5333\n    Batch 520/875, Loss: 1.0192\n    Batch 530/875, Loss: 1.0245\n    Batch 540/875, Loss: 0.8238\n    Batch 550/875, Loss: 0.5739\n    Batch 560/875, Loss: 0.4396\n    Batch 570/875, Loss: 0.6329\n    Batch 580/875, Loss: 0.7799\n    Batch 590/875, Loss: 1.3497\n    Batch 600/875, Loss: 0.7863\n    Batch 610/875, Loss: 0.4699\n    Batch 620/875, Loss: 0.9596\n    Batch 630/875, Loss: 0.4256\n    Batch 640/875, Loss: 0.4141\n    Batch 650/875, Loss: 0.4275\n    Batch 660/875, Loss: 1.7729\n    Batch 670/875, Loss: 0.4185\n    Batch 680/875, Loss: 0.3298\n    Batch 690/875, Loss: 0.6515\n    Batch 700/875, Loss: 2.1706\n    Batch 710/875, Loss: 0.4849\n    Batch 720/875, Loss: 0.5665\n    Batch 730/875, Loss: 1.4283\n    Batch 740/875, Loss: 0.3386\n    Batch 750/875, Loss: 0.9073\n    Batch 760/875, Loss: 0.6199\n    Batch 770/875, Loss: 0.9964\n    Batch 780/875, Loss: 0.5966\n    Batch 790/875, Loss: 1.7582\n    Batch 800/875, Loss: 0.6899\n    Batch 810/875, Loss: 1.9788\n    Batch 820/875, Loss: 1.5706\n    Batch 830/875, Loss: 0.4504\n    Batch 840/875, Loss: 0.6525\n    Batch 850/875, Loss: 0.8275\n    Batch 860/875, Loss: 0.4242\n    Batch 870/875, Loss: 0.5104\n    Batch 875/875, Loss: 0.4123\n    PSO Eval 22: LR=0.002678, Dropout=0.50, BS=16\n    First batch loaded, starting training...\n    Batch 10/438, Loss: 0.9224\n    Batch 20/438, Loss: 0.8195\n    Batch 30/438, Loss: 0.7651\n    Batch 40/438, Loss: 1.0439\n    Batch 50/438, Loss: 1.0918\n    Batch 60/438, Loss: 0.4708\n    Batch 70/438, Loss: 0.3042\n    Batch 80/438, Loss: 0.5023\n    Batch 90/438, Loss: 1.1929\n    Batch 100/438, Loss: 1.1517\n    Batch 110/438, Loss: 0.6472\n    Batch 120/438, Loss: 0.8958\n    Batch 130/438, Loss: 0.5431\n    Batch 140/438, Loss: 0.8425\n    Batch 150/438, Loss: 0.6064\n    Batch 160/438, Loss: 0.5990\n    Batch 170/438, Loss: 0.4389\n    Batch 180/438, Loss: 0.4699\n    Batch 190/438, Loss: 0.6464\n    Batch 200/438, Loss: 0.7189\n    Batch 210/438, Loss: 1.0496\n    Batch 220/438, Loss: 1.2268\n    Batch 230/438, Loss: 0.5825\n    Batch 240/438, Loss: 0.4902\n    Batch 250/438, Loss: 0.7221\n    Batch 260/438, Loss: 1.0478\n    Batch 270/438, Loss: 0.4860\n    Batch 280/438, Loss: 1.2257\n    Batch 290/438, Loss: 0.5936\n    Batch 300/438, Loss: 0.6995\n    Batch 310/438, Loss: 0.4010\n    Batch 320/438, Loss: 0.4645\n    Batch 330/438, Loss: 0.5181\n    Batch 340/438, Loss: 0.6854\n    Batch 350/438, Loss: 0.6280\n    Batch 360/438, Loss: 0.4566\n    Batch 370/438, Loss: 0.7055\n    Batch 380/438, Loss: 0.4838\n    Batch 390/438, Loss: 0.5687\n    Batch 400/438, Loss: 0.3956\n    Batch 410/438, Loss: 0.4973\n    Batch 420/438, Loss: 0.2228\n    Batch 430/438, Loss: 1.1147\n    Batch 438/438, Loss: 0.3283\n    PSO Eval 23: LR=0.000316, Dropout=0.28, BS=16\n    First batch loaded, starting training...\n    Batch 10/438, Loss: 1.0440\n    Batch 20/438, Loss: 0.8547\n    Batch 30/438, Loss: 1.0527\n    Batch 40/438, Loss: 0.9455\n    Batch 50/438, Loss: 0.7613\n    Batch 60/438, Loss: 0.6215\n    Batch 70/438, Loss: 1.3824\n    Batch 80/438, Loss: 0.7802\n    Batch 90/438, Loss: 0.7632\n    Batch 100/438, Loss: 0.5919\n    Batch 110/438, Loss: 0.9190\n    Batch 120/438, Loss: 0.5261\n    Batch 130/438, Loss: 0.8129\n    Batch 140/438, Loss: 1.2671\n    Batch 150/438, Loss: 0.5006\n    Batch 160/438, Loss: 0.4860\n    Batch 170/438, Loss: 0.8736\n    Batch 180/438, Loss: 0.7078\n    Batch 190/438, Loss: 0.7897\n    Batch 200/438, Loss: 0.6117\n    Batch 210/438, Loss: 0.9834\n    Batch 220/438, Loss: 0.5293\n    Batch 230/438, Loss: 0.8270\n    Batch 240/438, Loss: 0.4629\n    Batch 250/438, Loss: 0.6484\n    Batch 260/438, Loss: 0.8800\n    Batch 270/438, Loss: 0.5473\n    Batch 280/438, Loss: 0.6930\n    Batch 290/438, Loss: 0.7369\n    Batch 300/438, Loss: 0.6847\n    Batch 310/438, Loss: 0.2935\n    Batch 320/438, Loss: 0.7420\n    Batch 330/438, Loss: 0.4343\n    Batch 340/438, Loss: 1.1516\n    Batch 350/438, Loss: 0.7105\n    Batch 360/438, Loss: 0.5569\n    Batch 370/438, Loss: 0.5636\n    Batch 380/438, Loss: 0.6568\n    Batch 390/438, Loss: 0.5629\n    Batch 400/438, Loss: 0.5176\n    Batch 410/438, Loss: 0.5186\n    Batch 420/438, Loss: 0.5741\n    Batch 430/438, Loss: 0.7202\n    Batch 438/438, Loss: 1.6342\n    PSO Eval 24: LR=0.000755, Dropout=0.39, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.6631\n    Batch 20/219, Loss: 0.6643\n    Batch 30/219, Loss: 0.8374\n    Batch 40/219, Loss: 0.4046\n    Batch 50/219, Loss: 0.5175\n    Batch 60/219, Loss: 0.3891\n    Batch 70/219, Loss: 0.3504\n    Batch 80/219, Loss: 0.2406\n    Batch 90/219, Loss: 0.2879\n    Batch 100/219, Loss: 0.3919\n    Batch 110/219, Loss: 0.3698\n    Batch 120/219, Loss: 0.2403\n    Batch 130/219, Loss: 0.1962\n    Batch 140/219, Loss: 0.3345\n    Batch 150/219, Loss: 0.4600\n    Batch 160/219, Loss: 0.2387\n    Batch 170/219, Loss: 0.2203\n    Batch 180/219, Loss: 0.1848\n    Batch 190/219, Loss: 0.2527\n    Batch 200/219, Loss: 0.2991\n    Batch 210/219, Loss: 0.2413\n    Batch 219/219, Loss: 0.2371\n  PSO Iter 1/4: Best = 0.7949\n    PSO Eval 25: LR=0.000889, Dropout=0.23, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.9721\n    Batch 20/219, Loss: 0.7352\n    Batch 30/219, Loss: 0.5911\n    Batch 40/219, Loss: 0.4022\n    Batch 50/219, Loss: 0.4371\n    Batch 60/219, Loss: 0.5926\n    Batch 70/219, Loss: 0.2749\n    Batch 80/219, Loss: 0.7614\n    Batch 90/219, Loss: 0.4287\n    Batch 100/219, Loss: 0.3753\n    Batch 110/219, Loss: 0.2307\n    Batch 120/219, Loss: 0.5788\n    Batch 130/219, Loss: 0.3611\n    Batch 140/219, Loss: 0.2839\n    Batch 150/219, Loss: 0.3832\n    Batch 160/219, Loss: 0.3614\n    Batch 170/219, Loss: 0.2559\n    Batch 180/219, Loss: 0.2636\n    Batch 190/219, Loss: 0.4370\n    Batch 200/219, Loss: 0.2568\n    Batch 210/219, Loss: 0.2260\n    Batch 219/219, Loss: 0.3665\n    PSO Eval 26: LR=0.003000, Dropout=0.36, BS=16\n    First batch loaded, starting training...\n    Batch 10/438, Loss: 1.0705\n    Batch 20/438, Loss: 1.0277\n    Batch 30/438, Loss: 1.1201\n    Batch 40/438, Loss: 1.0868\n    Batch 50/438, Loss: 1.0371\n    Batch 60/438, Loss: 0.7752\n    Batch 70/438, Loss: 0.7056\n    Batch 80/438, Loss: 0.9740\n    Batch 90/438, Loss: 0.5444\n    Batch 100/438, Loss: 0.7613\n    Batch 110/438, Loss: 0.8484\n    Batch 120/438, Loss: 1.1843\n    Batch 130/438, Loss: 0.9904\n    Batch 140/438, Loss: 0.3711\n    Batch 150/438, Loss: 1.0242\n    Batch 160/438, Loss: 0.6505\n    Batch 170/438, Loss: 0.6592\n    Batch 180/438, Loss: 1.0438\n    Batch 190/438, Loss: 1.1533\n    Batch 200/438, Loss: 0.3865\n    Batch 210/438, Loss: 0.9163\n    Batch 220/438, Loss: 0.4237\n    Batch 230/438, Loss: 0.5859\n    Batch 240/438, Loss: 0.4972\n    Batch 250/438, Loss: 0.5060\n    Batch 260/438, Loss: 0.8919\n    Batch 270/438, Loss: 0.5900\n    Batch 280/438, Loss: 0.5071\n    Batch 290/438, Loss: 0.5769\n    Batch 300/438, Loss: 0.4494\n    Batch 310/438, Loss: 0.6749\n    Batch 320/438, Loss: 0.7516\n    Batch 330/438, Loss: 0.8995\n    Batch 340/438, Loss: 0.3309\n    Batch 350/438, Loss: 1.3277\n    Batch 360/438, Loss: 0.5398\n    Batch 370/438, Loss: 1.4449\n    Batch 380/438, Loss: 0.5729\n    Batch 390/438, Loss: 1.1405\n    Batch 400/438, Loss: 1.8037\n    Batch 410/438, Loss: 0.6617\n    Batch 420/438, Loss: 0.4336\n    Batch 430/438, Loss: 0.6209\n    Batch 438/438, Loss: 0.7897\n    PSO Eval 27: LR=0.002795, Dropout=0.43, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.7689\n    Batch 20/219, Loss: 1.2017\n    Batch 30/219, Loss: 0.6081\n    Batch 40/219, Loss: 0.4651\n    Batch 50/219, Loss: 0.5973\n    Batch 60/219, Loss: 0.4125\n    Batch 70/219, Loss: 0.3554\n    Batch 80/219, Loss: 0.5586\n    Batch 90/219, Loss: 0.4804\n    Batch 100/219, Loss: 0.5118\n    Batch 110/219, Loss: 0.4387\n    Batch 120/219, Loss: 0.4591\n    Batch 130/219, Loss: 0.7135\n    Batch 140/219, Loss: 0.3099\n    Batch 150/219, Loss: 0.3704\n    Batch 160/219, Loss: 0.4490\n    Batch 170/219, Loss: 0.2424\n    Batch 180/219, Loss: 0.3505\n    Batch 190/219, Loss: 0.3130\n    Batch 200/219, Loss: 0.3304\n    Batch 210/219, Loss: 0.3050\n    Batch 219/219, Loss: 0.3836\n    PSO Eval 28: LR=0.000641, Dropout=0.35, BS=16\n    First batch loaded, starting training...\n    Batch 10/438, Loss: 0.8140\n    Batch 20/438, Loss: 0.8968\n    Batch 30/438, Loss: 0.7904\n    Batch 40/438, Loss: 0.4297\n    Batch 50/438, Loss: 0.2416\n    Batch 60/438, Loss: 0.6151\n    Batch 70/438, Loss: 1.4302\n    Batch 80/438, Loss: 0.5774\n    Batch 90/438, Loss: 0.2171\n    Batch 100/438, Loss: 0.6310\n    Batch 110/438, Loss: 0.7628\n    Batch 120/438, Loss: 0.3521\n    Batch 130/438, Loss: 0.3041\n    Batch 140/438, Loss: 0.2326\n    Batch 150/438, Loss: 0.4982\n    Batch 160/438, Loss: 0.7394\n    Batch 170/438, Loss: 0.4124\n    Batch 180/438, Loss: 0.2820\n    Batch 190/438, Loss: 0.3864\n    Batch 200/438, Loss: 0.8404\n    Batch 210/438, Loss: 0.3428\n    Batch 220/438, Loss: 0.5302\n    Batch 230/438, Loss: 0.2676\n    Batch 240/438, Loss: 0.1419\n    Batch 250/438, Loss: 0.3182\n    Batch 260/438, Loss: 0.1806\n    Batch 270/438, Loss: 0.4487\n    Batch 280/438, Loss: 0.4127\n    Batch 290/438, Loss: 0.4154\n    Batch 300/438, Loss: 0.2430\n    Batch 310/438, Loss: 0.1572\n    Batch 320/438, Loss: 0.7339\n    Batch 330/438, Loss: 0.2332\n    Batch 340/438, Loss: 0.4169\n    Batch 350/438, Loss: 0.1666\n    Batch 360/438, Loss: 0.2769\n    Batch 370/438, Loss: 0.2257\n    Batch 380/438, Loss: 0.1800\n    Batch 390/438, Loss: 0.3724\n    Batch 400/438, Loss: 0.4502\n    Batch 410/438, Loss: 0.2344\n    Batch 420/438, Loss: 0.0910\n    Batch 430/438, Loss: 0.1082\n    Batch 438/438, Loss: 0.1326\n    PSO Eval 29: LR=0.003000, Dropout=0.22, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.7806\n    Batch 20/219, Loss: 0.6070\n    Batch 30/219, Loss: 0.5668\n    Batch 40/219, Loss: 0.3922\n    Batch 50/219, Loss: 0.4693\n    Batch 60/219, Loss: 0.8995\n    Batch 70/219, Loss: 0.3402\n    Batch 80/219, Loss: 0.3471\n    Batch 90/219, Loss: 0.2863\n    Batch 100/219, Loss: 0.4664\n    Batch 110/219, Loss: 0.4724\n    Batch 120/219, Loss: 0.4002\n    Batch 130/219, Loss: 0.5152\n    Batch 140/219, Loss: 0.3982\n    Batch 150/219, Loss: 0.4411\n    Batch 160/219, Loss: 0.2755\n    Batch 170/219, Loss: 0.6456\n    Batch 180/219, Loss: 0.4856\n    Batch 190/219, Loss: 0.2056\n    Batch 200/219, Loss: 0.2499\n    Batch 210/219, Loss: 0.2894\n    Batch 219/219, Loss: 0.2725\n    PSO Eval 30: LR=0.003000, Dropout=0.40, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.8295\n    Batch 20/219, Loss: 0.9571\n    Batch 30/219, Loss: 0.6614\n    Batch 40/219, Loss: 0.4182\n    Batch 50/219, Loss: 0.4871\n    Batch 60/219, Loss: 0.6294\n    Batch 70/219, Loss: 0.5461\n    Batch 80/219, Loss: 0.4552\n    Batch 90/219, Loss: 0.2196\n    Batch 100/219, Loss: 0.4295\n    Batch 110/219, Loss: 0.4963\n    Batch 120/219, Loss: 0.4463\n    Batch 130/219, Loss: 0.2328\n    Batch 140/219, Loss: 0.4513\n    Batch 150/219, Loss: 0.4436\n    Batch 160/219, Loss: 1.0766\n    Batch 170/219, Loss: 0.9026\n    Batch 180/219, Loss: 0.1478\n    Batch 190/219, Loss: 0.1776\n    Batch 200/219, Loss: 0.3319\n    Batch 210/219, Loss: 0.2209\n    Batch 219/219, Loss: 0.3203\n    PSO Eval 31: LR=0.001263, Dropout=0.36, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 1.0071\n    Batch 20/219, Loss: 0.9518\n    Batch 30/219, Loss: 0.7589\n    Batch 40/219, Loss: 0.4813\n    Batch 50/219, Loss: 0.6518\n    Batch 60/219, Loss: 0.3663\n    Batch 70/219, Loss: 0.6804\n    Batch 80/219, Loss: 0.5130\n    Batch 90/219, Loss: 0.3183\n    Batch 100/219, Loss: 0.2706\n    Batch 110/219, Loss: 0.2176\n    Batch 120/219, Loss: 0.3970\n    Batch 130/219, Loss: 0.2611\n    Batch 140/219, Loss: 0.1088\n    Batch 150/219, Loss: 0.1425\n    Batch 160/219, Loss: 0.3459\n    Batch 170/219, Loss: 0.2883\n    Batch 180/219, Loss: 0.5301\n    Batch 190/219, Loss: 0.3921\n    Batch 200/219, Loss: 0.3425\n    Batch 210/219, Loss: 0.4390\n    Batch 219/219, Loss: 0.4012\n    PSO Eval 32: LR=0.002022, Dropout=0.46, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.8303\n    Batch 20/219, Loss: 0.8517\n    Batch 30/219, Loss: 0.6803\n    Batch 40/219, Loss: 0.4407\n    Batch 50/219, Loss: 0.7349\n    Batch 60/219, Loss: 0.5426\n    Batch 70/219, Loss: 0.6950\n    Batch 80/219, Loss: 1.0092\n    Batch 90/219, Loss: 0.6047\n    Batch 100/219, Loss: 0.8887\n    Batch 110/219, Loss: 0.6811\n    Batch 120/219, Loss: 0.9420\n    Batch 130/219, Loss: 0.9720\n    Batch 140/219, Loss: 2.2011\n    Batch 150/219, Loss: 0.6592\n    Batch 160/219, Loss: 0.5305\n    Batch 170/219, Loss: 0.6054\n    Batch 180/219, Loss: 0.4675\n    Batch 190/219, Loss: 1.3324\n    Batch 200/219, Loss: 0.7792\n    Batch 210/219, Loss: 0.8499\n    Batch 219/219, Loss: 0.6418\n    PSO Eval 33: LR=0.001811, Dropout=0.33, BS=8\n    First batch loaded, starting training...\n    Batch 10/875, Loss: 2.5423\n    Batch 20/875, Loss: 0.7627\n    Batch 30/875, Loss: 2.4186\n    Batch 40/875, Loss: 1.0424\n    Batch 50/875, Loss: 0.8295\n    Batch 60/875, Loss: 1.1352\n    Batch 70/875, Loss: 0.5231\n    Batch 80/875, Loss: 0.5946\n    Batch 90/875, Loss: 1.9443\n    Batch 100/875, Loss: 0.6360\n    Batch 110/875, Loss: 0.8516\n    Batch 120/875, Loss: 0.8331\n    Batch 130/875, Loss: 0.2984\n    Batch 140/875, Loss: 2.3829\n    Batch 150/875, Loss: 0.5316\n    Batch 160/875, Loss: 0.4372\n    Batch 170/875, Loss: 0.3788\n    Batch 180/875, Loss: 1.1851\n    Batch 190/875, Loss: 1.0884\n    Batch 200/875, Loss: 0.5090\n    Batch 210/875, Loss: 0.8187\n    Batch 220/875, Loss: 1.0583\n    Batch 230/875, Loss: 1.0469\n    Batch 240/875, Loss: 0.4406\n    Batch 250/875, Loss: 0.9753\n    Batch 260/875, Loss: 1.1238\n    Batch 270/875, Loss: 0.9412\n    Batch 280/875, Loss: 0.5378\n    Batch 290/875, Loss: 1.2273\n    Batch 300/875, Loss: 0.6859\n    Batch 310/875, Loss: 0.9018\n    Batch 320/875, Loss: 1.1652\n    Batch 330/875, Loss: 1.3401\n    Batch 340/875, Loss: 1.0487\n    Batch 350/875, Loss: 0.8158\n    Batch 360/875, Loss: 0.4031\n    Batch 370/875, Loss: 0.5298\n    Batch 380/875, Loss: 0.4258\n    Batch 390/875, Loss: 0.3363\n    Batch 400/875, Loss: 0.3960\n    Batch 410/875, Loss: 1.0248\n    Batch 420/875, Loss: 0.6259\n    Batch 430/875, Loss: 0.5121\n    Batch 440/875, Loss: 0.7632\n    Batch 450/875, Loss: 0.6147\n    Batch 460/875, Loss: 0.5227\n    Batch 470/875, Loss: 0.4295\n    Batch 480/875, Loss: 0.5188\n    Batch 490/875, Loss: 0.5444\n    Batch 500/875, Loss: 0.8738\n    Batch 510/875, Loss: 0.5949\n    Batch 520/875, Loss: 0.8735\n    Batch 530/875, Loss: 0.9468\n    Batch 540/875, Loss: 0.8967\n    Batch 550/875, Loss: 1.8295\n    Batch 560/875, Loss: 0.4658\n    Batch 570/875, Loss: 0.6796\n    Batch 580/875, Loss: 0.6794\n    Batch 590/875, Loss: 0.8535\n    Batch 600/875, Loss: 0.5666\n    Batch 610/875, Loss: 0.8505\n    Batch 620/875, Loss: 0.5388\n    Batch 630/875, Loss: 0.6677\n    Batch 640/875, Loss: 1.0058\n    Batch 650/875, Loss: 0.2641\n    Batch 660/875, Loss: 1.0108\n    Batch 670/875, Loss: 0.9479\n    Batch 680/875, Loss: 0.4862\n    Batch 690/875, Loss: 1.1762\n    Batch 700/875, Loss: 0.7642\n    Batch 710/875, Loss: 1.0304\n    Batch 720/875, Loss: 0.5040\n    Batch 730/875, Loss: 0.6046\n    Batch 740/875, Loss: 0.3714\n    Batch 750/875, Loss: 0.6535\n    Batch 760/875, Loss: 0.9339\n    Batch 770/875, Loss: 0.7212\n    Batch 780/875, Loss: 0.5945\n    Batch 790/875, Loss: 0.7425\n    Batch 800/875, Loss: 0.9889\n    Batch 810/875, Loss: 0.7388\n    Batch 820/875, Loss: 0.6463\n    Batch 830/875, Loss: 1.4299\n    Batch 840/875, Loss: 0.8307\n    Batch 850/875, Loss: 0.4461\n    Batch 860/875, Loss: 0.6058\n    Batch 870/875, Loss: 0.6204\n    Batch 875/875, Loss: 0.6379\n    PSO Eval 34: LR=0.002221, Dropout=0.44, BS=16\n    First batch loaded, starting training...\n    Batch 10/438, Loss: 1.0163\n    Batch 20/438, Loss: 0.8673\n    Batch 30/438, Loss: 0.6416\n    Batch 40/438, Loss: 0.6638\n    Batch 50/438, Loss: 0.6882\n    Batch 60/438, Loss: 0.5408\n    Batch 70/438, Loss: 0.6185\n    Batch 80/438, Loss: 0.6123\n    Batch 90/438, Loss: 0.4636\n    Batch 100/438, Loss: 1.2292\n    Batch 110/438, Loss: 1.1760\n    Batch 120/438, Loss: 0.3846\n    Batch 130/438, Loss: 0.4160\n    Batch 140/438, Loss: 0.4226\n    Batch 150/438, Loss: 0.5306\n    Batch 160/438, Loss: 0.5715\n    Batch 170/438, Loss: 0.4228\n    Batch 180/438, Loss: 0.7255\n    Batch 190/438, Loss: 0.1969\n    Batch 200/438, Loss: 0.5270\n    Batch 210/438, Loss: 1.2008\n    Batch 220/438, Loss: 0.5446\n    Batch 230/438, Loss: 0.4218\n    Batch 240/438, Loss: 1.3485\n    Batch 250/438, Loss: 0.2711\n    Batch 260/438, Loss: 0.4703\n    Batch 270/438, Loss: 0.4247\n    Batch 280/438, Loss: 0.4321\n    Batch 290/438, Loss: 0.3842\n    Batch 300/438, Loss: 1.3277\n    Batch 310/438, Loss: 0.5238\n    Batch 320/438, Loss: 0.3112\n    Batch 330/438, Loss: 0.1281\n    Batch 340/438, Loss: 0.4160\n    Batch 350/438, Loss: 1.5488\n    Batch 360/438, Loss: 0.5361\n    Batch 370/438, Loss: 0.4806\n    Batch 380/438, Loss: 0.5745\n    Batch 390/438, Loss: 0.3520\n    Batch 400/438, Loss: 0.3646\n    Batch 410/438, Loss: 0.5313\n    Batch 420/438, Loss: 0.8990\n    Batch 430/438, Loss: 0.5866\n    Batch 438/438, Loss: 1.0465\n    PSO Eval 35: LR=0.003000, Dropout=0.42, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.7528\n    Batch 20/219, Loss: 0.8123\n    Batch 30/219, Loss: 0.5083\n    Batch 40/219, Loss: 0.6626\n    Batch 50/219, Loss: 0.3788\n    Batch 60/219, Loss: 0.4880\n    Batch 70/219, Loss: 0.6186\n    Batch 80/219, Loss: 0.5837\n    Batch 90/219, Loss: 0.3397\n    Batch 100/219, Loss: 0.5434\n    Batch 110/219, Loss: 0.3230\n    Batch 120/219, Loss: 0.2587\n    Batch 130/219, Loss: 0.5079\n    Batch 140/219, Loss: 0.3601\n    Batch 150/219, Loss: 0.8319\n    Batch 160/219, Loss: 0.3763\n    Batch 170/219, Loss: 0.1951\n    Batch 180/219, Loss: 0.4654\n    Batch 190/219, Loss: 0.3899\n    Batch 200/219, Loss: 0.5328\n    Batch 210/219, Loss: 0.2749\n    Batch 219/219, Loss: 0.8371\n    PSO Eval 36: LR=0.003000, Dropout=0.22, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.8440\n    Batch 20/219, Loss: 0.7553\n    Batch 30/219, Loss: 0.9429\n    Batch 40/219, Loss: 0.5608\n    Batch 50/219, Loss: 1.1908\n    Batch 60/219, Loss: 0.8110\n    Batch 70/219, Loss: 0.6826\n    Batch 80/219, Loss: 0.7520\n    Batch 90/219, Loss: 0.5651\n    Batch 100/219, Loss: 0.7089\n    Batch 110/219, Loss: 0.9188\n    Batch 120/219, Loss: 0.8211\n    Batch 130/219, Loss: 0.6168\n    Batch 140/219, Loss: 0.6426\n    Batch 150/219, Loss: 0.7115\n    Batch 160/219, Loss: 0.5993\n    Batch 170/219, Loss: 0.3874\n    Batch 180/219, Loss: 0.8145\n    Batch 190/219, Loss: 0.6138\n    Batch 200/219, Loss: 0.6465\n    Batch 210/219, Loss: 0.4806\n    Batch 219/219, Loss: 0.6405\n  PSO Iter 2/4: Best = 0.7949\n    PSO Eval 37: LR=0.000868, Dropout=0.19, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 1.1340\n    Batch 20/219, Loss: 0.5690\n    Batch 30/219, Loss: 0.6314\n    Batch 40/219, Loss: 0.4475\n    Batch 50/219, Loss: 0.4193\n    Batch 60/219, Loss: 0.5401\n    Batch 70/219, Loss: 0.2737\n    Batch 80/219, Loss: 0.4738\n    Batch 90/219, Loss: 0.7816\n    Batch 100/219, Loss: 0.3937\n    Batch 110/219, Loss: 0.2564\n    Batch 120/219, Loss: 0.2201\n    Batch 130/219, Loss: 0.6418\n    Batch 140/219, Loss: 0.7386\n    Batch 150/219, Loss: 0.2176\n    Batch 160/219, Loss: 0.5321\n    Batch 170/219, Loss: 0.1472\n    Batch 180/219, Loss: 0.6076\n    Batch 190/219, Loss: 0.1758\n    Batch 200/219, Loss: 0.2008\n    Batch 210/219, Loss: 0.4705\n    Batch 219/219, Loss: 0.2701\n    PSO Eval 38: LR=0.001966, Dropout=0.34, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.5354\n    Batch 20/219, Loss: 0.6009\n    Batch 30/219, Loss: 0.6326\n    Batch 40/219, Loss: 0.4938\n    Batch 50/219, Loss: 0.4305\n    Batch 60/219, Loss: 0.2048\n    Batch 70/219, Loss: 0.4133\n    Batch 80/219, Loss: 0.6824\n    Batch 90/219, Loss: 0.5093\n    Batch 100/219, Loss: 0.8191\n    Batch 110/219, Loss: 0.3967\n    Batch 120/219, Loss: 0.2408\n    Batch 130/219, Loss: 0.2474\n    Batch 140/219, Loss: 0.3524\n    Batch 150/219, Loss: 0.3193\n    Batch 160/219, Loss: 0.4357\n    Batch 170/219, Loss: 0.2681\n    Batch 180/219, Loss: 0.6210\n    Batch 190/219, Loss: 0.3782\n    Batch 200/219, Loss: 0.3564\n    Batch 210/219, Loss: 0.2813\n    Batch 219/219, Loss: 0.4275\n    PSO Eval 39: LR=0.003000, Dropout=0.43, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 1.1906\n    Batch 20/219, Loss: 1.0587\n    Batch 30/219, Loss: 0.7935\n    Batch 40/219, Loss: 0.4915\n    Batch 50/219, Loss: 0.4290\n    Batch 60/219, Loss: 0.5977\n    Batch 70/219, Loss: 0.4770\n    Batch 80/219, Loss: 0.5401\n    Batch 90/219, Loss: 0.3541\n    Batch 100/219, Loss: 0.2672\n    Batch 110/219, Loss: 0.7393\n    Batch 120/219, Loss: 0.2965\n    Batch 130/219, Loss: 0.5468\n    Batch 140/219, Loss: 0.2428\n    Batch 150/219, Loss: 0.3623\n    Batch 160/219, Loss: 0.1952\n    Batch 170/219, Loss: 0.5060\n    Batch 180/219, Loss: 0.7021\n    Batch 190/219, Loss: 0.5526\n    Batch 200/219, Loss: 0.4799\n    Batch 210/219, Loss: 0.2784\n    Batch 219/219, Loss: 0.4899\n    PSO Eval 40: LR=0.001011, Dropout=0.32, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.7782\n    Batch 20/219, Loss: 0.7825\n    Batch 30/219, Loss: 0.7089\n    Batch 40/219, Loss: 0.5710\n    Batch 50/219, Loss: 0.9088\n    Batch 60/219, Loss: 0.5780\n    Batch 70/219, Loss: 0.2682\n    Batch 80/219, Loss: 0.6792\n    Batch 90/219, Loss: 0.5446\n    Batch 100/219, Loss: 0.3094\n    Batch 110/219, Loss: 0.1972\n    Batch 120/219, Loss: 0.2527\n    Batch 130/219, Loss: 0.3733\n    Batch 140/219, Loss: 0.2787\n    Batch 150/219, Loss: 0.3308\n    Batch 160/219, Loss: 0.4045\n    Batch 170/219, Loss: 0.2471\n    Batch 180/219, Loss: 0.2205\n    Batch 190/219, Loss: 0.4361\n    Batch 200/219, Loss: 0.2755\n    Batch 210/219, Loss: 0.5073\n    Batch 219/219, Loss: 0.2380\n    PSO Eval 41: LR=0.003000, Dropout=0.21, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.8701\n    Batch 20/219, Loss: 0.8456\n    Batch 30/219, Loss: 0.6347\n    Batch 40/219, Loss: 0.6670\n    Batch 50/219, Loss: 0.5739\n    Batch 60/219, Loss: 0.4713\n    Batch 70/219, Loss: 0.5629\n    Batch 80/219, Loss: 0.3359\n    Batch 90/219, Loss: 0.4592\n    Batch 100/219, Loss: 0.2782\n    Batch 110/219, Loss: 0.4518\n    Batch 120/219, Loss: 0.7270\n    Batch 130/219, Loss: 0.6341\n    Batch 140/219, Loss: 0.2909\n    Batch 150/219, Loss: 0.2513\n    Batch 160/219, Loss: 0.3083\n    Batch 170/219, Loss: 0.4667\n    Batch 180/219, Loss: 0.4733\n    Batch 190/219, Loss: 0.3925\n    Batch 200/219, Loss: 1.1789\n    Batch 210/219, Loss: 0.6488\n    Batch 219/219, Loss: 0.4081\n    PSO Eval 42: LR=0.000871, Dropout=0.32, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.8304\n    Batch 20/219, Loss: 0.6301\n    Batch 30/219, Loss: 0.5659\n    Batch 40/219, Loss: 0.3299\n    Batch 50/219, Loss: 0.6391\n    Batch 60/219, Loss: 0.3069\n    Batch 70/219, Loss: 0.2826\n    Batch 80/219, Loss: 0.1892\n    Batch 90/219, Loss: 0.4531\n    Batch 100/219, Loss: 0.2817\n    Batch 110/219, Loss: 0.3717\n    Batch 120/219, Loss: 0.4903\n    Batch 130/219, Loss: 0.3001\n    Batch 140/219, Loss: 0.1723\n    Batch 150/219, Loss: 0.1865\n    Batch 160/219, Loss: 0.2556\n    Batch 170/219, Loss: 0.1890\n    Batch 180/219, Loss: 0.2183\n    Batch 190/219, Loss: 0.1576\n    Batch 200/219, Loss: 0.2588\n    Batch 210/219, Loss: 0.4014\n    Batch 219/219, Loss: 0.2444\n    PSO Eval 43: LR=0.000933, Dropout=0.31, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.7023\n    Batch 20/219, Loss: 0.6933\n    Batch 30/219, Loss: 0.6346\n    Batch 40/219, Loss: 0.3955\n    Batch 50/219, Loss: 0.5578\n    Batch 60/219, Loss: 0.3907\n    Batch 70/219, Loss: 0.2740\n    Batch 80/219, Loss: 0.5128\n    Batch 90/219, Loss: 0.3891\n    Batch 100/219, Loss: 0.2418\n    Batch 110/219, Loss: 0.4716\n    Batch 120/219, Loss: 0.2954\n    Batch 130/219, Loss: 0.4446\n    Batch 140/219, Loss: 0.8265\n    Batch 150/219, Loss: 0.1951\n    Batch 160/219, Loss: 0.2190\n    Batch 170/219, Loss: 0.1100\n    Batch 180/219, Loss: 0.2394\n    Batch 190/219, Loss: 0.1814\n    Batch 200/219, Loss: 0.3993\n    Batch 210/219, Loss: 0.1409\n    Batch 219/219, Loss: 0.2656\n    PSO Eval 44: LR=0.000962, Dropout=0.31, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.9627\n    Batch 20/219, Loss: 0.7756\n    Batch 30/219, Loss: 0.5098\n    Batch 40/219, Loss: 0.4399\n    Batch 50/219, Loss: 0.4659\n    Batch 60/219, Loss: 0.2674\n    Batch 70/219, Loss: 0.2809\n    Batch 80/219, Loss: 0.2424\n    Batch 90/219, Loss: 0.2846\n    Batch 100/219, Loss: 0.3586\n    Batch 110/219, Loss: 0.4992\n    Batch 120/219, Loss: 0.2247\n    Batch 130/219, Loss: 0.2777\n    Batch 140/219, Loss: 0.1813\n    Batch 150/219, Loss: 0.3427\n    Batch 160/219, Loss: 0.1144\n    Batch 170/219, Loss: 0.3495\n    Batch 180/219, Loss: 0.2900\n    Batch 190/219, Loss: 0.2046\n    Batch 200/219, Loss: 0.2755\n    Batch 210/219, Loss: 0.1671\n    Batch 219/219, Loss: 0.3048\n    PSO Eval 45: LR=0.001212, Dropout=0.27, BS=8\n    First batch loaded, starting training...\n    Batch 10/875, Loss: 1.0673\n    Batch 20/875, Loss: 0.8715\n    Batch 30/875, Loss: 1.9377\n    Batch 40/875, Loss: 0.7977\n    Batch 50/875, Loss: 0.5659\n    Batch 60/875, Loss: 2.1595\n    Batch 70/875, Loss: 0.7971\n    Batch 80/875, Loss: 1.0143\n    Batch 90/875, Loss: 0.9068\n    Batch 100/875, Loss: 0.9044\n    Batch 110/875, Loss: 0.8860\n    Batch 120/875, Loss: 0.7660\n    Batch 130/875, Loss: 0.8445\n    Batch 140/875, Loss: 1.1044\n    Batch 150/875, Loss: 0.8110\n    Batch 160/875, Loss: 0.7305\n    Batch 170/875, Loss: 0.8172\n    Batch 180/875, Loss: 1.1115\n    Batch 190/875, Loss: 0.8633\n    Batch 200/875, Loss: 1.1632\n    Batch 210/875, Loss: 0.6954\n    Batch 220/875, Loss: 0.4811\n    Batch 230/875, Loss: 1.7968\n    Batch 240/875, Loss: 0.7774\n    Batch 250/875, Loss: 0.7752\n    Batch 260/875, Loss: 0.3996\n    Batch 270/875, Loss: 0.4104\n    Batch 280/875, Loss: 1.6690\n    Batch 290/875, Loss: 0.6857\n    Batch 300/875, Loss: 0.9872\n    Batch 310/875, Loss: 0.5934\n    Batch 320/875, Loss: 0.5451\n    Batch 330/875, Loss: 0.2462\n    Batch 340/875, Loss: 1.3285\n    Batch 350/875, Loss: 0.8728\n    Batch 360/875, Loss: 0.5617\n    Batch 370/875, Loss: 0.8183\n    Batch 380/875, Loss: 0.5001\n    Batch 390/875, Loss: 1.0668\n    Batch 400/875, Loss: 1.1127\n    Batch 410/875, Loss: 0.7819\n    Batch 420/875, Loss: 0.7510\n    Batch 430/875, Loss: 1.5326\n    Batch 440/875, Loss: 1.0752\n    Batch 450/875, Loss: 0.5406\n    Batch 460/875, Loss: 1.0302\n    Batch 470/875, Loss: 0.4327\n    Batch 480/875, Loss: 0.6372\n    Batch 490/875, Loss: 0.6991\n    Batch 500/875, Loss: 0.5216\n    Batch 510/875, Loss: 1.0658\n    Batch 520/875, Loss: 0.9419\n    Batch 530/875, Loss: 0.6544\n    Batch 540/875, Loss: 0.8154\n    Batch 550/875, Loss: 0.9453\n    Batch 560/875, Loss: 0.7849\n    Batch 570/875, Loss: 1.1813\n    Batch 580/875, Loss: 1.6371\n    Batch 590/875, Loss: 0.9735\n    Batch 600/875, Loss: 2.5438\n    Batch 610/875, Loss: 0.8712\n    Batch 620/875, Loss: 0.5581\n    Batch 630/875, Loss: 0.4006\n    Batch 640/875, Loss: 0.6077\n    Batch 650/875, Loss: 0.7780\n    Batch 660/875, Loss: 0.8330\n    Batch 670/875, Loss: 1.2964\n    Batch 680/875, Loss: 1.3461\n    Batch 690/875, Loss: 0.4694\n    Batch 700/875, Loss: 0.4218\n    Batch 710/875, Loss: 0.2844\n    Batch 720/875, Loss: 0.5804\n    Batch 730/875, Loss: 0.4734\n    Batch 740/875, Loss: 0.5976\n    Batch 750/875, Loss: 1.6979\n    Batch 760/875, Loss: 1.1616\n    Batch 770/875, Loss: 0.4994\n    Batch 780/875, Loss: 0.4735\n    Batch 790/875, Loss: 1.0154\n    Batch 800/875, Loss: 0.3875\n    Batch 810/875, Loss: 0.5852\n    Batch 820/875, Loss: 0.6272\n    Batch 830/875, Loss: 0.8853\n    Batch 840/875, Loss: 0.5135\n    Batch 850/875, Loss: 0.5271\n    Batch 860/875, Loss: 0.8699\n    Batch 870/875, Loss: 0.5759\n    Batch 875/875, Loss: 0.6880\n    PSO Eval 46: LR=0.001576, Dropout=0.39, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.7783\n    Batch 20/219, Loss: 1.3728\n    Batch 30/219, Loss: 0.3727\n    Batch 40/219, Loss: 0.3291\n    Batch 50/219, Loss: 0.4882\n    Batch 60/219, Loss: 0.6737\n    Batch 70/219, Loss: 0.4490\n    Batch 80/219, Loss: 0.3408\n    Batch 90/219, Loss: 0.5595\n    Batch 100/219, Loss: 0.3511\n    Batch 110/219, Loss: 0.2335\n    Batch 120/219, Loss: 0.1922\n    Batch 130/219, Loss: 0.2396\n    Batch 140/219, Loss: 0.2848\n    Batch 150/219, Loss: 0.6960\n    Batch 160/219, Loss: 0.2290\n    Batch 170/219, Loss: 0.2420\n    Batch 180/219, Loss: 0.1457\n    Batch 190/219, Loss: 0.2365\n    Batch 200/219, Loss: 0.3451\n    Batch 210/219, Loss: 0.4996\n    Batch 219/219, Loss: 0.2974\n    PSO Eval 47: LR=0.003000, Dropout=0.37, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 1.2376\n    Batch 20/219, Loss: 0.5993\n    Batch 30/219, Loss: 0.7284\n    Batch 40/219, Loss: 0.5912\n    Batch 50/219, Loss: 0.6024\n    Batch 60/219, Loss: 0.6177\n    Batch 70/219, Loss: 0.6504\n    Batch 80/219, Loss: 0.3322\n    Batch 90/219, Loss: 0.8011\n    Batch 100/219, Loss: 0.8550\n    Batch 110/219, Loss: 0.4332\n    Batch 120/219, Loss: 0.2794\n    Batch 130/219, Loss: 0.8678\n    Batch 140/219, Loss: 0.3264\n    Batch 150/219, Loss: 0.2345\n    Batch 160/219, Loss: 0.6236\n    Batch 170/219, Loss: 0.4035\n    Batch 180/219, Loss: 0.7836\n    Batch 190/219, Loss: 0.3862\n    Batch 200/219, Loss: 0.6059\n    Batch 210/219, Loss: 0.4395\n    Batch 219/219, Loss: 0.3624\n    PSO Eval 48: LR=0.001099, Dropout=0.38, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.5753\n    Batch 20/219, Loss: 0.5453\n    Batch 30/219, Loss: 0.6595\n    Batch 40/219, Loss: 0.5779\n    Batch 50/219, Loss: 0.5515\n    Batch 60/219, Loss: 0.2828\n    Batch 70/219, Loss: 0.4423\n    Batch 80/219, Loss: 1.6215\n    Batch 90/219, Loss: 0.2669\n    Batch 100/219, Loss: 0.4549\n    Batch 110/219, Loss: 0.3248\n    Batch 120/219, Loss: 0.2177\n    Batch 130/219, Loss: 0.2913\n    Batch 140/219, Loss: 0.3830\n    Batch 150/219, Loss: 0.7362\n    Batch 160/219, Loss: 0.5403\n    Batch 170/219, Loss: 0.4346\n    Batch 180/219, Loss: 0.2475\n    Batch 190/219, Loss: 0.3680\n    Batch 200/219, Loss: 0.2527\n    Batch 210/219, Loss: 0.2336\n    Batch 219/219, Loss: 0.5224\n  PSO Iter 3/4: Best = 0.8402\n    PSO Eval 49: LR=0.000956, Dropout=0.35, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.8473\n    Batch 20/219, Loss: 0.4874\n    Batch 30/219, Loss: 0.6044\n    Batch 40/219, Loss: 0.4408\n    Batch 50/219, Loss: 0.3012\n    Batch 60/219, Loss: 0.2967\n    Batch 70/219, Loss: 0.3657\n    Batch 80/219, Loss: 0.2913\n    Batch 90/219, Loss: 0.3575\n    Batch 100/219, Loss: 0.8005\n    Batch 110/219, Loss: 0.4905\n    Batch 120/219, Loss: 0.1731\n    Batch 130/219, Loss: 0.2659\n    Batch 140/219, Loss: 0.3329\n    Batch 150/219, Loss: 0.2042\n    Batch 160/219, Loss: 0.2910\n    Batch 170/219, Loss: 0.2380\n    Batch 180/219, Loss: 0.2245\n    Batch 190/219, Loss: 0.2605\n    Batch 200/219, Loss: 0.2309\n    Batch 210/219, Loss: 0.2472\n    Batch 219/219, Loss: 0.2100\n    PSO Eval 50: LR=0.000362, Dropout=0.29, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.9705\n    Batch 20/219, Loss: 0.4438\n    Batch 30/219, Loss: 0.5017\n    Batch 40/219, Loss: 0.4064\n    Batch 50/219, Loss: 0.4635\n    Batch 60/219, Loss: 0.4369\n    Batch 70/219, Loss: 0.2302\n    Batch 80/219, Loss: 0.3169\n    Batch 90/219, Loss: 0.3645\n    Batch 100/219, Loss: 0.2738\n    Batch 110/219, Loss: 0.6215\n    Batch 120/219, Loss: 0.3486\n    Batch 130/219, Loss: 0.3433\n    Batch 140/219, Loss: 0.1731\n    Batch 150/219, Loss: 0.3492\n    Batch 160/219, Loss: 0.3947\n    Batch 170/219, Loss: 0.2483\n    Batch 180/219, Loss: 0.1833\n    Batch 190/219, Loss: 0.1282\n    Batch 200/219, Loss: 0.2193\n    Batch 210/219, Loss: 0.4059\n    Batch 219/219, Loss: 0.1318\n    PSO Eval 51: LR=0.001018, Dropout=0.32, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.8495\n    Batch 20/219, Loss: 0.6272\n    Batch 30/219, Loss: 0.4275\n    Batch 40/219, Loss: 0.6900\n    Batch 50/219, Loss: 0.2755\n    Batch 60/219, Loss: 0.3794\n    Batch 70/219, Loss: 0.6904\n    Batch 80/219, Loss: 1.0981\n    Batch 90/219, Loss: 0.2907\n    Batch 100/219, Loss: 0.2681\n    Batch 110/219, Loss: 0.3398\n    Batch 120/219, Loss: 0.1962\n    Batch 130/219, Loss: 0.6267\n    Batch 140/219, Loss: 0.4536\n    Batch 150/219, Loss: 0.4725\n    Batch 160/219, Loss: 0.2749\n    Batch 170/219, Loss: 0.2719\n    Batch 180/219, Loss: 0.2843\n    Batch 190/219, Loss: 0.4150\n    Batch 200/219, Loss: 0.2119\n    Batch 210/219, Loss: 0.2266\n    Batch 219/219, Loss: 0.2325\n    PSO Eval 52: LR=0.001240, Dropout=0.29, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.6866\n    Batch 20/219, Loss: 0.8769\n    Batch 30/219, Loss: 0.5868\n    Batch 40/219, Loss: 0.6416\n    Batch 50/219, Loss: 0.8010\n    Batch 60/219, Loss: 0.2868\n    Batch 70/219, Loss: 0.2543\n    Batch 80/219, Loss: 0.2628\n    Batch 90/219, Loss: 0.2659\n    Batch 100/219, Loss: 0.4447\n    Batch 110/219, Loss: 0.2215\n    Batch 120/219, Loss: 0.2444\n    Batch 130/219, Loss: 0.5770\n    Batch 140/219, Loss: 0.5932\n    Batch 150/219, Loss: 0.2291\n    Batch 160/219, Loss: 0.2267\n    Batch 170/219, Loss: 0.3993\n    Batch 180/219, Loss: 0.1718\n    Batch 190/219, Loss: 0.2644\n    Batch 200/219, Loss: 0.3388\n    Batch 210/219, Loss: 0.2933\n    Batch 219/219, Loss: 0.1707\n    PSO Eval 53: LR=0.002287, Dropout=0.30, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.7776\n    Batch 20/219, Loss: 0.4116\n    Batch 30/219, Loss: 0.5570\n    Batch 40/219, Loss: 0.8279\n    Batch 50/219, Loss: 0.6138\n    Batch 60/219, Loss: 0.2528\n    Batch 70/219, Loss: 0.3623\n    Batch 80/219, Loss: 0.3594\n    Batch 90/219, Loss: 0.2970\n    Batch 100/219, Loss: 0.3740\n    Batch 110/219, Loss: 0.3775\n    Batch 120/219, Loss: 0.3610\n    Batch 130/219, Loss: 0.3045\n    Batch 140/219, Loss: 0.4059\n    Batch 150/219, Loss: 0.3118\n    Batch 160/219, Loss: 0.2858\n    Batch 170/219, Loss: 1.4503\n    Batch 180/219, Loss: 0.2265\n    Batch 190/219, Loss: 0.2555\n    Batch 200/219, Loss: 0.3668\n    Batch 210/219, Loss: 0.4942\n    Batch 219/219, Loss: 0.4288\n    PSO Eval 54: LR=0.000253, Dropout=0.23, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.9461\n    Batch 20/219, Loss: 0.3953\n    Batch 30/219, Loss: 0.4387\n    Batch 40/219, Loss: 0.6411\n    Batch 50/219, Loss: 0.8373\n    Batch 60/219, Loss: 0.5584\n    Batch 70/219, Loss: 0.4538\n    Batch 80/219, Loss: 0.3792\n    Batch 90/219, Loss: 0.2833\n    Batch 100/219, Loss: 0.3863\n    Batch 110/219, Loss: 0.5233\n    Batch 120/219, Loss: 0.5561\n    Batch 130/219, Loss: 0.4466\n    Batch 140/219, Loss: 0.3042\n    Batch 150/219, Loss: 0.4678\n    Batch 160/219, Loss: 0.2637\n    Batch 170/219, Loss: 0.2749\n    Batch 180/219, Loss: 0.3120\n    Batch 190/219, Loss: 0.2604\n    Batch 200/219, Loss: 0.2074\n    Batch 210/219, Loss: 0.2914\n    Batch 219/219, Loss: 0.2008\n    PSO Eval 55: LR=0.000755, Dropout=0.28, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.7497\n    Batch 20/219, Loss: 0.8041\n    Batch 30/219, Loss: 0.6214\n    Batch 40/219, Loss: 0.3956\n    Batch 50/219, Loss: 0.4298\n    Batch 60/219, Loss: 0.3775\n    Batch 70/219, Loss: 0.5587\n    Batch 80/219, Loss: 0.3787\n    Batch 90/219, Loss: 0.2921\n    Batch 100/219, Loss: 0.2256\n    Batch 110/219, Loss: 0.2371\n    Batch 120/219, Loss: 0.2739\n    Batch 130/219, Loss: 0.2439\n    Batch 140/219, Loss: 0.2167\n    Batch 150/219, Loss: 0.5025\n    Batch 160/219, Loss: 0.3328\n    Batch 170/219, Loss: 0.2730\n    Batch 180/219, Loss: 0.1381\n    Batch 190/219, Loss: 0.5748\n    Batch 200/219, Loss: 0.2876\n    Batch 210/219, Loss: 0.2356\n    Batch 219/219, Loss: 0.2692\n    PSO Eval 56: LR=0.000559, Dropout=0.20, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.6852\n    Batch 20/219, Loss: 0.5145\n    Batch 30/219, Loss: 0.5505\n    Batch 40/219, Loss: 0.6230\n    Batch 50/219, Loss: 0.3312\n    Batch 60/219, Loss: 0.2922\n    Batch 70/219, Loss: 0.2798\n    Batch 80/219, Loss: 0.2427\n    Batch 90/219, Loss: 0.3969\n    Batch 100/219, Loss: 0.2834\n    Batch 110/219, Loss: 0.2468\n    Batch 120/219, Loss: 0.3072\n    Batch 130/219, Loss: 0.2551\n    Batch 140/219, Loss: 0.1908\n    Batch 150/219, Loss: 0.1906\n    Batch 160/219, Loss: 0.1680\n    Batch 170/219, Loss: 0.2966\n    Batch 180/219, Loss: 0.2007\n    Batch 190/219, Loss: 0.3128\n    Batch 200/219, Loss: 0.3273\n    Batch 210/219, Loss: 0.7665\n    Batch 219/219, Loss: 0.1898\n    PSO Eval 57: LR=0.000977, Dropout=0.26, BS=16\n    First batch loaded, starting training...\n    Batch 10/438, Loss: 0.7458\n    Batch 20/438, Loss: 1.0395\n    Batch 30/438, Loss: 0.9880\n    Batch 40/438, Loss: 0.3786\n    Batch 50/438, Loss: 0.6751\n    Batch 60/438, Loss: 0.7461\n    Batch 70/438, Loss: 0.3922\n    Batch 80/438, Loss: 0.3996\n    Batch 90/438, Loss: 0.3807\n    Batch 100/438, Loss: 0.5029\n    Batch 110/438, Loss: 0.5380\n    Batch 120/438, Loss: 0.5401\n    Batch 130/438, Loss: 0.3743\n    Batch 140/438, Loss: 0.2664\n    Batch 150/438, Loss: 0.2465\n    Batch 160/438, Loss: 0.4701\n    Batch 170/438, Loss: 0.4522\n    Batch 180/438, Loss: 0.3332\n    Batch 190/438, Loss: 0.3665\n    Batch 200/438, Loss: 0.3450\n    Batch 210/438, Loss: 0.3106\n    Batch 220/438, Loss: 0.2511\n    Batch 230/438, Loss: 0.2770\n    Batch 240/438, Loss: 0.2791\n    Batch 250/438, Loss: 0.3892\n    Batch 260/438, Loss: 0.3969\n    Batch 270/438, Loss: 0.4878\n    Batch 280/438, Loss: 0.3303\n    Batch 290/438, Loss: 0.2331\n    Batch 300/438, Loss: 0.4650\n    Batch 310/438, Loss: 0.1975\n    Batch 320/438, Loss: 0.4293\n    Batch 330/438, Loss: 1.2427\n    Batch 340/438, Loss: 0.3692\n    Batch 350/438, Loss: 0.3150\n    Batch 360/438, Loss: 0.3127\n    Batch 370/438, Loss: 0.3111\n    Batch 380/438, Loss: 0.2546\n    Batch 390/438, Loss: 0.3436\n    Batch 400/438, Loss: 0.2486\n    Batch 410/438, Loss: 0.2527\n    Batch 420/438, Loss: 0.3370\n    Batch 430/438, Loss: 0.2206\n    Batch 438/438, Loss: 0.1420\n    PSO Eval 58: LR=0.000623, Dropout=0.25, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.9783\n    Batch 20/219, Loss: 0.7500\n    Batch 30/219, Loss: 0.8762\n    Batch 40/219, Loss: 0.7760\n    Batch 50/219, Loss: 0.4260\n    Batch 60/219, Loss: 0.3871\n    Batch 70/219, Loss: 0.3178\n    Batch 80/219, Loss: 0.3244\n    Batch 90/219, Loss: 0.3421\n    Batch 100/219, Loss: 0.2644\n    Batch 110/219, Loss: 0.3573\n    Batch 120/219, Loss: 0.2744\n    Batch 130/219, Loss: 0.2118\n    Batch 140/219, Loss: 0.1893\n    Batch 150/219, Loss: 0.2047\n    Batch 160/219, Loss: 0.2891\n    Batch 170/219, Loss: 0.3634\n    Batch 180/219, Loss: 0.2475\n    Batch 190/219, Loss: 0.2196\n    Batch 200/219, Loss: 0.2397\n    Batch 210/219, Loss: 0.1412\n    Batch 219/219, Loss: 0.2719\n    PSO Eval 59: LR=0.001375, Dropout=0.35, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 1.0583\n    Batch 20/219, Loss: 0.5762\n    Batch 30/219, Loss: 0.3259\n    Batch 40/219, Loss: 0.4340\n    Batch 50/219, Loss: 0.4136\n    Batch 60/219, Loss: 0.7158\n    Batch 70/219, Loss: 0.5518\n    Batch 80/219, Loss: 0.3919\n    Batch 90/219, Loss: 0.3984\n    Batch 100/219, Loss: 0.3876\n    Batch 110/219, Loss: 0.3802\n    Batch 120/219, Loss: 0.6705\n    Batch 130/219, Loss: 0.4527\n    Batch 140/219, Loss: 0.2032\n    Batch 150/219, Loss: 0.6695\n    Batch 160/219, Loss: 0.2134\n    Batch 170/219, Loss: 0.3590\n    Batch 180/219, Loss: 0.3665\n    Batch 190/219, Loss: 0.2604\n    Batch 200/219, Loss: 0.2133\n    Batch 210/219, Loss: 0.9914\n    Batch 219/219, Loss: 0.5590\n    PSO Eval 60: LR=0.000447, Dropout=0.41, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 1.0539\n    Batch 20/219, Loss: 0.5583\n    Batch 30/219, Loss: 0.6663\n    Batch 40/219, Loss: 0.4593\n    Batch 50/219, Loss: 0.4111\n    Batch 60/219, Loss: 0.3729\n    Batch 70/219, Loss: 1.9070\n    Batch 80/219, Loss: 0.4023\n    Batch 90/219, Loss: 0.2220\n    Batch 100/219, Loss: 0.2181\n    Batch 110/219, Loss: 0.3424\n    Batch 120/219, Loss: 0.2335\n    Batch 130/219, Loss: 0.2846\n    Batch 140/219, Loss: 0.2938\n    Batch 150/219, Loss: 0.2540\n    Batch 160/219, Loss: 0.3585\n    Batch 170/219, Loss: 0.1682\n    Batch 180/219, Loss: 0.2789\n    Batch 190/219, Loss: 0.3516\n    Batch 200/219, Loss: 0.2216\n    Batch 210/219, Loss: 0.5459\n    Batch 219/219, Loss: 0.2143\n  PSO Iter 4/4: Best = 0.8402\n\nPSO Result: Score=0.8402, Time=1791.0s\n  Best Hyperparams: {'lr': np.float64(0.0009333928602728289), 'weight_decay': np.float64(0.0038949077998631262), 'dropout': np.float64(0.3120520836413773), 'unfreeze_epoch': 0, 'batch_size': 32, 'augment_strength': np.float64(0.5776677691991392)}\n\n--- Running BAT ---\n    BAT Eval 1: LR=0.002818, Dropout=0.17, BS=16\n    First batch loaded, starting training...\n    Batch 10/438, Loss: 0.6003\n    Batch 20/438, Loss: 1.1890\n    Batch 30/438, Loss: 1.6888\n    Batch 40/438, Loss: 0.8745\n    Batch 50/438, Loss: 0.8291\n    Batch 60/438, Loss: 0.6311\n    Batch 70/438, Loss: 0.5160\n    Batch 80/438, Loss: 0.6698\n    Batch 90/438, Loss: 0.6883\n    Batch 100/438, Loss: 0.6288\n    Batch 110/438, Loss: 1.8529\n    Batch 120/438, Loss: 0.7721\n    Batch 130/438, Loss: 1.5546\n    Batch 140/438, Loss: 0.3448\n    Batch 150/438, Loss: 0.7374\n    Batch 160/438, Loss: 0.4584\n    Batch 170/438, Loss: 0.6795\n    Batch 180/438, Loss: 1.4038\n    Batch 190/438, Loss: 0.4924\n    Batch 200/438, Loss: 0.5007\n    Batch 210/438, Loss: 0.5256\n    Batch 220/438, Loss: 0.3999\n    Batch 230/438, Loss: 0.7654\n    Batch 240/438, Loss: 0.6869\n    Batch 250/438, Loss: 0.7149\n    Batch 260/438, Loss: 0.5156\n    Batch 270/438, Loss: 0.5903\n    Batch 280/438, Loss: 0.9411\n    Batch 290/438, Loss: 0.4124\n    Batch 300/438, Loss: 0.3162\n    Batch 310/438, Loss: 0.4016\n    Batch 320/438, Loss: 0.3486\n    Batch 330/438, Loss: 1.9450\n    Batch 340/438, Loss: 1.0568\n    Batch 350/438, Loss: 0.2628\n    Batch 360/438, Loss: 0.6914\n    Batch 370/438, Loss: 0.7651\n    Batch 380/438, Loss: 0.3951\n    Batch 390/438, Loss: 0.4039\n    Batch 400/438, Loss: 0.4320\n    Batch 410/438, Loss: 0.5036\n    Batch 420/438, Loss: 0.4174\n    Batch 430/438, Loss: 0.3238\n    Batch 438/438, Loss: 0.4007\n    BAT Eval 2: LR=0.000013, Dropout=0.07, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 1.4015\n    Batch 20/219, Loss: 1.3592\n    Batch 30/219, Loss: 1.3200\n    Batch 40/219, Loss: 1.2708\n    Batch 50/219, Loss: 1.2433\n    Batch 60/219, Loss: 1.2443\n    Batch 70/219, Loss: 1.2187\n    Batch 80/219, Loss: 1.1545\n    Batch 90/219, Loss: 1.1034\n    Batch 100/219, Loss: 1.3035\n    Batch 110/219, Loss: 1.1005\n    Batch 120/219, Loss: 1.0596\n    Batch 130/219, Loss: 1.0525\n    Batch 140/219, Loss: 1.0661\n    Batch 150/219, Loss: 1.0051\n    Batch 160/219, Loss: 1.0273\n    Batch 170/219, Loss: 1.3231\n    Batch 180/219, Loss: 0.9128\n    Batch 190/219, Loss: 1.0623\n    Batch 200/219, Loss: 0.9690\n    Batch 210/219, Loss: 0.8376\n    Batch 219/219, Loss: 1.0728\n    BAT Eval 3: LR=0.002845, Dropout=0.37, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.8177\n    Batch 20/219, Loss: 0.7671\n    Batch 30/219, Loss: 0.9369\n    Batch 40/219, Loss: 0.4709\n    Batch 50/219, Loss: 0.6371\n    Batch 60/219, Loss: 0.3612\n    Batch 70/219, Loss: 0.6033\n    Batch 80/219, Loss: 0.6161\n    Batch 90/219, Loss: 0.2781\n    Batch 100/219, Loss: 0.6568\n    Batch 110/219, Loss: 0.5632\n    Batch 120/219, Loss: 0.2952\n    Batch 130/219, Loss: 0.3903\n    Batch 140/219, Loss: 0.2544\n    Batch 150/219, Loss: 0.3825\n    Batch 160/219, Loss: 0.3062\n    Batch 170/219, Loss: 0.5737\n    Batch 180/219, Loss: 0.2770\n    Batch 190/219, Loss: 0.4356\n    Batch 200/219, Loss: 0.3744\n    Batch 210/219, Loss: 0.1761\n    Batch 219/219, Loss: 1.0739\n    BAT Eval 4: LR=0.000123, Dropout=0.31, BS=8\n    First batch loaded, starting training...\n    Batch 10/875, Loss: 1.3840\n    Batch 20/875, Loss: 1.2436\n    Batch 30/875, Loss: 0.9449\n    Batch 40/875, Loss: 0.8259\n    Batch 50/875, Loss: 1.1011\n    Batch 60/875, Loss: 0.7687\n    Batch 70/875, Loss: 1.0381\n    Batch 80/875, Loss: 0.5176\n    Batch 90/875, Loss: 0.8757\n    Batch 100/875, Loss: 0.5172\n    Batch 110/875, Loss: 0.6013\n    Batch 120/875, Loss: 0.7881\n    Batch 130/875, Loss: 0.6659\n    Batch 140/875, Loss: 0.8119\n    Batch 150/875, Loss: 0.4676\n    Batch 160/875, Loss: 0.6724\n    Batch 170/875, Loss: 0.6196\n    Batch 180/875, Loss: 0.6824\n    Batch 190/875, Loss: 2.2154\n    Batch 200/875, Loss: 0.4219\n    Batch 210/875, Loss: 0.4002\n    Batch 220/875, Loss: 0.6399\n    Batch 230/875, Loss: 0.5132\n    Batch 240/875, Loss: 0.4585\n    Batch 250/875, Loss: 0.3618\n    Batch 260/875, Loss: 0.3098\n    Batch 270/875, Loss: 0.3788\n    Batch 280/875, Loss: 2.4111\n    Batch 290/875, Loss: 0.7655\n    Batch 300/875, Loss: 0.3911\n    Batch 310/875, Loss: 0.6544\n    Batch 320/875, Loss: 1.3227\n    Batch 330/875, Loss: 0.3939\n    Batch 340/875, Loss: 0.6298\n    Batch 350/875, Loss: 0.2129\n    Batch 360/875, Loss: 0.2868\n    Batch 370/875, Loss: 0.4907\n    Batch 380/875, Loss: 0.4449\n    Batch 390/875, Loss: 0.3716\n    Batch 400/875, Loss: 0.2712\n    Batch 410/875, Loss: 0.2077\n    Batch 420/875, Loss: 0.2366\n    Batch 430/875, Loss: 0.2226\n    Batch 440/875, Loss: 0.3171\n    Batch 450/875, Loss: 0.2967\n    Batch 460/875, Loss: 0.2725\n    Batch 470/875, Loss: 0.4624\n    Batch 480/875, Loss: 0.3356\n    Batch 490/875, Loss: 0.3167\n    Batch 500/875, Loss: 0.3185\n    Batch 510/875, Loss: 1.3903\n    Batch 520/875, Loss: 0.3949\n    Batch 530/875, Loss: 0.2169\n    Batch 540/875, Loss: 0.1218\n    Batch 550/875, Loss: 0.6939\n    Batch 560/875, Loss: 0.2722\n    Batch 570/875, Loss: 0.4505\n    Batch 580/875, Loss: 0.4304\n    Batch 590/875, Loss: 0.2743\n    Batch 600/875, Loss: 0.2352\n    Batch 610/875, Loss: 0.2372\n    Batch 620/875, Loss: 0.7531\n    Batch 630/875, Loss: 0.1507\n    Batch 640/875, Loss: 0.2982\n    Batch 650/875, Loss: 0.2335\n    Batch 660/875, Loss: 0.2549\n    Batch 670/875, Loss: 0.3588\n    Batch 680/875, Loss: 0.8237\n    Batch 690/875, Loss: 0.2965\n    Batch 700/875, Loss: 0.1978\n    Batch 710/875, Loss: 0.2226\n    Batch 720/875, Loss: 0.4583\n    Batch 730/875, Loss: 0.1564\n    Batch 740/875, Loss: 0.2974\n    Batch 750/875, Loss: 0.1552\n    Batch 760/875, Loss: 0.1347\n    Batch 770/875, Loss: 0.2444\n    Batch 780/875, Loss: 0.3226\n    Batch 790/875, Loss: 1.1184\n    Batch 800/875, Loss: 1.2268\n    Batch 810/875, Loss: 0.2259\n    Batch 820/875, Loss: 0.2255\n    Batch 830/875, Loss: 0.1699\n    Batch 840/875, Loss: 0.3923\n    Batch 850/875, Loss: 0.5625\n    Batch 860/875, Loss: 0.1985\n    Batch 870/875, Loss: 0.1855\n    Batch 875/875, Loss: 0.1581\n    BAT Eval 5: LR=0.000040, Dropout=0.49, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 1.3349\n    Batch 20/219, Loss: 1.2620\n    Batch 30/219, Loss: 1.1974\n    Batch 40/219, Loss: 0.9738\n    Batch 50/219, Loss: 0.8303\n    Batch 60/219, Loss: 0.6804\n    Batch 70/219, Loss: 1.1963\n    Batch 80/219, Loss: 0.7412\n    Batch 90/219, Loss: 0.6488\n    Batch 100/219, Loss: 0.7744\n    Batch 110/219, Loss: 0.9620\n    Batch 120/219, Loss: 1.1269\n    Batch 130/219, Loss: 0.6797\n    Batch 140/219, Loss: 0.6343\n    Batch 150/219, Loss: 0.5492\n    Batch 160/219, Loss: 0.6705\n    Batch 170/219, Loss: 0.3839\n    Batch 180/219, Loss: 0.5453\n    Batch 190/219, Loss: 0.3775\n    Batch 200/219, Loss: 0.5161\n    Batch 210/219, Loss: 0.5322\n    Batch 219/219, Loss: 0.7339\n    BAT Eval 6: LR=0.000562, Dropout=0.36, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.8457\n    Batch 20/219, Loss: 1.0004\n    Batch 30/219, Loss: 1.3625\n    Batch 40/219, Loss: 0.8478\n    Batch 50/219, Loss: 1.0151\n    Batch 60/219, Loss: 0.8175\n    Batch 70/219, Loss: 0.8017\n    Batch 80/219, Loss: 0.7199\n    Batch 90/219, Loss: 0.8883\n    Batch 100/219, Loss: 0.8867\n    Batch 110/219, Loss: 0.6177\n    Batch 120/219, Loss: 0.7287\n    Batch 130/219, Loss: 0.8717\n    Batch 140/219, Loss: 0.5773\n    Batch 150/219, Loss: 0.5096\n    Batch 160/219, Loss: 0.4085\n    Batch 170/219, Loss: 0.6687\n    Batch 180/219, Loss: 0.7342\n    Batch 190/219, Loss: 0.6775\n    Batch 200/219, Loss: 0.6754\n    Batch 210/219, Loss: 0.5888\n    Batch 219/219, Loss: 0.3790\n    BAT Eval 7: LR=0.000136, Dropout=0.15, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 1.0041\n    Batch 20/219, Loss: 0.7164\n    Batch 30/219, Loss: 1.0545\n    Batch 40/219, Loss: 0.7491\n    Batch 50/219, Loss: 0.6145\n    Batch 60/219, Loss: 0.6590\n    Batch 70/219, Loss: 0.4207\n    Batch 80/219, Loss: 0.3794\n    Batch 90/219, Loss: 0.4694\n    Batch 100/219, Loss: 0.3683\n    Batch 110/219, Loss: 0.3811\n    Batch 120/219, Loss: 0.4210\n    Batch 130/219, Loss: 0.3196\n    Batch 140/219, Loss: 0.2758\n    Batch 150/219, Loss: 0.7059\n    Batch 160/219, Loss: 0.3949\n    Batch 170/219, Loss: 0.3918\n    Batch 180/219, Loss: 0.3347\n    Batch 190/219, Loss: 0.3038\n    Batch 200/219, Loss: 0.2831\n    Batch 210/219, Loss: 0.2618\n    Batch 219/219, Loss: 0.1316\n    BAT Eval 8: LR=0.001110, Dropout=0.06, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 1.0389\n    Batch 20/219, Loss: 0.8352\n    Batch 30/219, Loss: 0.9329\n    Batch 40/219, Loss: 0.6685\n    Batch 50/219, Loss: 0.7284\n    Batch 60/219, Loss: 0.5748\n    Batch 70/219, Loss: 0.6101\n    Batch 80/219, Loss: 0.6968\n    Batch 90/219, Loss: 0.6924\n    Batch 100/219, Loss: 0.4752\n    Batch 110/219, Loss: 0.7021\n    Batch 120/219, Loss: 0.8646\n    Batch 130/219, Loss: 0.8063\n    Batch 140/219, Loss: 0.5502\n    Batch 150/219, Loss: 0.4787\n    Batch 160/219, Loss: 0.6852\n    Batch 170/219, Loss: 0.5944\n    Batch 180/219, Loss: 0.5468\n    Batch 190/219, Loss: 0.8136\n    Batch 200/219, Loss: 0.8471\n    Batch 210/219, Loss: 0.5834\n    Batch 219/219, Loss: 0.6381\n    BAT Eval 9: LR=0.001319, Dropout=0.11, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.7600\n    Batch 20/219, Loss: 0.6873\n    Batch 30/219, Loss: 0.4867\n    Batch 40/219, Loss: 0.5254\n    Batch 50/219, Loss: 0.4398\n    Batch 60/219, Loss: 0.3979\n    Batch 70/219, Loss: 0.2656\n    Batch 80/219, Loss: 0.3921\n    Batch 90/219, Loss: 0.2985\n    Batch 100/219, Loss: 0.2254\n    Batch 110/219, Loss: 0.2951\n    Batch 120/219, Loss: 0.2412\n    Batch 130/219, Loss: 0.5575\n    Batch 140/219, Loss: 0.2490\n    Batch 150/219, Loss: 0.5613\n    Batch 160/219, Loss: 0.2476\n    Batch 170/219, Loss: 0.2504\n    Batch 180/219, Loss: 0.1652\n    Batch 190/219, Loss: 0.3693\n    Batch 200/219, Loss: 0.4530\n    Batch 210/219, Loss: 0.4904\n    Batch 219/219, Loss: 0.4360\n    BAT Eval 10: LR=0.000043, Dropout=0.59, BS=16\n    First batch loaded, starting training...\n    Batch 10/438, Loss: 1.3689\n    Batch 20/438, Loss: 1.3630\n    Batch 30/438, Loss: 1.2938\n    Batch 40/438, Loss: 1.4034\n    Batch 50/438, Loss: 1.2500\n    Batch 60/438, Loss: 1.1220\n    Batch 70/438, Loss: 1.2061\n    Batch 80/438, Loss: 1.0942\n    Batch 90/438, Loss: 1.2766\n    Batch 100/438, Loss: 1.1987\n    Batch 110/438, Loss: 1.0170\n    Batch 120/438, Loss: 1.0185\n    Batch 130/438, Loss: 1.3272\n    Batch 140/438, Loss: 1.2030\n    Batch 150/438, Loss: 1.2420\n    Batch 160/438, Loss: 1.0316\n    Batch 170/438, Loss: 1.8964\n    Batch 180/438, Loss: 0.9737\n    Batch 190/438, Loss: 0.9117\n    Batch 200/438, Loss: 0.8522\n    Batch 210/438, Loss: 1.3097\n    Batch 220/438, Loss: 0.8860\n    Batch 230/438, Loss: 1.1429\n    Batch 240/438, Loss: 0.7495\n    Batch 250/438, Loss: 0.7800\n    Batch 260/438, Loss: 0.8920\n    Batch 270/438, Loss: 0.8987\n    Batch 280/438, Loss: 0.8164\n    Batch 290/438, Loss: 1.1337\n    Batch 300/438, Loss: 1.4751\n    Batch 310/438, Loss: 1.5399\n    Batch 320/438, Loss: 1.1139\n    Batch 330/438, Loss: 0.9003\n    Batch 340/438, Loss: 0.8808\n    Batch 350/438, Loss: 0.9231\n    Batch 360/438, Loss: 1.5937\n    Batch 370/438, Loss: 1.3975\n    Batch 380/438, Loss: 0.9464\n    Batch 390/438, Loss: 0.9925\n    Batch 400/438, Loss: 0.6010\n    Batch 410/438, Loss: 1.3311\n    Batch 420/438, Loss: 0.7864\n    Batch 430/438, Loss: 1.4899\n    Batch 438/438, Loss: 1.1847\n    BAT Eval 11: LR=0.000126, Dropout=0.19, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 1.2496\n    Batch 20/219, Loss: 1.0885\n    Batch 30/219, Loss: 0.8086\n    Batch 40/219, Loss: 1.0466\n    Batch 50/219, Loss: 1.2534\n    Batch 60/219, Loss: 0.7952\n    Batch 70/219, Loss: 0.7920\n    Batch 80/219, Loss: 0.7904\n    Batch 90/219, Loss: 0.7844\n    Batch 100/219, Loss: 0.8232\n    Batch 110/219, Loss: 0.5736\n    Batch 120/219, Loss: 0.9329\n    Batch 130/219, Loss: 1.2198\n    Batch 140/219, Loss: 0.9836\n    Batch 150/219, Loss: 0.9344\n    Batch 160/219, Loss: 0.5969\n    Batch 170/219, Loss: 0.6668\n    Batch 180/219, Loss: 0.7774\n    Batch 190/219, Loss: 0.8107\n    Batch 200/219, Loss: 0.7683\n    Batch 210/219, Loss: 1.0090\n    Batch 219/219, Loss: 0.7213\n    BAT Eval 12: LR=0.000029, Dropout=0.08, BS=16\n    First batch loaded, starting training...\n    Batch 10/438, Loss: 1.2962\n    Batch 20/438, Loss: 1.1739\n    Batch 30/438, Loss: 1.0476\n    Batch 40/438, Loss: 1.0634\n    Batch 50/438, Loss: 1.1440\n    Batch 60/438, Loss: 0.8918\n    Batch 70/438, Loss: 0.6694\n    Batch 80/438, Loss: 0.8664\n    Batch 90/438, Loss: 0.7319\n    Batch 100/438, Loss: 2.0748\n    Batch 110/438, Loss: 0.6608\n    Batch 120/438, Loss: 0.9963\n    Batch 130/438, Loss: 0.8452\n    Batch 140/438, Loss: 0.6196\n    Batch 150/438, Loss: 0.5325\n    Batch 160/438, Loss: 0.8006\n    Batch 170/438, Loss: 0.6962\n    Batch 180/438, Loss: 0.5556\n    Batch 190/438, Loss: 0.5532\n    Batch 200/438, Loss: 2.0641\n    Batch 210/438, Loss: 1.0077\n    Batch 220/438, Loss: 0.9984\n    Batch 230/438, Loss: 0.5042\n    Batch 240/438, Loss: 1.2967\n    Batch 250/438, Loss: 0.4767\n    Batch 260/438, Loss: 0.7045\n    Batch 270/438, Loss: 0.4897\n    Batch 280/438, Loss: 0.4566\n    Batch 290/438, Loss: 0.5109\n    Batch 300/438, Loss: 0.4175\n    Batch 310/438, Loss: 0.4798\n    Batch 320/438, Loss: 0.6138\n    Batch 330/438, Loss: 0.3739\n    Batch 340/438, Loss: 0.8420\n    Batch 350/438, Loss: 0.4418\n    Batch 360/438, Loss: 0.6568\n    Batch 370/438, Loss: 0.3558\n    Batch 380/438, Loss: 0.8520\n    Batch 390/438, Loss: 0.7300\n    Batch 400/438, Loss: 0.3816\n    Batch 410/438, Loss: 0.8060\n    Batch 420/438, Loss: 0.5911\n    Batch 430/438, Loss: 0.5245\n    Batch 438/438, Loss: 0.4360\n    BAT Eval 13: LR=0.001393, Dropout=0.11, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.7812\n    Batch 20/219, Loss: 0.5719\n    Batch 30/219, Loss: 0.5889\n    Batch 40/219, Loss: 0.4337\n    Batch 50/219, Loss: 0.5295\n    Batch 60/219, Loss: 0.6382\n    Batch 70/219, Loss: 0.2267\n    Batch 80/219, Loss: 0.3371\n    Batch 90/219, Loss: 0.5706\n    Batch 100/219, Loss: 0.2990\n    Batch 110/219, Loss: 0.2692\n    Batch 120/219, Loss: 0.2958\n    Batch 130/219, Loss: 0.2256\n    Batch 140/219, Loss: 0.2601\n    Batch 150/219, Loss: 0.2369\n    Batch 160/219, Loss: 0.6000\n    Batch 170/219, Loss: 0.2465\n    Batch 180/219, Loss: 0.4023\n    Batch 190/219, Loss: 0.2291\n    Batch 200/219, Loss: 0.7611\n    Batch 210/219, Loss: 0.6854\n    Batch 219/219, Loss: 0.2409\n    BAT Eval 14: LR=0.001382, Dropout=0.11, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.9346\n    Batch 20/219, Loss: 0.8449\n    Batch 30/219, Loss: 0.3492\n    Batch 40/219, Loss: 0.3814\n    Batch 50/219, Loss: 0.7829\n    Batch 60/219, Loss: 0.4505\n    Batch 70/219, Loss: 0.5178\n    Batch 80/219, Loss: 0.4691\n    Batch 90/219, Loss: 0.6456\n    Batch 100/219, Loss: 0.3148\n    Batch 110/219, Loss: 0.2094\n    Batch 120/219, Loss: 0.3461\n    Batch 130/219, Loss: 0.1914\n    Batch 140/219, Loss: 0.3045\n    Batch 150/219, Loss: 0.1781\n    Batch 160/219, Loss: 0.1624\n    Batch 170/219, Loss: 0.2135\n    Batch 180/219, Loss: 0.1728\n    Batch 190/219, Loss: 0.2645\n    Batch 200/219, Loss: 0.2982\n    Batch 210/219, Loss: 0.4642\n    Batch 219/219, Loss: 0.3007\n    BAT Eval 15: LR=0.001370, Dropout=0.11, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.6668\n    Batch 20/219, Loss: 0.5219\n    Batch 30/219, Loss: 0.5266\n    Batch 40/219, Loss: 0.4531\n    Batch 50/219, Loss: 0.4805\n    Batch 60/219, Loss: 0.9798\n    Batch 70/219, Loss: 0.3880\n    Batch 80/219, Loss: 0.2885\n    Batch 90/219, Loss: 0.2898\n    Batch 100/219, Loss: 0.3955\n    Batch 110/219, Loss: 0.6026\n    Batch 120/219, Loss: 0.4766\n    Batch 130/219, Loss: 0.4101\n    Batch 140/219, Loss: 0.3806\n    Batch 150/219, Loss: 0.2928\n    Batch 160/219, Loss: 0.4458\n    Batch 170/219, Loss: 0.2970\n    Batch 180/219, Loss: 0.3525\n    Batch 190/219, Loss: 0.3236\n    Batch 200/219, Loss: 0.1653\n    Batch 210/219, Loss: 0.2070\n    Batch 219/219, Loss: 0.1747\n    BAT Eval 16: LR=0.000016, Dropout=0.47, BS=8\n    First batch loaded, starting training...\n    Batch 10/875, Loss: 1.3419\n    Batch 20/875, Loss: 1.3221\n    Batch 30/875, Loss: 1.2529\n    Batch 40/875, Loss: 1.3811\n    Batch 50/875, Loss: 1.2382\n    Batch 60/875, Loss: 1.3731\n    Batch 70/875, Loss: 1.2338\n    Batch 80/875, Loss: 1.2729\n    Batch 90/875, Loss: 1.4825\n    Batch 100/875, Loss: 1.2448\n    Batch 110/875, Loss: 1.1753\n    Batch 120/875, Loss: 1.2060\n    Batch 130/875, Loss: 1.1236\n    Batch 140/875, Loss: 1.2551\n    Batch 150/875, Loss: 1.0233\n    Batch 160/875, Loss: 1.1117\n    Batch 170/875, Loss: 1.0678\n    Batch 180/875, Loss: 1.0345\n    Batch 190/875, Loss: 1.0217\n    Batch 200/875, Loss: 0.8552\n    Batch 210/875, Loss: 0.9448\n    Batch 220/875, Loss: 1.0579\n    Batch 230/875, Loss: 0.7118\n    Batch 240/875, Loss: 0.9777\n    Batch 250/875, Loss: 0.9252\n    Batch 260/875, Loss: 0.9605\n    Batch 270/875, Loss: 1.8115\n    Batch 280/875, Loss: 0.9442\n    Batch 290/875, Loss: 0.7025\n    Batch 300/875, Loss: 0.6092\n    Batch 310/875, Loss: 1.7333\n    Batch 320/875, Loss: 0.7294\n    Batch 330/875, Loss: 1.0092\n    Batch 340/875, Loss: 0.6832\n    Batch 350/875, Loss: 0.6832\n    Batch 360/875, Loss: 0.7324\n    Batch 370/875, Loss: 0.6611\n    Batch 380/875, Loss: 0.6704\n    Batch 390/875, Loss: 0.6025\n    Batch 400/875, Loss: 1.0443\n    Batch 410/875, Loss: 0.8080\n    Batch 420/875, Loss: 0.5834\n    Batch 430/875, Loss: 0.4337\n    Batch 440/875, Loss: 0.9573\n    Batch 450/875, Loss: 0.6708\n    Batch 460/875, Loss: 0.5693\n    Batch 470/875, Loss: 0.4643\n    Batch 480/875, Loss: 0.8732\n    Batch 490/875, Loss: 0.5551\n    Batch 500/875, Loss: 0.5957\n    Batch 510/875, Loss: 0.5194\n    Batch 520/875, Loss: 0.4977\n    Batch 530/875, Loss: 1.7944\n    Batch 540/875, Loss: 0.9880\n    Batch 550/875, Loss: 0.4911\n    Batch 560/875, Loss: 0.8329\n    Batch 570/875, Loss: 0.7952\n    Batch 580/875, Loss: 0.8875\n    Batch 590/875, Loss: 0.4836\n    Batch 600/875, Loss: 0.6240\n    Batch 610/875, Loss: 1.9064\n    Batch 620/875, Loss: 0.6780\n    Batch 630/875, Loss: 0.3782\n    Batch 640/875, Loss: 0.5426\n    Batch 650/875, Loss: 0.4690\n    Batch 660/875, Loss: 0.4271\n    Batch 670/875, Loss: 0.4756\n    Batch 680/875, Loss: 0.4629\n    Batch 690/875, Loss: 0.5051\n    Batch 700/875, Loss: 0.9578\n    Batch 710/875, Loss: 0.5060\n    Batch 720/875, Loss: 0.5329\n    Batch 730/875, Loss: 0.3531\n    Batch 740/875, Loss: 0.9911\n    Batch 750/875, Loss: 0.8041\n    Batch 760/875, Loss: 0.4855\n    Batch 770/875, Loss: 0.3817\n    Batch 780/875, Loss: 0.8680\n    Batch 790/875, Loss: 0.6921\n    Batch 800/875, Loss: 0.4951\n    Batch 810/875, Loss: 0.5989\n    Batch 820/875, Loss: 0.9003\n    Batch 830/875, Loss: 0.4630\n    Batch 840/875, Loss: 0.9639\n    Batch 850/875, Loss: 0.5455\n    Batch 860/875, Loss: 0.8200\n    Batch 870/875, Loss: 0.4828\n    Batch 875/875, Loss: 0.2767\n    BAT Eval 17: LR=0.000010, Dropout=0.60, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 1.3901\n    Batch 20/219, Loss: 1.3553\n    Batch 30/219, Loss: 1.4084\n    Batch 40/219, Loss: 1.3364\n    Batch 50/219, Loss: 1.2978\n    Batch 60/219, Loss: 1.2949\n    Batch 70/219, Loss: 1.2333\n    Batch 80/219, Loss: 1.2644\n    Batch 90/219, Loss: 1.2198\n    Batch 100/219, Loss: 1.1223\n    Batch 110/219, Loss: 1.1505\n    Batch 120/219, Loss: 1.1219\n    Batch 130/219, Loss: 1.0786\n    Batch 140/219, Loss: 1.1180\n    Batch 150/219, Loss: 1.0918\n    Batch 160/219, Loss: 1.0913\n    Batch 170/219, Loss: 1.0572\n    Batch 180/219, Loss: 0.9886\n    Batch 190/219, Loss: 0.8629\n    Batch 200/219, Loss: 0.8717\n    Batch 210/219, Loss: 0.9385\n    Batch 219/219, Loss: 0.9963\n    BAT Eval 18: LR=0.000269, Dropout=0.58, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 1.0819\n    Batch 20/219, Loss: 1.0033\n    Batch 30/219, Loss: 1.3451\n    Batch 40/219, Loss: 1.4110\n    Batch 50/219, Loss: 0.7638\n    Batch 60/219, Loss: 0.7500\n    Batch 70/219, Loss: 1.7986\n    Batch 80/219, Loss: 0.9004\n    Batch 90/219, Loss: 1.0849\n    Batch 100/219, Loss: 0.9636\n    Batch 110/219, Loss: 0.9581\n    Batch 120/219, Loss: 0.8674\n    Batch 130/219, Loss: 0.6902\n    Batch 140/219, Loss: 0.6821\n    Batch 150/219, Loss: 0.7124\n    Batch 160/219, Loss: 0.5440\n    Batch 170/219, Loss: 0.8043\n    Batch 180/219, Loss: 0.5384\n    Batch 190/219, Loss: 0.7063\n    Batch 200/219, Loss: 0.8818\n    Batch 210/219, Loss: 0.6858\n    Batch 219/219, Loss: 0.7671\n    BAT Eval 19: LR=0.001371, Dropout=0.11, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.8676\n    Batch 20/219, Loss: 0.5783\n    Batch 30/219, Loss: 0.5139\n    Batch 40/219, Loss: 0.6087\n    Batch 50/219, Loss: 0.2721\n    Batch 60/219, Loss: 0.5201\n    Batch 70/219, Loss: 0.4217\n    Batch 80/219, Loss: 0.3193\n    Batch 90/219, Loss: 0.5002\n    Batch 100/219, Loss: 0.5697\n    Batch 110/219, Loss: 0.5908\n    Batch 120/219, Loss: 0.4769\n    Batch 130/219, Loss: 0.4611\n    Batch 140/219, Loss: 0.3113\n    Batch 150/219, Loss: 0.6140\n    Batch 160/219, Loss: 0.9918\n    Batch 170/219, Loss: 0.4729\n    Batch 180/219, Loss: 0.3167\n    Batch 190/219, Loss: 0.3121\n    Batch 200/219, Loss: 0.3543\n    Batch 210/219, Loss: 0.2741\n    Batch 219/219, Loss: 0.2618\n    BAT Eval 20: LR=0.000970, Dropout=0.01, BS=16\n    First batch loaded, starting training...\n    Batch 10/438, Loss: 0.6837\n    Batch 20/438, Loss: 0.9570\n    Batch 30/438, Loss: 0.5643\n    Batch 40/438, Loss: 0.7481\n    Batch 50/438, Loss: 0.4381\n    Batch 60/438, Loss: 0.6588\n    Batch 70/438, Loss: 0.7523\n    Batch 80/438, Loss: 0.8375\n    Batch 90/438, Loss: 0.6025\n    Batch 100/438, Loss: 0.5530\n    Batch 110/438, Loss: 0.4173\n    Batch 120/438, Loss: 0.7673\n    Batch 130/438, Loss: 0.9346\n    Batch 140/438, Loss: 0.7393\n    Batch 150/438, Loss: 0.6719\n    Batch 160/438, Loss: 0.3590\n    Batch 170/438, Loss: 0.4737\n    Batch 180/438, Loss: 0.4457\n    Batch 190/438, Loss: 0.5514\n    Batch 200/438, Loss: 0.5403\n    Batch 210/438, Loss: 0.3626\n    Batch 220/438, Loss: 0.5930\n    Batch 230/438, Loss: 0.4998\n    Batch 240/438, Loss: 0.5688\n    Batch 250/438, Loss: 0.4663\n    Batch 260/438, Loss: 0.9258\n    Batch 270/438, Loss: 0.5805\n    Batch 280/438, Loss: 0.6229\n    Batch 290/438, Loss: 0.7206\n    Batch 300/438, Loss: 0.7205\n    Batch 310/438, Loss: 0.6786\n    Batch 320/438, Loss: 0.8829\n    Batch 330/438, Loss: 0.4891\n    Batch 340/438, Loss: 0.7546\n    Batch 350/438, Loss: 0.3874\n    Batch 360/438, Loss: 0.5474\n    Batch 370/438, Loss: 0.4189\n    Batch 380/438, Loss: 0.5093\n    Batch 390/438, Loss: 0.3005\n    Batch 400/438, Loss: 0.5451\n    Batch 410/438, Loss: 0.4956\n    Batch 420/438, Loss: 0.8602\n    Batch 430/438, Loss: 0.3751\n    Batch 438/438, Loss: 0.6997\n    BAT Eval 21: LR=0.001288, Dropout=0.11, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.6175\n    Batch 20/219, Loss: 0.3561\n    Batch 30/219, Loss: 0.5763\n    Batch 40/219, Loss: 0.6649\n    Batch 50/219, Loss: 0.3587\n    Batch 60/219, Loss: 0.4642\n    Batch 70/219, Loss: 0.2333\n    Batch 80/219, Loss: 0.3328\n    Batch 90/219, Loss: 0.2386\n    Batch 100/219, Loss: 0.2119\n    Batch 110/219, Loss: 0.2765\n    Batch 120/219, Loss: 0.1764\n    Batch 130/219, Loss: 0.4441\n    Batch 140/219, Loss: 0.3765\n    Batch 150/219, Loss: 0.2757\n    Batch 160/219, Loss: 0.2639\n    Batch 170/219, Loss: 0.3105\n    Batch 180/219, Loss: 0.2164\n    Batch 190/219, Loss: 0.1591\n    Batch 200/219, Loss: 0.5579\n    Batch 210/219, Loss: 0.1662\n    Batch 219/219, Loss: 0.5310\n    BAT Eval 22: LR=0.001300, Dropout=0.11, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.7932\n    Batch 20/219, Loss: 0.6129\n    Batch 30/219, Loss: 0.6315\n    Batch 40/219, Loss: 0.4490\n    Batch 50/219, Loss: 0.4716\n    Batch 60/219, Loss: 0.2137\n    Batch 70/219, Loss: 0.2588\n    Batch 80/219, Loss: 0.7400\n    Batch 90/219, Loss: 0.3581\n    Batch 100/219, Loss: 0.2426\n    Batch 110/219, Loss: 0.5591\n    Batch 120/219, Loss: 0.2986\n    Batch 130/219, Loss: 0.6817\n    Batch 140/219, Loss: 0.1482\n    Batch 150/219, Loss: 0.3271\n    Batch 160/219, Loss: 0.1258\n    Batch 170/219, Loss: 0.2422\n    Batch 180/219, Loss: 0.4819\n    Batch 190/219, Loss: 0.2339\n    Batch 200/219, Loss: 0.2432\n    Batch 210/219, Loss: 0.4295\n    Batch 219/219, Loss: 0.2326\n    BAT Eval 23: LR=0.000010, Dropout=0.34, BS=16\n    First batch loaded, starting training...\n    Batch 10/438, Loss: 1.3779\n    Batch 20/438, Loss: 1.3590\n    Batch 30/438, Loss: 1.3174\n    Batch 40/438, Loss: 1.3282\n    Batch 50/438, Loss: 1.3577\n    Batch 60/438, Loss: 1.2904\n    Batch 70/438, Loss: 1.2418\n    Batch 80/438, Loss: 1.2461\n    Batch 90/438, Loss: 1.3510\n    Batch 100/438, Loss: 1.1644\n    Batch 110/438, Loss: 1.2044\n    Batch 120/438, Loss: 1.2381\n    Batch 130/438, Loss: 1.2163\n    Batch 140/438, Loss: 1.2218\n    Batch 150/438, Loss: 1.4068\n    Batch 160/438, Loss: 1.0972\n    Batch 170/438, Loss: 1.1221\n    Batch 180/438, Loss: 1.6025\n    Batch 190/438, Loss: 1.3913\n    Batch 200/438, Loss: 1.1665\n    Batch 210/438, Loss: 1.5124\n    Batch 220/438, Loss: 1.1869\n    Batch 230/438, Loss: 1.0500\n    Batch 240/438, Loss: 1.0372\n    Batch 250/438, Loss: 0.9496\n    Batch 260/438, Loss: 1.1051\n    Batch 270/438, Loss: 1.0615\n    Batch 280/438, Loss: 1.4344\n    Batch 290/438, Loss: 1.4503\n    Batch 300/438, Loss: 1.0800\n    Batch 310/438, Loss: 0.9708\n    Batch 320/438, Loss: 0.9727\n    Batch 330/438, Loss: 1.0556\n    Batch 340/438, Loss: 1.0780\n    Batch 350/438, Loss: 1.0767\n    Batch 360/438, Loss: 0.9165\n    Batch 370/438, Loss: 1.3479\n    Batch 380/438, Loss: 1.1458\n    Batch 390/438, Loss: 0.9177\n    Batch 400/438, Loss: 1.3181\n    Batch 410/438, Loss: 0.9977\n    Batch 420/438, Loss: 1.1161\n    Batch 430/438, Loss: 0.8104\n    Batch 438/438, Loss: 1.3511\n    BAT Eval 24: LR=0.001320, Dropout=0.12, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.6385\n    Batch 20/219, Loss: 0.8788\n    Batch 30/219, Loss: 0.4016\n    Batch 40/219, Loss: 0.2774\n    Batch 50/219, Loss: 0.3306\n    Batch 60/219, Loss: 0.4237\n    Batch 70/219, Loss: 0.3382\n    Batch 80/219, Loss: 0.6919\n    Batch 90/219, Loss: 0.4101\n    Batch 100/219, Loss: 0.3563\n    Batch 110/219, Loss: 0.3727\n    Batch 120/219, Loss: 0.3118\n    Batch 130/219, Loss: 0.2210\n    Batch 140/219, Loss: 0.4301\n    Batch 150/219, Loss: 0.4492\n    Batch 160/219, Loss: 0.2604\n    Batch 170/219, Loss: 0.3217\n    Batch 180/219, Loss: 0.3431\n    Batch 190/219, Loss: 0.1856\n    Batch 200/219, Loss: 0.2262\n    Batch 210/219, Loss: 0.4442\n    Batch 219/219, Loss: 0.1626\n  BAT Iter 1/4: Best = 0.8121\n    BAT Eval 25: LR=0.001284, Dropout=0.11, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.9949\n    Batch 20/219, Loss: 0.4962\n    Batch 30/219, Loss: 0.4033\n    Batch 40/219, Loss: 0.3715\n    Batch 50/219, Loss: 0.4748\n    Batch 60/219, Loss: 0.3085\n    Batch 70/219, Loss: 0.6587\n    Batch 80/219, Loss: 0.3281\n    Batch 90/219, Loss: 0.1926\n    Batch 100/219, Loss: 0.2277\n    Batch 110/219, Loss: 0.3106\n    Batch 120/219, Loss: 0.2394\n    Batch 130/219, Loss: 0.3107\n    Batch 140/219, Loss: 0.3396\n    Batch 150/219, Loss: 0.6800\n    Batch 160/219, Loss: 0.3888\n    Batch 170/219, Loss: 0.2131\n    Batch 180/219, Loss: 0.2117\n    Batch 190/219, Loss: 0.3862\n    Batch 200/219, Loss: 0.3138\n    Batch 210/219, Loss: 0.2691\n    Batch 219/219, Loss: 0.2795\n    BAT Eval 26: LR=0.001316, Dropout=0.11, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 1.2351\n    Batch 20/219, Loss: 0.9253\n    Batch 30/219, Loss: 0.7229\n    Batch 40/219, Loss: 0.3823\n    Batch 50/219, Loss: 0.5058\n    Batch 60/219, Loss: 0.6365\n    Batch 70/219, Loss: 0.4784\n    Batch 80/219, Loss: 0.6061\n    Batch 90/219, Loss: 0.2482\n    Batch 100/219, Loss: 0.2929\n    Batch 110/219, Loss: 0.4960\n    Batch 120/219, Loss: 0.2945\n    Batch 130/219, Loss: 0.2993\n    Batch 140/219, Loss: 0.3189\n    Batch 150/219, Loss: 0.2358\n    Batch 160/219, Loss: 0.3241\n    Batch 170/219, Loss: 0.3135\n    Batch 180/219, Loss: 0.1606\n    Batch 190/219, Loss: 0.2984\n    Batch 200/219, Loss: 0.5624\n    Batch 210/219, Loss: 0.5832\n    Batch 219/219, Loss: 0.2129\n    BAT Eval 27: LR=0.003000, Dropout=0.60, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 1.0681\n    Batch 20/219, Loss: 0.8844\n    Batch 30/219, Loss: 0.9034\n    Batch 40/219, Loss: 0.7443\n    Batch 50/219, Loss: 0.9958\n    Batch 60/219, Loss: 0.9357\n    Batch 70/219, Loss: 0.8505\n    Batch 80/219, Loss: 0.7966\n    Batch 90/219, Loss: 0.7258\n    Batch 100/219, Loss: 0.8688\n    Batch 110/219, Loss: 0.6980\n    Batch 120/219, Loss: 0.8688\n    Batch 130/219, Loss: 0.7573\n    Batch 140/219, Loss: 0.6460\n    Batch 150/219, Loss: 0.6657\n    Batch 160/219, Loss: 0.7393\n    Batch 170/219, Loss: 0.7610\n    Batch 180/219, Loss: 0.8210\n    Batch 190/219, Loss: 0.6898\n    Batch 200/219, Loss: 0.7084\n    Batch 210/219, Loss: 0.5617\n    Batch 219/219, Loss: 1.3724\n    BAT Eval 28: LR=0.000010, Dropout=0.58, BS=8\n    First batch loaded, starting training...\n    Batch 10/875, Loss: 1.3802\n    Batch 20/875, Loss: 1.3808\n    Batch 30/875, Loss: 1.3463\n    Batch 40/875, Loss: 1.3436\n    Batch 50/875, Loss: 1.3793\n    Batch 60/875, Loss: 1.4033\n    Batch 70/875, Loss: 1.4086\n    Batch 80/875, Loss: 1.3945\n    Batch 90/875, Loss: 1.3983\n    Batch 100/875, Loss: 1.3642\n    Batch 110/875, Loss: 1.2940\n    Batch 120/875, Loss: 1.2593\n    Batch 130/875, Loss: 1.1648\n    Batch 140/875, Loss: 1.3877\n    Batch 150/875, Loss: 1.1702\n    Batch 160/875, Loss: 1.4002\n    Batch 170/875, Loss: 1.3445\n    Batch 180/875, Loss: 1.2141\n    Batch 190/875, Loss: 1.2400\n    Batch 200/875, Loss: 1.0837\n    Batch 210/875, Loss: 1.1100\n    Batch 220/875, Loss: 1.2679\n    Batch 230/875, Loss: 1.2662\n    Batch 240/875, Loss: 1.1696\n    Batch 250/875, Loss: 1.1423\n    Batch 260/875, Loss: 1.2802\n    Batch 270/875, Loss: 1.4803\n    Batch 280/875, Loss: 1.2554\n    Batch 290/875, Loss: 1.0681\n    Batch 300/875, Loss: 1.2678\n    Batch 310/875, Loss: 1.3276\n    Batch 320/875, Loss: 1.2113\n    Batch 330/875, Loss: 1.5120\n    Batch 340/875, Loss: 1.0378\n    Batch 350/875, Loss: 0.8881\n    Batch 360/875, Loss: 1.1208\n    Batch 370/875, Loss: 0.8148\n    Batch 380/875, Loss: 0.9893\n    Batch 390/875, Loss: 1.2312\n    Batch 400/875, Loss: 0.8917\n    Batch 410/875, Loss: 1.2468\n    Batch 420/875, Loss: 0.9086\n    Batch 430/875, Loss: 1.6491\n    Batch 440/875, Loss: 1.7074\n    Batch 450/875, Loss: 1.0099\n    Batch 460/875, Loss: 0.8245\n    Batch 470/875, Loss: 0.9379\n    Batch 480/875, Loss: 0.8209\n    Batch 490/875, Loss: 0.8506\n    Batch 500/875, Loss: 1.2813\n    Batch 510/875, Loss: 1.4524\n    Batch 520/875, Loss: 0.7834\n    Batch 530/875, Loss: 0.6353\n    Batch 540/875, Loss: 0.6613\n    Batch 550/875, Loss: 1.1254\n    Batch 560/875, Loss: 0.8026\n    Batch 570/875, Loss: 0.8633\n    Batch 580/875, Loss: 0.5723\n    Batch 590/875, Loss: 1.0225\n    Batch 600/875, Loss: 0.5884\n    Batch 610/875, Loss: 0.6461\n    Batch 620/875, Loss: 0.6316\n    Batch 630/875, Loss: 0.6170\n    Batch 640/875, Loss: 1.8521\n    Batch 650/875, Loss: 1.3557\n    Batch 660/875, Loss: 0.7959\n    Batch 670/875, Loss: 0.8465\n    Batch 680/875, Loss: 0.8610\n    Batch 690/875, Loss: 0.8415\n    Batch 700/875, Loss: 0.8857\n    Batch 710/875, Loss: 0.5016\n    Batch 720/875, Loss: 0.6296\n    Batch 730/875, Loss: 0.7088\n    Batch 740/875, Loss: 0.9485\n    Batch 750/875, Loss: 0.4688\n    Batch 760/875, Loss: 0.7838\n    Batch 770/875, Loss: 0.5735\n    Batch 780/875, Loss: 0.6320\n    Batch 790/875, Loss: 1.1229\n    Batch 800/875, Loss: 0.5056\n    Batch 810/875, Loss: 0.9537\n    Batch 820/875, Loss: 1.0608\n    Batch 830/875, Loss: 0.4449\n    Batch 840/875, Loss: 0.6908\n    Batch 850/875, Loss: 0.7553\n    Batch 860/875, Loss: 0.4672\n    Batch 870/875, Loss: 1.0230\n    Batch 875/875, Loss: 0.9635\n    BAT Eval 29: LR=0.001259, Dropout=0.11, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.5752\n    Batch 20/219, Loss: 0.4590\n    Batch 30/219, Loss: 0.5170\n    Batch 40/219, Loss: 0.3549\n    Batch 50/219, Loss: 0.2955\n    Batch 60/219, Loss: 0.4530\n    Batch 70/219, Loss: 0.3624\n    Batch 80/219, Loss: 0.3583\n    Batch 90/219, Loss: 0.2594\n    Batch 100/219, Loss: 0.4473\n    Batch 110/219, Loss: 0.4909\n    Batch 120/219, Loss: 0.9712\n    Batch 130/219, Loss: 0.3415\n    Batch 140/219, Loss: 0.3813\n    Batch 150/219, Loss: 0.2537\n    Batch 160/219, Loss: 0.1809\n    Batch 170/219, Loss: 0.3036\n    Batch 180/219, Loss: 0.1478\n    Batch 190/219, Loss: 0.3141\n    Batch 200/219, Loss: 0.4042\n    Batch 210/219, Loss: 0.1248\n    Batch 219/219, Loss: 0.1786\n    BAT Eval 30: LR=0.000093, Dropout=0.60, BS=16\n    First batch loaded, starting training...\n    Batch 10/438, Loss: 1.3305\n    Batch 20/438, Loss: 1.2347\n    Batch 30/438, Loss: 1.1705\n    Batch 40/438, Loss: 1.4524\n    Batch 50/438, Loss: 1.0985\n    Batch 60/438, Loss: 0.9675\n    Batch 70/438, Loss: 1.1074\n    Batch 80/438, Loss: 1.1059\n    Batch 90/438, Loss: 0.8240\n    Batch 100/438, Loss: 0.8431\n    Batch 110/438, Loss: 0.7813\n    Batch 120/438, Loss: 1.0887\n    Batch 130/438, Loss: 1.1421\n    Batch 140/438, Loss: 1.3069\n    Batch 150/438, Loss: 0.9884\n    Batch 160/438, Loss: 1.2289\n    Batch 170/438, Loss: 1.0398\n    Batch 180/438, Loss: 0.6838\n    Batch 190/438, Loss: 0.7469\n    Batch 200/438, Loss: 1.0687\n    Batch 210/438, Loss: 1.0659\n    Batch 220/438, Loss: 0.7788\n    Batch 230/438, Loss: 0.7961\n    Batch 240/438, Loss: 1.2536\n    Batch 250/438, Loss: 0.5994\n    Batch 260/438, Loss: 0.8636\n    Batch 270/438, Loss: 0.7943\n    Batch 280/438, Loss: 0.9424\n    Batch 290/438, Loss: 0.8510\n    Batch 300/438, Loss: 0.8096\n    Batch 310/438, Loss: 0.4637\n    Batch 320/438, Loss: 1.4962\n    Batch 330/438, Loss: 0.9625\n    Batch 340/438, Loss: 0.7222\n    Batch 350/438, Loss: 0.7851\n    Batch 360/438, Loss: 0.8634\n    Batch 370/438, Loss: 0.7651\n    Batch 380/438, Loss: 0.7649\n    Batch 390/438, Loss: 1.2613\n    Batch 400/438, Loss: 0.7331\n    Batch 410/438, Loss: 0.5913\n    Batch 420/438, Loss: 0.5572\n    Batch 430/438, Loss: 1.3628\n    Batch 438/438, Loss: 1.4091\n    BAT Eval 31: LR=0.001315, Dropout=0.11, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.6226\n    Batch 20/219, Loss: 0.3894\n    Batch 30/219, Loss: 0.6710\n    Batch 40/219, Loss: 0.3309\n    Batch 50/219, Loss: 0.5120\n    Batch 60/219, Loss: 0.4339\n    Batch 70/219, Loss: 0.4153\n    Batch 80/219, Loss: 0.4987\n    Batch 90/219, Loss: 0.3570\n    Batch 100/219, Loss: 0.2334\n    Batch 110/219, Loss: 0.2007\n    Batch 120/219, Loss: 0.7167\n    Batch 130/219, Loss: 0.4576\n    Batch 140/219, Loss: 0.6326\n    Batch 150/219, Loss: 0.2894\n    Batch 160/219, Loss: 0.2087\n    Batch 170/219, Loss: 0.7970\n    Batch 180/219, Loss: 0.4826\n    Batch 190/219, Loss: 0.1508\n    Batch 200/219, Loss: 0.2751\n    Batch 210/219, Loss: 0.3045\n    Batch 219/219, Loss: 0.2853\n    BAT Eval 32: LR=0.001310, Dropout=0.11, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.8536\n    Batch 20/219, Loss: 0.7392\n    Batch 30/219, Loss: 0.6239\n    Batch 40/219, Loss: 0.7457\n    Batch 50/219, Loss: 0.7732\n    Batch 60/219, Loss: 0.5054\n    Batch 70/219, Loss: 0.5925\n    Batch 80/219, Loss: 0.4991\n    Batch 90/219, Loss: 0.5130\n    Batch 100/219, Loss: 0.4166\n    Batch 110/219, Loss: 0.2471\n    Batch 120/219, Loss: 0.2049\n    Batch 130/219, Loss: 0.1773\n    Batch 140/219, Loss: 0.2414\n    Batch 150/219, Loss: 0.2295\n    Batch 160/219, Loss: 0.4275\n    Batch 170/219, Loss: 0.2758\n    Batch 180/219, Loss: 0.2754\n    Batch 190/219, Loss: 0.1878\n    Batch 200/219, Loss: 0.2750\n    Batch 210/219, Loss: 0.2905\n    Batch 219/219, Loss: 0.4919\n    BAT Eval 33: LR=0.001319, Dropout=0.11, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 1.0911\n    Batch 20/219, Loss: 0.4693\n    Batch 30/219, Loss: 0.3257\n    Batch 40/219, Loss: 0.2977\n    Batch 50/219, Loss: 0.4192\n    Batch 60/219, Loss: 0.3292\n    Batch 70/219, Loss: 0.4694\n    Batch 80/219, Loss: 0.2768\n    Batch 90/219, Loss: 0.1918\n    Batch 100/219, Loss: 0.6149\n    Batch 110/219, Loss: 0.2509\n    Batch 120/219, Loss: 0.2753\n    Batch 130/219, Loss: 0.9462\n    Batch 140/219, Loss: 0.3170\n    Batch 150/219, Loss: 0.2967\n    Batch 160/219, Loss: 0.2233\n    Batch 170/219, Loss: 0.2522\n    Batch 180/219, Loss: 0.1908\n    Batch 190/219, Loss: 0.4654\n    Batch 200/219, Loss: 0.2933\n    Batch 210/219, Loss: 0.2645\n    Batch 219/219, Loss: 0.1745\n    BAT Eval 34: LR=0.001292, Dropout=0.11, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 1.2390\n    Batch 20/219, Loss: 0.9745\n    Batch 30/219, Loss: 0.4832\n    Batch 40/219, Loss: 0.3095\n    Batch 50/219, Loss: 0.5776\n    Batch 60/219, Loss: 0.1956\n    Batch 70/219, Loss: 0.3324\n    Batch 80/219, Loss: 0.5798\n    Batch 90/219, Loss: 0.4445\n    Batch 100/219, Loss: 0.2833\n    Batch 110/219, Loss: 0.2795\n    Batch 120/219, Loss: 0.1471\n    Batch 130/219, Loss: 0.2398\n    Batch 140/219, Loss: 0.3695\n    Batch 150/219, Loss: 0.2590\n    Batch 160/219, Loss: 0.2086\n    Batch 170/219, Loss: 0.2989\n    Batch 180/219, Loss: 0.1496\n    Batch 190/219, Loss: 0.6264\n    Batch 200/219, Loss: 0.1593\n    Batch 210/219, Loss: 0.1868\n    Batch 219/219, Loss: 0.2348\n    BAT Eval 35: LR=0.001270, Dropout=0.11, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.6963\n    Batch 20/219, Loss: 0.7979\n    Batch 30/219, Loss: 0.5179\n    Batch 40/219, Loss: 0.3023\n    Batch 50/219, Loss: 0.3125\n    Batch 60/219, Loss: 0.4220\n    Batch 70/219, Loss: 0.5092\n    Batch 80/219, Loss: 0.5045\n    Batch 90/219, Loss: 0.5113\n    Batch 100/219, Loss: 0.1359\n    Batch 110/219, Loss: 0.2989\n    Batch 120/219, Loss: 0.3690\n    Batch 130/219, Loss: 0.3391\n    Batch 140/219, Loss: 0.3543\n    Batch 150/219, Loss: 0.6317\n    Batch 160/219, Loss: 0.1855\n    Batch 170/219, Loss: 0.2563\n    Batch 180/219, Loss: 0.2405\n    Batch 190/219, Loss: 0.3075\n    Batch 200/219, Loss: 0.1278\n    Batch 210/219, Loss: 0.1678\n    Batch 219/219, Loss: 0.3432\n    BAT Eval 36: LR=0.001388, Dropout=0.11, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.5953\n    Batch 20/219, Loss: 0.5448\n    Batch 30/219, Loss: 1.2925\n    Batch 40/219, Loss: 1.3285\n    Batch 50/219, Loss: 0.2642\n    Batch 60/219, Loss: 0.5883\n    Batch 70/219, Loss: 0.8143\n    Batch 80/219, Loss: 0.4217\n    Batch 90/219, Loss: 0.3090\n    Batch 100/219, Loss: 0.6017\n    Batch 110/219, Loss: 0.3421\n    Batch 120/219, Loss: 0.1926\n    Batch 130/219, Loss: 0.1461\n    Batch 140/219, Loss: 0.5350\n    Batch 150/219, Loss: 0.2312\n    Batch 160/219, Loss: 0.4085\n    Batch 170/219, Loss: 0.3254\n    Batch 180/219, Loss: 0.4948\n    Batch 190/219, Loss: 0.3890\n    Batch 200/219, Loss: 0.3555\n    Batch 210/219, Loss: 0.2315\n    Batch 219/219, Loss: 0.2213\n  BAT Iter 2/4: Best = 0.8121\n    BAT Eval 37: LR=0.001313, Dropout=0.11, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.7467\n    Batch 20/219, Loss: 0.5893\n    Batch 30/219, Loss: 0.5250\n    Batch 40/219, Loss: 0.3060\n    Batch 50/219, Loss: 0.3152\n    Batch 60/219, Loss: 0.7347\n    Batch 70/219, Loss: 0.4756\n    Batch 80/219, Loss: 0.3698\n    Batch 90/219, Loss: 0.6531\n    Batch 100/219, Loss: 0.2622\n    Batch 110/219, Loss: 0.2036\n    Batch 120/219, Loss: 0.1726\n    Batch 130/219, Loss: 0.3778\n    Batch 140/219, Loss: 0.2669\n    Batch 150/219, Loss: 0.4464\n    Batch 160/219, Loss: 0.2631\n    Batch 170/219, Loss: 0.3147\n    Batch 180/219, Loss: 0.4830\n    Batch 190/219, Loss: 0.3388\n    Batch 200/219, Loss: 0.1901\n    Batch 210/219, Loss: 0.1652\n    Batch 219/219, Loss: 0.3038\n    BAT Eval 38: LR=0.001344, Dropout=0.11, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.6740\n    Batch 20/219, Loss: 0.2809\n    Batch 30/219, Loss: 0.3069\n    Batch 40/219, Loss: 0.3955\n    Batch 50/219, Loss: 0.3169\n    Batch 60/219, Loss: 0.4513\n    Batch 70/219, Loss: 0.4918\n    Batch 80/219, Loss: 0.4396\n    Batch 90/219, Loss: 0.4178\n    Batch 100/219, Loss: 0.3339\n    Batch 110/219, Loss: 0.2531\n    Batch 120/219, Loss: 0.4363\n    Batch 130/219, Loss: 0.3327\n    Batch 140/219, Loss: 0.2623\n    Batch 150/219, Loss: 0.4271\n    Batch 160/219, Loss: 0.4968\n    Batch 170/219, Loss: 0.2023\n    Batch 180/219, Loss: 0.3235\n    Batch 190/219, Loss: 0.3475\n    Batch 200/219, Loss: 0.1783\n    Batch 210/219, Loss: 0.3889\n    Batch 219/219, Loss: 0.2445\n    BAT Eval 39: LR=0.001344, Dropout=0.12, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 1.6882\n    Batch 20/219, Loss: 0.4921\n    Batch 30/219, Loss: 0.3198\n    Batch 40/219, Loss: 0.4652\n    Batch 50/219, Loss: 0.5070\n    Batch 60/219, Loss: 0.3129\n    Batch 70/219, Loss: 0.3601\n    Batch 80/219, Loss: 0.3330\n    Batch 90/219, Loss: 0.2514\n    Batch 100/219, Loss: 0.3016\n    Batch 110/219, Loss: 0.2115\n    Batch 120/219, Loss: 0.3090\n    Batch 130/219, Loss: 0.3111\n    Batch 140/219, Loss: 0.2794\n    Batch 150/219, Loss: 0.2384\n    Batch 160/219, Loss: 0.2949\n    Batch 170/219, Loss: 0.2232\n    Batch 180/219, Loss: 0.4526\n    Batch 190/219, Loss: 0.2547\n    Batch 200/219, Loss: 0.4886\n    Batch 210/219, Loss: 0.4569\n    Batch 219/219, Loss: 0.3620\n    BAT Eval 40: LR=0.001367, Dropout=0.11, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.9671\n    Batch 20/219, Loss: 0.6177\n    Batch 30/219, Loss: 0.5351\n    Batch 40/219, Loss: 0.4923\n    Batch 50/219, Loss: 0.5271\n    Batch 60/219, Loss: 0.4808\n    Batch 70/219, Loss: 0.2428\n    Batch 80/219, Loss: 0.2526\n    Batch 90/219, Loss: 0.5727\n    Batch 100/219, Loss: 0.2698\n    Batch 110/219, Loss: 0.4622\n    Batch 120/219, Loss: 0.4539\n    Batch 130/219, Loss: 0.5341\n    Batch 140/219, Loss: 0.4480\n    Batch 150/219, Loss: 0.3822\n    Batch 160/219, Loss: 0.1988\n    Batch 170/219, Loss: 0.5202\n    Batch 180/219, Loss: 0.5004\n    Batch 190/219, Loss: 0.5598\n    Batch 200/219, Loss: 0.3551\n    Batch 210/219, Loss: 0.2213\n    Batch 219/219, Loss: 0.2031\n    BAT Eval 41: LR=0.001289, Dropout=0.11, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.9156\n    Batch 20/219, Loss: 1.0337\n    Batch 30/219, Loss: 0.4281\n    Batch 40/219, Loss: 0.4950\n    Batch 50/219, Loss: 0.4908\n    Batch 60/219, Loss: 0.2533\n    Batch 70/219, Loss: 0.6987\n    Batch 80/219, Loss: 0.3447\n    Batch 90/219, Loss: 0.1560\n    Batch 100/219, Loss: 0.2307\n    Batch 110/219, Loss: 0.3069\n    Batch 120/219, Loss: 0.4625\n    Batch 130/219, Loss: 0.3971\n    Batch 140/219, Loss: 0.2426\n    Batch 150/219, Loss: 0.2575\n    Batch 160/219, Loss: 0.4690\n    Batch 170/219, Loss: 0.4301\n    Batch 180/219, Loss: 0.3641\n    Batch 190/219, Loss: 0.3108\n    Batch 200/219, Loss: 0.3199\n    Batch 210/219, Loss: 0.3969\n    Batch 219/219, Loss: 0.2045\n    BAT Eval 42: LR=0.000022, Dropout=0.60, BS=16\n    First batch loaded, starting training...\n    Batch 10/438, Loss: 1.3098\n    Batch 20/438, Loss: 1.2638\n    Batch 30/438, Loss: 1.2602\n    Batch 40/438, Loss: 1.2433\n    Batch 50/438, Loss: 1.2015\n    Batch 60/438, Loss: 1.1924\n    Batch 70/438, Loss: 1.3021\n    Batch 80/438, Loss: 1.3035\n    Batch 90/438, Loss: 1.2193\n    Batch 100/438, Loss: 0.9894\n    Batch 110/438, Loss: 1.0888\n    Batch 120/438, Loss: 0.9759\n    Batch 130/438, Loss: 1.1630\n    Batch 140/438, Loss: 0.9341\n    Batch 150/438, Loss: 1.3460\n    Batch 160/438, Loss: 1.0036\n    Batch 170/438, Loss: 1.1773\n    Batch 180/438, Loss: 0.7882\n    Batch 190/438, Loss: 1.8299\n    Batch 200/438, Loss: 0.9137\n    Batch 210/438, Loss: 0.8660\n    Batch 220/438, Loss: 0.8566\n    Batch 230/438, Loss: 0.7803\n    Batch 240/438, Loss: 1.1911\n    Batch 250/438, Loss: 1.2091\n    Batch 260/438, Loss: 1.0469\n    Batch 270/438, Loss: 1.3278\n    Batch 280/438, Loss: 1.3480\n    Batch 290/438, Loss: 0.8829\n    Batch 300/438, Loss: 0.9409\n    Batch 310/438, Loss: 1.0211\n    Batch 320/438, Loss: 0.8605\n    Batch 330/438, Loss: 0.9461\n    Batch 340/438, Loss: 0.8840\n    Batch 350/438, Loss: 0.8151\n    Batch 360/438, Loss: 0.6644\n    Batch 370/438, Loss: 1.2832\n    Batch 380/438, Loss: 0.8947\n    Batch 390/438, Loss: 0.9048\n    Batch 400/438, Loss: 1.0776\n    Batch 410/438, Loss: 0.8446\n    Batch 420/438, Loss: 0.9143\n    Batch 430/438, Loss: 1.2784\n    Batch 438/438, Loss: 0.7669\n    BAT Eval 43: LR=0.001348, Dropout=0.11, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.7124\n    Batch 20/219, Loss: 1.3565\n    Batch 30/219, Loss: 0.5590\n    Batch 40/219, Loss: 0.5297\n    Batch 50/219, Loss: 0.4524\n    Batch 60/219, Loss: 0.4707\n    Batch 70/219, Loss: 0.5866\n    Batch 80/219, Loss: 0.4507\n    Batch 90/219, Loss: 0.2636\n    Batch 100/219, Loss: 0.2553\n    Batch 110/219, Loss: 0.3071\n    Batch 120/219, Loss: 0.5904\n    Batch 130/219, Loss: 0.6297\n    Batch 140/219, Loss: 0.3266\n    Batch 150/219, Loss: 0.2091\n    Batch 160/219, Loss: 0.3057\n    Batch 170/219, Loss: 0.1815\n    Batch 180/219, Loss: 0.2694\n    Batch 190/219, Loss: 0.2619\n    Batch 200/219, Loss: 0.1632\n    Batch 210/219, Loss: 0.1509\n    Batch 219/219, Loss: 0.4189\n    BAT Eval 44: LR=0.000733, Dropout=0.00, BS=8\n    First batch loaded, starting training...\n    Batch 10/875, Loss: 2.7092\n    Batch 20/875, Loss: 1.0658\n    Batch 30/875, Loss: 0.9168\n    Batch 40/875, Loss: 0.7266\n    Batch 50/875, Loss: 0.5201\n    Batch 60/875, Loss: 0.8427\n    Batch 70/875, Loss: 0.7904\n    Batch 80/875, Loss: 0.6471\n    Batch 90/875, Loss: 0.7103\n    Batch 100/875, Loss: 0.7528\n    Batch 110/875, Loss: 1.2742\n    Batch 120/875, Loss: 0.7058\n    Batch 130/875, Loss: 0.7993\n    Batch 140/875, Loss: 0.2813\n    Batch 150/875, Loss: 0.4521\n    Batch 160/875, Loss: 0.6436\n    Batch 170/875, Loss: 2.0683\n    Batch 180/875, Loss: 1.1426\n    Batch 190/875, Loss: 1.0219\n    Batch 200/875, Loss: 0.7433\n    Batch 210/875, Loss: 1.3490\n    Batch 220/875, Loss: 1.2278\n    Batch 230/875, Loss: 0.4111\n    Batch 240/875, Loss: 0.5953\n    Batch 250/875, Loss: 0.4488\n    Batch 260/875, Loss: 0.5123\n    Batch 270/875, Loss: 0.7797\n    Batch 280/875, Loss: 0.7808\n    Batch 290/875, Loss: 0.8758\n    Batch 300/875, Loss: 1.1525\n    Batch 310/875, Loss: 0.5678\n    Batch 320/875, Loss: 0.3686\n    Batch 330/875, Loss: 1.1160\n    Batch 340/875, Loss: 0.3806\n    Batch 350/875, Loss: 0.8724\n    Batch 360/875, Loss: 0.3667\n    Batch 370/875, Loss: 0.4383\n    Batch 380/875, Loss: 1.7265\n    Batch 390/875, Loss: 0.9058\n    Batch 400/875, Loss: 0.7169\n    Batch 410/875, Loss: 0.5517\n    Batch 420/875, Loss: 0.3925\n    Batch 430/875, Loss: 0.8506\n    Batch 440/875, Loss: 0.5101\n    Batch 450/875, Loss: 0.6542\n    Batch 460/875, Loss: 0.5097\n    Batch 470/875, Loss: 0.7388\n    Batch 480/875, Loss: 0.4267\n    Batch 490/875, Loss: 0.8047\n    Batch 500/875, Loss: 0.4209\n    Batch 510/875, Loss: 0.3037\n    Batch 520/875, Loss: 0.6995\n    Batch 530/875, Loss: 0.4450\n    Batch 540/875, Loss: 0.2927\n    Batch 550/875, Loss: 0.9145\n    Batch 560/875, Loss: 0.3627\n    Batch 570/875, Loss: 0.4465\n    Batch 580/875, Loss: 0.8508\n    Batch 590/875, Loss: 0.8616\n    Batch 600/875, Loss: 0.4565\n    Batch 610/875, Loss: 0.5480\n    Batch 620/875, Loss: 0.8473\n    Batch 630/875, Loss: 1.4013\n    Batch 640/875, Loss: 0.6037\n    Batch 650/875, Loss: 0.4448\n    Batch 660/875, Loss: 1.0041\n    Batch 670/875, Loss: 0.4922\n    Batch 680/875, Loss: 0.9120\n    Batch 690/875, Loss: 0.9396\n    Batch 700/875, Loss: 0.6171\n    Batch 710/875, Loss: 0.2153\n    Batch 720/875, Loss: 0.4193\n    Batch 730/875, Loss: 0.7439\n    Batch 740/875, Loss: 0.3276\n    Batch 750/875, Loss: 1.5991\n    Batch 760/875, Loss: 0.7267\n    Batch 770/875, Loss: 0.3574\n    Batch 780/875, Loss: 0.4603\n    Batch 790/875, Loss: 0.7325\n    Batch 800/875, Loss: 1.1635\n    Batch 810/875, Loss: 0.3922\n    Batch 820/875, Loss: 0.7324\n    Batch 830/875, Loss: 1.0435\n    Batch 840/875, Loss: 0.9210\n    Batch 850/875, Loss: 0.5163\n    Batch 860/875, Loss: 0.6712\n    Batch 870/875, Loss: 0.6570\n    Batch 875/875, Loss: 0.7119\n    BAT Eval 45: LR=0.001319, Dropout=0.11, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 1.4675\n    Batch 20/219, Loss: 0.8730\n    Batch 30/219, Loss: 0.6279\n    Batch 40/219, Loss: 0.3605\n    Batch 50/219, Loss: 0.4103\n    Batch 60/219, Loss: 0.4173\n    Batch 70/219, Loss: 0.6183\n    Batch 80/219, Loss: 0.3367\n    Batch 90/219, Loss: 0.3046\n    Batch 100/219, Loss: 0.2459\n    Batch 110/219, Loss: 0.3118\n    Batch 120/219, Loss: 0.2636\n    Batch 130/219, Loss: 0.3512\n    Batch 140/219, Loss: 0.3876\n    Batch 150/219, Loss: 0.2400\n    Batch 160/219, Loss: 0.4326\n    Batch 170/219, Loss: 0.1871\n    Batch 180/219, Loss: 0.8968\n    Batch 190/219, Loss: 0.4536\n    Batch 200/219, Loss: 0.2552\n    Batch 210/219, Loss: 0.8833\n    Batch 219/219, Loss: 0.2284\n    BAT Eval 46: LR=0.001285, Dropout=0.11, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.8594\n    Batch 20/219, Loss: 0.8242\n    Batch 30/219, Loss: 0.7333\n    Batch 40/219, Loss: 0.6337\n    Batch 50/219, Loss: 0.3934\n    Batch 60/219, Loss: 0.6111\n    Batch 70/219, Loss: 0.5784\n    Batch 80/219, Loss: 0.4229\n    Batch 90/219, Loss: 0.3258\n    Batch 100/219, Loss: 0.3280\n    Batch 110/219, Loss: 0.3786\n    Batch 120/219, Loss: 0.6041\n    Batch 130/219, Loss: 0.2374\n    Batch 140/219, Loss: 0.2390\n    Batch 150/219, Loss: 0.4000\n    Batch 160/219, Loss: 0.5235\n    Batch 170/219, Loss: 0.3869\n    Batch 180/219, Loss: 0.4211\n    Batch 190/219, Loss: 0.2038\n    Batch 200/219, Loss: 0.1795\n    Batch 210/219, Loss: 0.2206\n    Batch 219/219, Loss: 0.3755\n    BAT Eval 47: LR=0.001362, Dropout=0.11, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.7863\n    Batch 20/219, Loss: 0.5230\n    Batch 30/219, Loss: 0.4639\n    Batch 40/219, Loss: 0.8160\n    Batch 50/219, Loss: 0.3432\n    Batch 60/219, Loss: 0.3325\n    Batch 70/219, Loss: 0.3876\n    Batch 80/219, Loss: 0.3671\n    Batch 90/219, Loss: 0.6180\n    Batch 100/219, Loss: 0.5512\n    Batch 110/219, Loss: 0.4051\n    Batch 120/219, Loss: 0.2433\n    Batch 130/219, Loss: 0.3833\n    Batch 140/219, Loss: 0.4489\n    Batch 150/219, Loss: 0.3886\n    Batch 160/219, Loss: 0.3427\n    Batch 170/219, Loss: 0.2334\n    Batch 180/219, Loss: 0.2135\n    Batch 190/219, Loss: 0.5839\n    Batch 200/219, Loss: 0.3772\n    Batch 210/219, Loss: 0.5008\n    Batch 219/219, Loss: 0.5812\n    BAT Eval 48: LR=0.000852, Dropout=0.11, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.5546\n    Batch 20/219, Loss: 0.6842\n    Batch 30/219, Loss: 0.6095\n    Batch 40/219, Loss: 0.5852\n    Batch 50/219, Loss: 0.2833\n    Batch 60/219, Loss: 0.2953\n    Batch 70/219, Loss: 0.4560\n    Batch 80/219, Loss: 0.1959\n    Batch 90/219, Loss: 0.4518\n    Batch 100/219, Loss: 0.1500\n    Batch 110/219, Loss: 0.3551\n    Batch 120/219, Loss: 0.1770\n    Batch 130/219, Loss: 0.2160\n    Batch 140/219, Loss: 0.5790\n    Batch 150/219, Loss: 0.4541\n    Batch 160/219, Loss: 0.4897\n    Batch 170/219, Loss: 0.2520\n    Batch 180/219, Loss: 0.2240\n    Batch 190/219, Loss: 0.3588\n    Batch 200/219, Loss: 0.1621\n    Batch 210/219, Loss: 0.1227\n    Batch 219/219, Loss: 0.1664\n  BAT Iter 3/4: Best = 0.8121\n    BAT Eval 49: LR=0.001287, Dropout=0.11, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.6508\n    Batch 20/219, Loss: 0.9427\n    Batch 30/219, Loss: 0.8473\n    Batch 40/219, Loss: 0.4036\n    Batch 50/219, Loss: 0.3868\n    Batch 60/219, Loss: 0.6446\n    Batch 70/219, Loss: 0.4143\n    Batch 80/219, Loss: 0.4479\n    Batch 90/219, Loss: 0.6663\n    Batch 100/219, Loss: 0.5167\n    Batch 110/219, Loss: 0.5174\n    Batch 120/219, Loss: 0.2930\n    Batch 130/219, Loss: 0.1491\n    Batch 140/219, Loss: 0.3493\n    Batch 150/219, Loss: 0.1763\n    Batch 160/219, Loss: 0.2544\n    Batch 170/219, Loss: 0.4195\n    Batch 180/219, Loss: 0.2812\n    Batch 190/219, Loss: 0.1827\n    Batch 200/219, Loss: 0.2712\n    Batch 210/219, Loss: 0.3377\n    Batch 219/219, Loss: 0.3155\n    BAT Eval 50: LR=0.001322, Dropout=0.11, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 1.0620\n    Batch 20/219, Loss: 0.4196\n    Batch 30/219, Loss: 0.7827\n    Batch 40/219, Loss: 0.5145\n    Batch 50/219, Loss: 0.4312\n    Batch 60/219, Loss: 0.9519\n    Batch 70/219, Loss: 0.3194\n    Batch 80/219, Loss: 0.3631\n    Batch 90/219, Loss: 0.4740\n    Batch 100/219, Loss: 0.3700\n    Batch 110/219, Loss: 0.1469\n    Batch 120/219, Loss: 0.2772\n    Batch 130/219, Loss: 0.3599\n    Batch 140/219, Loss: 0.2578\n    Batch 150/219, Loss: 0.1901\n    Batch 160/219, Loss: 0.1396\n    Batch 170/219, Loss: 0.3973\n    Batch 180/219, Loss: 0.2619\n    Batch 190/219, Loss: 0.2086\n    Batch 200/219, Loss: 0.3335\n    Batch 210/219, Loss: 0.1723\n    Batch 219/219, Loss: 0.1339\n    BAT Eval 51: LR=0.001371, Dropout=0.11, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 1.1660\n    Batch 20/219, Loss: 0.6494\n    Batch 30/219, Loss: 0.7156\n    Batch 40/219, Loss: 0.3579\n    Batch 50/219, Loss: 0.4690\n    Batch 60/219, Loss: 0.6846\n    Batch 70/219, Loss: 0.3689\n    Batch 80/219, Loss: 0.2815\n    Batch 90/219, Loss: 0.2369\n    Batch 100/219, Loss: 0.4611\n    Batch 110/219, Loss: 0.2486\n    Batch 120/219, Loss: 0.3348\n    Batch 130/219, Loss: 0.3229\n    Batch 140/219, Loss: 0.3122\n    Batch 150/219, Loss: 0.2492\n    Batch 160/219, Loss: 0.3560\n    Batch 170/219, Loss: 0.1986\n    Batch 180/219, Loss: 0.2506\n    Batch 190/219, Loss: 0.1882\n    Batch 200/219, Loss: 0.0889\n    Batch 210/219, Loss: 0.2004\n    Batch 219/219, Loss: 0.1910\n    BAT Eval 52: LR=0.001272, Dropout=0.11, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.8499\n    Batch 20/219, Loss: 0.6204\n    Batch 30/219, Loss: 0.3797\n    Batch 40/219, Loss: 0.5538\n    Batch 50/219, Loss: 0.3979\n    Batch 60/219, Loss: 0.2913\n    Batch 70/219, Loss: 0.3221\n    Batch 80/219, Loss: 0.5950\n    Batch 90/219, Loss: 0.5171\n    Batch 100/219, Loss: 0.4415\n    Batch 110/219, Loss: 0.6055\n    Batch 120/219, Loss: 0.4251\n    Batch 130/219, Loss: 0.2502\n    Batch 140/219, Loss: 0.2580\n    Batch 150/219, Loss: 0.3521\n    Batch 160/219, Loss: 0.1735\n    Batch 170/219, Loss: 0.5232\n    Batch 180/219, Loss: 0.1794\n    Batch 190/219, Loss: 0.3882\n    Batch 200/219, Loss: 0.3431\n    Batch 210/219, Loss: 0.5550\n    Batch 219/219, Loss: 0.3577\n    BAT Eval 53: LR=0.001284, Dropout=0.11, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.6196\n    Batch 20/219, Loss: 0.6496\n    Batch 30/219, Loss: 0.3358\n    Batch 40/219, Loss: 0.8355\n    Batch 50/219, Loss: 0.4748\n    Batch 60/219, Loss: 0.5254\n    Batch 70/219, Loss: 0.2690\n    Batch 80/219, Loss: 0.2537\n    Batch 90/219, Loss: 0.3949\n    Batch 100/219, Loss: 0.2334\n    Batch 110/219, Loss: 0.2732\n    Batch 120/219, Loss: 0.1724\n    Batch 130/219, Loss: 0.3456\n    Batch 140/219, Loss: 0.3131\n    Batch 150/219, Loss: 0.2392\n    Batch 160/219, Loss: 0.2313\n    Batch 170/219, Loss: 0.2269\n    Batch 180/219, Loss: 0.2323\n    Batch 190/219, Loss: 0.2633\n    Batch 200/219, Loss: 0.1701\n    Batch 210/219, Loss: 0.2341\n    Batch 219/219, Loss: 0.2119\n    BAT Eval 54: LR=0.001290, Dropout=0.12, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.9855\n    Batch 20/219, Loss: 0.9227\n    Batch 30/219, Loss: 0.6794\n    Batch 40/219, Loss: 0.9189\n    Batch 50/219, Loss: 0.5073\n    Batch 60/219, Loss: 0.3344\n    Batch 70/219, Loss: 0.5644\n    Batch 80/219, Loss: 0.3920\n    Batch 90/219, Loss: 0.4414\n    Batch 100/219, Loss: 0.2973\n    Batch 110/219, Loss: 0.2110\n    Batch 120/219, Loss: 0.2120\n    Batch 130/219, Loss: 0.4063\n    Batch 140/219, Loss: 0.2831\n    Batch 150/219, Loss: 0.2541\n    Batch 160/219, Loss: 0.1679\n    Batch 170/219, Loss: 0.2083\n    Batch 180/219, Loss: 0.2884\n    Batch 190/219, Loss: 0.3775\n    Batch 200/219, Loss: 0.1757\n    Batch 210/219, Loss: 0.2193\n    Batch 219/219, Loss: 0.2099\n    BAT Eval 55: LR=0.000010, Dropout=0.24, BS=8\n    First batch loaded, starting training...\n    Batch 10/875, Loss: 1.3245\n    Batch 20/875, Loss: 1.4037\n    Batch 30/875, Loss: 1.3574\n    Batch 40/875, Loss: 1.2819\n    Batch 50/875, Loss: 1.3894\n    Batch 60/875, Loss: 1.3300\n    Batch 70/875, Loss: 1.3583\n    Batch 80/875, Loss: 1.3219\n    Batch 90/875, Loss: 1.2778\n    Batch 100/875, Loss: 1.3196\n    Batch 110/875, Loss: 1.2958\n    Batch 120/875, Loss: 1.2012\n    Batch 130/875, Loss: 1.1859\n    Batch 140/875, Loss: 1.2535\n    Batch 150/875, Loss: 1.1066\n    Batch 160/875, Loss: 1.4508\n    Batch 170/875, Loss: 1.2146\n    Batch 180/875, Loss: 1.4976\n    Batch 190/875, Loss: 1.1150\n    Batch 200/875, Loss: 1.2976\n    Batch 210/875, Loss: 1.0022\n    Batch 220/875, Loss: 1.0788\n    Batch 230/875, Loss: 1.5798\n    Batch 240/875, Loss: 1.1521\n    Batch 250/875, Loss: 1.0983\n    Batch 260/875, Loss: 0.9102\n    Batch 270/875, Loss: 0.8878\n    Batch 280/875, Loss: 0.7926\n    Batch 290/875, Loss: 0.7874\n    Batch 300/875, Loss: 0.6347\n    Batch 310/875, Loss: 0.6761\n    Batch 320/875, Loss: 0.6177\n    Batch 330/875, Loss: 0.9233\n    Batch 340/875, Loss: 0.9207\n    Batch 350/875, Loss: 0.7622\n    Batch 360/875, Loss: 0.8591\n    Batch 370/875, Loss: 0.6724\n    Batch 380/875, Loss: 0.9812\n    Batch 390/875, Loss: 0.9341\n    Batch 400/875, Loss: 1.0114\n    Batch 410/875, Loss: 0.9046\n    Batch 420/875, Loss: 1.0236\n    Batch 430/875, Loss: 0.4876\n    Batch 440/875, Loss: 0.6291\n    Batch 450/875, Loss: 0.5212\n    Batch 460/875, Loss: 0.5301\n    Batch 470/875, Loss: 1.1226\n    Batch 480/875, Loss: 0.8151\n    Batch 490/875, Loss: 0.5423\n    Batch 500/875, Loss: 0.7178\n    Batch 510/875, Loss: 1.0444\n    Batch 520/875, Loss: 0.9905\n    Batch 530/875, Loss: 0.8245\n    Batch 540/875, Loss: 0.5702\n    Batch 550/875, Loss: 1.8655\n    Batch 560/875, Loss: 0.5128\n    Batch 570/875, Loss: 1.9004\n    Batch 580/875, Loss: 0.9538\n    Batch 590/875, Loss: 0.9937\n    Batch 600/875, Loss: 0.7938\n    Batch 610/875, Loss: 0.6744\n    Batch 620/875, Loss: 1.1511\n    Batch 630/875, Loss: 0.6560\n    Batch 640/875, Loss: 0.5595\n    Batch 650/875, Loss: 1.0251\n    Batch 660/875, Loss: 0.8131\n    Batch 670/875, Loss: 0.5437\n    Batch 680/875, Loss: 0.3895\n    Batch 690/875, Loss: 0.5843\n    Batch 700/875, Loss: 0.5758\n    Batch 710/875, Loss: 0.4626\n    Batch 720/875, Loss: 0.6387\n    Batch 730/875, Loss: 0.4906\n    Batch 740/875, Loss: 0.9347\n    Batch 750/875, Loss: 0.4293\n    Batch 760/875, Loss: 0.5653\n    Batch 770/875, Loss: 1.1996\n    Batch 780/875, Loss: 0.5280\n    Batch 790/875, Loss: 0.6488\n    Batch 800/875, Loss: 0.5705\n    Batch 810/875, Loss: 0.5595\n    Batch 820/875, Loss: 1.9186\n    Batch 830/875, Loss: 0.9733\n    Batch 840/875, Loss: 0.9866\n    Batch 850/875, Loss: 0.4550\n    Batch 860/875, Loss: 0.6991\n    Batch 870/875, Loss: 0.7885\n    Batch 875/875, Loss: 1.0758\n    BAT Eval 56: LR=0.000660, Dropout=0.00, BS=8\n    First batch loaded, starting training...\n    Batch 10/875, Loss: 0.7026\n    Batch 20/875, Loss: 0.8034\n    Batch 30/875, Loss: 1.7948\n    Batch 40/875, Loss: 0.8694\n    Batch 50/875, Loss: 0.6778\n    Batch 60/875, Loss: 1.3520\n    Batch 70/875, Loss: 0.7788\n    Batch 80/875, Loss: 0.6939\n    Batch 90/875, Loss: 0.7451\n    Batch 100/875, Loss: 0.5897\n    Batch 110/875, Loss: 1.1387\n    Batch 120/875, Loss: 1.1531\n    Batch 130/875, Loss: 1.9569\n    Batch 140/875, Loss: 1.5232\n    Batch 150/875, Loss: 0.5899\n    Batch 160/875, Loss: 0.8630\n    Batch 170/875, Loss: 0.9719\n    Batch 180/875, Loss: 1.2287\n    Batch 190/875, Loss: 0.9191\n    Batch 200/875, Loss: 0.8791\n    Batch 210/875, Loss: 0.7448\n    Batch 220/875, Loss: 0.5938\n    Batch 230/875, Loss: 0.3557\n    Batch 240/875, Loss: 0.5274\n    Batch 250/875, Loss: 0.4127\n    Batch 260/875, Loss: 1.6394\n    Batch 270/875, Loss: 0.6666\n    Batch 280/875, Loss: 1.0207\n    Batch 290/875, Loss: 0.8530\n    Batch 300/875, Loss: 0.3233\n    Batch 310/875, Loss: 0.5521\n    Batch 320/875, Loss: 0.4092\n    Batch 330/875, Loss: 0.4057\n    Batch 340/875, Loss: 0.7559\n    Batch 350/875, Loss: 0.7537\n    Batch 360/875, Loss: 0.4178\n    Batch 370/875, Loss: 0.9132\n    Batch 380/875, Loss: 0.7316\n    Batch 390/875, Loss: 0.9470\n    Batch 400/875, Loss: 0.6626\n    Batch 410/875, Loss: 1.4403\n    Batch 420/875, Loss: 0.6751\n    Batch 430/875, Loss: 1.4646\n    Batch 440/875, Loss: 0.4815\n    Batch 450/875, Loss: 0.6853\n    Batch 460/875, Loss: 1.0377\n    Batch 470/875, Loss: 2.3059\n    Batch 480/875, Loss: 0.5954\n    Batch 490/875, Loss: 0.3437\n    Batch 500/875, Loss: 0.8640\n    Batch 510/875, Loss: 0.8259\n    Batch 520/875, Loss: 0.7524\n    Batch 530/875, Loss: 1.6109\n    Batch 540/875, Loss: 0.6159\n    Batch 550/875, Loss: 1.8587\n    Batch 560/875, Loss: 0.4673\n    Batch 570/875, Loss: 0.5728\n    Batch 580/875, Loss: 0.4530\n    Batch 590/875, Loss: 0.4295\n    Batch 600/875, Loss: 0.4967\n    Batch 610/875, Loss: 0.6999\n    Batch 620/875, Loss: 0.3133\n    Batch 630/875, Loss: 0.7437\n    Batch 640/875, Loss: 0.4842\n    Batch 650/875, Loss: 0.8829\n    Batch 660/875, Loss: 0.8226\n    Batch 670/875, Loss: 0.4106\n    Batch 680/875, Loss: 0.3278\n    Batch 690/875, Loss: 0.4933\n    Batch 700/875, Loss: 0.3412\n    Batch 710/875, Loss: 0.2423\n    Batch 720/875, Loss: 0.7277\n    Batch 730/875, Loss: 0.9775\n    Batch 740/875, Loss: 1.1819\n    Batch 750/875, Loss: 0.9286\n    Batch 760/875, Loss: 0.5997\n    Batch 770/875, Loss: 0.3271\n    Batch 780/875, Loss: 0.5199\n    Batch 790/875, Loss: 0.9469\n    Batch 800/875, Loss: 0.6643\n    Batch 810/875, Loss: 0.6290\n    Batch 820/875, Loss: 0.4648\n    Batch 830/875, Loss: 0.2977\n    Batch 840/875, Loss: 2.5066\n    Batch 850/875, Loss: 0.5735\n    Batch 860/875, Loss: 1.0929\n    Batch 870/875, Loss: 1.0573\n    Batch 875/875, Loss: 0.3141\n    BAT Eval 57: LR=0.001334, Dropout=0.11, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.9912\n    Batch 20/219, Loss: 0.4794\n    Batch 30/219, Loss: 0.5222\n    Batch 40/219, Loss: 0.4093\n    Batch 50/219, Loss: 0.6513\n    Batch 60/219, Loss: 0.4330\n    Batch 70/219, Loss: 1.2011\n    Batch 80/219, Loss: 0.5033\n    Batch 90/219, Loss: 0.4460\n    Batch 100/219, Loss: 0.5794\n    Batch 110/219, Loss: 0.1874\n    Batch 120/219, Loss: 0.2004\n    Batch 130/219, Loss: 0.2332\n    Batch 140/219, Loss: 0.2192\n    Batch 150/219, Loss: 0.2184\n    Batch 160/219, Loss: 0.3038\n    Batch 170/219, Loss: 0.1849\n    Batch 180/219, Loss: 0.3912\n    Batch 190/219, Loss: 0.3896\n    Batch 200/219, Loss: 0.1822\n    Batch 210/219, Loss: 0.2255\n    Batch 219/219, Loss: 0.4236\n    BAT Eval 58: LR=0.001307, Dropout=0.11, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 1.1459\n    Batch 20/219, Loss: 0.6398\n    Batch 30/219, Loss: 0.6658\n    Batch 40/219, Loss: 0.4056\n    Batch 50/219, Loss: 0.3226\n    Batch 60/219, Loss: 0.3210\n    Batch 70/219, Loss: 0.4022\n    Batch 80/219, Loss: 0.5869\n    Batch 90/219, Loss: 0.3834\n    Batch 100/219, Loss: 0.2255\n    Batch 110/219, Loss: 0.4944\n    Batch 120/219, Loss: 0.2727\n    Batch 130/219, Loss: 0.4634\n    Batch 140/219, Loss: 0.1775\n    Batch 150/219, Loss: 0.6868\n    Batch 160/219, Loss: 0.5461\n    Batch 170/219, Loss: 0.3723\n    Batch 180/219, Loss: 0.4093\n    Batch 190/219, Loss: 0.4346\n    Batch 200/219, Loss: 0.1955\n    Batch 210/219, Loss: 0.2747\n    Batch 219/219, Loss: 0.1823\n    BAT Eval 59: LR=0.000010, Dropout=0.53, BS=8\n    First batch loaded, starting training...\n    Batch 10/875, Loss: 1.4163\n    Batch 20/875, Loss: 1.4397\n    Batch 30/875, Loss: 1.4341\n    Batch 40/875, Loss: 1.4530\n    Batch 50/875, Loss: 1.4835\n    Batch 60/875, Loss: 1.3797\n    Batch 70/875, Loss: 1.4081\n    Batch 80/875, Loss: 1.3405\n    Batch 90/875, Loss: 1.4208\n    Batch 100/875, Loss: 1.3128\n    Batch 110/875, Loss: 1.3313\n    Batch 120/875, Loss: 1.2990\n    Batch 130/875, Loss: 1.2693\n    Batch 140/875, Loss: 1.3099\n    Batch 150/875, Loss: 1.2993\n    Batch 160/875, Loss: 1.2790\n    Batch 170/875, Loss: 1.3406\n    Batch 180/875, Loss: 1.2054\n    Batch 190/875, Loss: 1.3656\n    Batch 200/875, Loss: 1.3952\n    Batch 210/875, Loss: 1.2234\n    Batch 220/875, Loss: 1.2107\n    Batch 230/875, Loss: 1.4336\n    Batch 240/875, Loss: 1.2492\n    Batch 250/875, Loss: 1.3229\n    Batch 260/875, Loss: 1.3474\n    Batch 270/875, Loss: 1.1347\n    Batch 280/875, Loss: 1.2159\n    Batch 290/875, Loss: 1.1040\n    Batch 300/875, Loss: 1.1797\n    Batch 310/875, Loss: 1.0846\n    Batch 320/875, Loss: 1.3794\n    Batch 330/875, Loss: 1.1702\n    Batch 340/875, Loss: 1.2148\n    Batch 350/875, Loss: 1.2020\n    Batch 360/875, Loss: 1.3235\n    Batch 370/875, Loss: 1.0201\n    Batch 380/875, Loss: 1.5725\n    Batch 390/875, Loss: 1.0195\n    Batch 400/875, Loss: 1.1501\n    Batch 410/875, Loss: 1.3293\n    Batch 420/875, Loss: 1.1549\n    Batch 430/875, Loss: 1.3236\n    Batch 440/875, Loss: 0.9996\n    Batch 450/875, Loss: 1.2293\n    Batch 460/875, Loss: 1.1230\n    Batch 470/875, Loss: 1.2805\n    Batch 480/875, Loss: 1.1154\n    Batch 490/875, Loss: 1.2084\n    Batch 500/875, Loss: 1.0359\n    Batch 510/875, Loss: 0.9657\n    Batch 520/875, Loss: 1.0570\n    Batch 530/875, Loss: 1.6920\n    Batch 540/875, Loss: 1.6744\n    Batch 550/875, Loss: 1.0000\n    Batch 560/875, Loss: 1.2182\n    Batch 570/875, Loss: 0.9087\n    Batch 580/875, Loss: 1.1414\n    Batch 590/875, Loss: 1.1193\n    Batch 600/875, Loss: 1.2567\n    Batch 610/875, Loss: 0.9641\n    Batch 620/875, Loss: 0.9616\n    Batch 630/875, Loss: 0.9597\n    Batch 640/875, Loss: 1.7066\n    Batch 650/875, Loss: 0.8690\n    Batch 660/875, Loss: 1.2606\n    Batch 670/875, Loss: 1.2113\n    Batch 680/875, Loss: 0.7792\n    Batch 690/875, Loss: 1.2680\n    Batch 700/875, Loss: 1.1252\n    Batch 710/875, Loss: 0.9307\n    Batch 720/875, Loss: 1.0002\n    Batch 730/875, Loss: 1.2053\n    Batch 740/875, Loss: 1.0478\n    Batch 750/875, Loss: 1.0638\n    Batch 760/875, Loss: 1.1723\n    Batch 770/875, Loss: 0.8278\n    Batch 780/875, Loss: 1.0546\n    Batch 790/875, Loss: 1.0109\n    Batch 800/875, Loss: 1.1877\n    Batch 810/875, Loss: 1.1455\n    Batch 820/875, Loss: 1.0132\n    Batch 830/875, Loss: 1.1327\n    Batch 840/875, Loss: 0.8822\n    Batch 850/875, Loss: 1.3418\n    Batch 860/875, Loss: 0.9222\n    Batch 870/875, Loss: 0.9290\n    Batch 875/875, Loss: 0.9486\n    BAT Eval 60: LR=0.001330, Dropout=0.11, BS=32\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.8522\n    Batch 20/219, Loss: 0.8431\n    Batch 30/219, Loss: 0.7976\n    Batch 40/219, Loss: 0.3710\n    Batch 50/219, Loss: 0.4131\n    Batch 60/219, Loss: 0.4345\n    Batch 70/219, Loss: 0.2957\n    Batch 80/219, Loss: 0.4146\n    Batch 90/219, Loss: 0.3167\n    Batch 100/219, Loss: 0.5748\n    Batch 110/219, Loss: 0.3087\n    Batch 120/219, Loss: 0.9543\n    Batch 130/219, Loss: 0.3001\n    Batch 140/219, Loss: 0.2002\n    Batch 150/219, Loss: 0.5839\n    Batch 160/219, Loss: 0.3112\n    Batch 170/219, Loss: 0.3323\n    Batch 180/219, Loss: 0.1436\n    Batch 190/219, Loss: 0.3075\n    Batch 200/219, Loss: 0.1707\n    Batch 210/219, Loss: 0.2738\n    Batch 219/219, Loss: 0.1476\n  BAT Iter 4/4: Best = 0.8218\n\nBAT Result: Score=0.8218, Time=1838.8s\n  Best Hyperparams: {'lr': np.float64(0.0012842246670162053), 'weight_decay': np.float64(0.0006161896679975339), 'dropout': np.float64(0.1091762005774508), 'unfreeze_epoch': 0, 'batch_size': 32, 'augment_strength': np.float64(1.1701413127764428)}\n\n--------------------------------------------------\nPHASE 1 SUMMARY\n--------------------------------------------------\nalgorithm  best_score  time_seconds       lr  weight_decay  dropout  unfreeze_epoch  batch_size  augment_strength\n       DE    0.797766   2474.866984 0.000107      0.005582 0.299811               0           8          1.108017\n      GWO    0.828895   1697.871904 0.000698      0.007269 0.600000               0          32          0.790954\n      PSO    0.840161   1790.982354 0.000933      0.003895 0.312052               0          32          0.577668\n      BAT    0.821778   1838.819566 0.001284      0.000616 0.109176               0          32          1.170141\n  Saved: plots/phase_1_algorithm_comparison.png\n  Saved: plots/phase_1_convergence_curves.png\n\nPhase 1 results saved to: /kaggle/working/runs/20251224_062739_d69fab59/phase1_results.csv\nBest hyperparameters found: {'lr': np.float64(0.0009333928602728289), 'weight_decay': np.float64(0.0038949077998631262), 'dropout': np.float64(0.3120520836413773), 'unfreeze_epoch': 0, 'batch_size': 32, 'augment_strength': np.float64(0.5776677691991392)}\n","output_type":"stream"}],"execution_count":16},{"id":"acbf9d9c","cell_type":"markdown","source":"## Phase 2: SA-Tuned WOA and GWO\n\nUse Simulated Annealing to optimize parameters of Whale Optimization Algorithm and Grey Wolf Optimizer.","metadata":{}},{"id":"e3725037","cell_type":"code","source":"print(\"\\n\" + \"#\"*60)\nprint(\"# PHASE 2: SA-TUNED WOA AND GWO\")\nprint(\"#\"*60)\n\nphase2_results = run_phase2_optimization(datasets, metadata, config, device, run_dir)\n\n# Save Phase 2 results\nphase2_path = os.path.join(run_dir, 'phase2_algo_param_tuning.csv')\nphase2_df = pd.DataFrame([\n    {'algorithm': 'WOA', 'tuned_params': str(phase2_results['WOA']['tuned_params']),\n     'score': phase2_results['WOA']['best_score'], 'time': phase2_results['WOA']['time_seconds']},\n    {'algorithm': 'GWO', 'tuned_params': str(phase2_results['GWO']['tuned_params']),\n     'score': phase2_results['GWO']['best_score'], 'time': phase2_results['GWO']['time_seconds']}\n])\nphase2_df.to_csv(phase2_path, index=False)\nprint(f\"\\nPhase 2 results saved to: {phase2_path}\")\n\n# Memory cleanup after Phase 2\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\ngc.collect()\n\n# Select top metaheuristics\nprint(\"\\n\" + \"#\"*60)\nprint(\"# SELECTING TOP METAHEURISTICS\")\nprint(\"#\"*60)\n\ntop_algorithms = select_top_metaheuristics(phase1_results, phase2_results, run_dir)\nprint(f\"Top algorithms selected: {top_algorithms}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T08:53:39.527401Z","iopub.execute_input":"2025-12-24T08:53:39.528044Z","iopub.status.idle":"2025-12-24T08:57:49.726069Z","shell.execute_reply.started":"2025-12-24T08:53:39.528018Z","shell.execute_reply":"2025-12-24T08:57:49.725235Z"}},"outputs":[{"name":"stdout","text":"\n############################################################\n# PHASE 2: SA-TUNED WOA AND GWO\n############################################################\n\n==================================================\nPHASE 2: SA TUNING OF WOA AND GWO\n==================================================\n\n--- Tuning WOA with SA ---\n    First batch loaded, starting training...\n    Batch 10/438, Loss: 1.3404\n    Batch 20/438, Loss: 1.3090\n    Batch 30/438, Loss: 1.3996\n    Batch 40/438, Loss: 1.2212\n    Batch 50/438, Loss: 1.1816\n    Batch 60/438, Loss: 1.2115\n    Batch 70/438, Loss: 1.2159\n    Batch 80/438, Loss: 1.0637\n    Batch 90/438, Loss: 1.2206\n    Batch 100/438, Loss: 0.9498\n    Batch 110/438, Loss: 1.0966\n    Batch 120/438, Loss: 1.2855\n    Batch 130/438, Loss: 1.0729\n    Batch 140/438, Loss: 1.2710\n    Batch 150/438, Loss: 0.9137\n    Batch 160/438, Loss: 1.3911\n    Batch 170/438, Loss: 1.0429\n    Batch 180/438, Loss: 1.3661\n    Batch 190/438, Loss: 1.3163\n    Batch 200/438, Loss: 1.2158\n    Batch 210/438, Loss: 0.7877\n    Batch 220/438, Loss: 0.8895\n    Batch 230/438, Loss: 0.7976\n    Batch 240/438, Loss: 1.3026\n    Batch 250/438, Loss: 1.2387\n    Batch 260/438, Loss: 0.8233\n    Batch 270/438, Loss: 0.8056\n    Batch 280/438, Loss: 1.3380\n    Batch 290/438, Loss: 0.6109\n    Batch 300/438, Loss: 0.7142\n    Batch 310/438, Loss: 1.0281\n    Batch 320/438, Loss: 0.9465\n    Batch 330/438, Loss: 0.8250\n    Batch 340/438, Loss: 0.8993\n    Batch 350/438, Loss: 0.8101\n    Batch 360/438, Loss: 1.3766\n    Batch 370/438, Loss: 1.0948\n    Batch 380/438, Loss: 1.2134\n    Batch 390/438, Loss: 1.2497\n    Batch 400/438, Loss: 0.9516\n    Batch 410/438, Loss: 1.0644\n    Batch 420/438, Loss: 1.3948\n    Batch 430/438, Loss: 0.8966\n    Batch 438/438, Loss: 1.0138\n    First batch loaded, starting training...\n    Batch 10/438, Loss: 1.3505\n    Batch 20/438, Loss: 1.1861\n    Batch 30/438, Loss: 1.1646\n    Batch 40/438, Loss: 1.0055\n    Batch 50/438, Loss: 0.9234\n    Batch 60/438, Loss: 0.9173\n    Batch 70/438, Loss: 0.8557\n    Batch 80/438, Loss: 1.0768\n    Batch 90/438, Loss: 0.6066\n    Batch 100/438, Loss: 0.5968\n    Batch 110/438, Loss: 0.8267\n    Batch 120/438, Loss: 0.7108\n    Batch 130/438, Loss: 0.4817\n    Batch 140/438, Loss: 0.5499\n    Batch 150/438, Loss: 0.5079\n    Batch 160/438, Loss: 0.4448\n    Batch 170/438, Loss: 0.3916\n    Batch 180/438, Loss: 0.5001\n    Batch 190/438, Loss: 0.5667\n    Batch 200/438, Loss: 0.4803\n    Batch 210/438, Loss: 0.4638\n    Batch 220/438, Loss: 1.1112\n    Batch 230/438, Loss: 0.7943\n    Batch 240/438, Loss: 0.5135\n    Batch 250/438, Loss: 0.2932\n    Batch 260/438, Loss: 0.3845\n    Batch 270/438, Loss: 0.6176\n    Batch 280/438, Loss: 1.0278\n    Batch 290/438, Loss: 0.3717\n    Batch 300/438, Loss: 0.3037\n    Batch 310/438, Loss: 0.4602\n    Batch 320/438, Loss: 0.5459\n    Batch 330/438, Loss: 0.3185\n    Batch 340/438, Loss: 0.2231\n    Batch 350/438, Loss: 0.5912\n    Batch 360/438, Loss: 0.5585\n    Batch 370/438, Loss: 0.4000\n    Batch 380/438, Loss: 0.5168\n    Batch 390/438, Loss: 1.1314\n    Batch 400/438, Loss: 0.3752\n    Batch 410/438, Loss: 0.2801\n    Batch 420/438, Loss: 0.3245\n    Batch 430/438, Loss: 0.2205\n    Batch 438/438, Loss: 0.2868\n    First batch loaded, starting training...\n    Batch 10/438, Loss: 1.4042\n    Batch 20/438, Loss: 1.3131\n    Batch 30/438, Loss: 1.2856\n    Batch 40/438, Loss: 1.1275\n    Batch 50/438, Loss: 1.1207\n    Batch 60/438, Loss: 1.1197\n    Batch 70/438, Loss: 1.1509\n    Batch 80/438, Loss: 0.8574\n    Batch 90/438, Loss: 0.9365\n    Batch 100/438, Loss: 0.7771\n    Batch 110/438, Loss: 1.0729\n    Batch 120/438, Loss: 1.1331\n    Batch 130/438, Loss: 1.0292\n    Batch 140/438, Loss: 0.7003\n    Batch 150/438, Loss: 0.8137\n    Batch 160/438, Loss: 0.6155\n    Batch 170/438, Loss: 0.7702\n    Batch 180/438, Loss: 0.9060\n    Batch 190/438, Loss: 0.3973\n    Batch 200/438, Loss: 0.6451\n    Batch 210/438, Loss: 0.5504\n    Batch 220/438, Loss: 0.6572\n    Batch 230/438, Loss: 0.7257\n    Batch 240/438, Loss: 0.6328\n    Batch 250/438, Loss: 0.9365\n    Batch 260/438, Loss: 0.8764\n    Batch 270/438, Loss: 0.5301\n    Batch 280/438, Loss: 0.6725\n    Batch 290/438, Loss: 0.9277\n    Batch 300/438, Loss: 0.6573\n    Batch 310/438, Loss: 1.3228\n    Batch 320/438, Loss: 0.4378\n    Batch 330/438, Loss: 0.7480\n    Batch 340/438, Loss: 0.4303\n    Batch 350/438, Loss: 1.1794\n    Batch 360/438, Loss: 0.4431\n    Batch 370/438, Loss: 0.3904\n    Batch 380/438, Loss: 0.6148\n    Batch 390/438, Loss: 0.3907\n    Batch 400/438, Loss: 0.8922\n    Batch 410/438, Loss: 1.8788\n    Batch 420/438, Loss: 0.4319\n    Batch 430/438, Loss: 0.4006\n    Batch 438/438, Loss: 0.3320\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 1.1606\n    Batch 20/219, Loss: 0.8451\n    Batch 30/219, Loss: 0.6970\n    Batch 40/219, Loss: 0.5434\n    Batch 50/219, Loss: 0.6668\n    Batch 60/219, Loss: 0.7593\n    Batch 70/219, Loss: 0.4661\n    Batch 80/219, Loss: 0.3418\n    Batch 90/219, Loss: 0.3498\n    Batch 100/219, Loss: 0.4856\n    Batch 110/219, Loss: 0.2994\n    Batch 120/219, Loss: 0.3091\n    Batch 130/219, Loss: 0.3309\n    Batch 140/219, Loss: 0.4382\n    Batch 150/219, Loss: 0.2614\n    Batch 160/219, Loss: 0.4875\n    Batch 170/219, Loss: 0.3194\n    Batch 180/219, Loss: 0.3450\n    Batch 190/219, Loss: 0.5653\n    Batch 200/219, Loss: 0.3711\n    Batch 210/219, Loss: 0.3420\n    Batch 219/219, Loss: 0.1818\nTuned WOA params: {'a_decay_start': np.float64(1.4330690156107841), 'a_decay_end': np.float64(0.47382592622008074), 'b_spiral': np.float64(1.7322953751869943), 'p_threshold': np.float64(0.4973390325694045)}\nWOA tuning score: 0.6900\n\n--- Tuning GWO with SA ---\n    First batch loaded, starting training...\n    Batch 10/875, Loss: 1.3678\n    Batch 20/875, Loss: 1.2764\n    Batch 30/875, Loss: 1.3517\n    Batch 40/875, Loss: 1.3429\n    Batch 50/875, Loss: 1.1606\n    Batch 60/875, Loss: 0.9705\n    Batch 70/875, Loss: 1.2458\n    Batch 80/875, Loss: 1.1118\n    Batch 90/875, Loss: 1.3544\n    Batch 100/875, Loss: 0.8388\n    Batch 110/875, Loss: 1.1541\n    Batch 120/875, Loss: 1.0981\n    Batch 130/875, Loss: 1.1378\n    Batch 140/875, Loss: 0.9635\n    Batch 150/875, Loss: 1.9439\n    Batch 160/875, Loss: 1.0474\n    Batch 170/875, Loss: 0.9987\n    Batch 180/875, Loss: 1.1413\n    Batch 190/875, Loss: 0.9340\n    Batch 200/875, Loss: 0.9626\n    Batch 210/875, Loss: 1.2757\n    Batch 220/875, Loss: 1.9846\n    Batch 230/875, Loss: 1.0519\n    Batch 240/875, Loss: 1.2944\n    Batch 250/875, Loss: 0.6732\n    Batch 260/875, Loss: 0.7173\n    Batch 270/875, Loss: 1.1848\n    Batch 280/875, Loss: 1.2002\n    Batch 290/875, Loss: 1.8794\n    Batch 300/875, Loss: 1.2177\n    Batch 310/875, Loss: 0.9695\n    Batch 320/875, Loss: 1.1632\n    Batch 330/875, Loss: 0.6441\n    Batch 340/875, Loss: 0.8436\n    Batch 350/875, Loss: 1.0199\n    Batch 360/875, Loss: 0.6491\n    Batch 370/875, Loss: 0.5828\n    Batch 380/875, Loss: 1.3284\n    Batch 390/875, Loss: 1.9504\n    Batch 400/875, Loss: 0.7092\n    Batch 410/875, Loss: 1.1041\n    Batch 420/875, Loss: 1.9380\n    Batch 430/875, Loss: 0.9485\n    Batch 440/875, Loss: 1.2298\n    Batch 450/875, Loss: 1.8657\n    Batch 460/875, Loss: 0.9524\n    Batch 470/875, Loss: 0.8759\n    Batch 480/875, Loss: 0.9937\n    Batch 490/875, Loss: 1.1895\n    Batch 500/875, Loss: 1.1585\n    Batch 510/875, Loss: 1.0173\n    Batch 520/875, Loss: 0.7949\n    Batch 530/875, Loss: 1.0010\n    Batch 540/875, Loss: 0.9741\n    Batch 550/875, Loss: 1.0972\n    Batch 560/875, Loss: 1.6577\n    Batch 570/875, Loss: 1.7371\n    Batch 580/875, Loss: 0.9979\n    Batch 590/875, Loss: 0.7654\n    Batch 600/875, Loss: 1.0716\n    Batch 610/875, Loss: 1.2073\n    Batch 620/875, Loss: 0.7554\n    Batch 630/875, Loss: 0.6983\n    Batch 640/875, Loss: 0.9260\n    Batch 650/875, Loss: 0.9712\n    Batch 660/875, Loss: 0.6893\n    Batch 670/875, Loss: 1.2150\n    Batch 680/875, Loss: 0.7559\n    Batch 690/875, Loss: 0.6619\n    Batch 700/875, Loss: 0.7487\n    Batch 710/875, Loss: 0.9395\n    Batch 720/875, Loss: 0.7243\n    Batch 730/875, Loss: 0.7942\n    Batch 740/875, Loss: 1.1565\n    Batch 750/875, Loss: 0.5932\n    Batch 760/875, Loss: 0.7927\n    Batch 770/875, Loss: 0.7205\n    Batch 780/875, Loss: 0.8830\n    Batch 790/875, Loss: 1.7504\n    Batch 800/875, Loss: 1.0726\n    Batch 810/875, Loss: 1.1407\n    Batch 820/875, Loss: 1.1097\n    Batch 830/875, Loss: 0.9370\n    Batch 840/875, Loss: 0.7204\n    Batch 850/875, Loss: 0.7447\n    Batch 860/875, Loss: 1.6545\n    Batch 870/875, Loss: 0.5091\n    Batch 875/875, Loss: 0.6834\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 1.1391\n    Batch 20/219, Loss: 0.8173\n    Batch 30/219, Loss: 0.8413\n    Batch 40/219, Loss: 0.7288\n    Batch 50/219, Loss: 0.7491\n    Batch 60/219, Loss: 1.0284\n    Batch 70/219, Loss: 0.7535\n    Batch 80/219, Loss: 0.7991\n    Batch 90/219, Loss: 0.5473\n    Batch 100/219, Loss: 0.8860\n    Batch 110/219, Loss: 1.2297\n    Batch 120/219, Loss: 0.9015\n    Batch 130/219, Loss: 0.8987\n    Batch 140/219, Loss: 0.7459\n    Batch 150/219, Loss: 0.6788\n    Batch 160/219, Loss: 0.5706\n    Batch 170/219, Loss: 0.7072\n    Batch 180/219, Loss: 0.5698\n    Batch 190/219, Loss: 0.6618\n    Batch 200/219, Loss: 0.5201\n    Batch 210/219, Loss: 0.6533\n    Batch 219/219, Loss: 0.5785\n    First batch loaded, starting training...\n    Batch 10/875, Loss: 1.3469\n    Batch 20/875, Loss: 1.2114\n    Batch 30/875, Loss: 1.6545\n    Batch 40/875, Loss: 0.8924\n    Batch 50/875, Loss: 0.6958\n    Batch 60/875, Loss: 0.8518\n    Batch 70/875, Loss: 1.1384\n    Batch 80/875, Loss: 1.0179\n    Batch 90/875, Loss: 0.6266\n    Batch 100/875, Loss: 0.5082\n    Batch 110/875, Loss: 0.5655\n    Batch 120/875, Loss: 0.6646\n    Batch 130/875, Loss: 0.5497\n    Batch 140/875, Loss: 0.9930\n    Batch 150/875, Loss: 0.6092\n    Batch 160/875, Loss: 0.4565\n    Batch 170/875, Loss: 0.4938\n    Batch 180/875, Loss: 0.6688\n    Batch 190/875, Loss: 1.9467\n    Batch 200/875, Loss: 0.5946\n    Batch 210/875, Loss: 0.6315\n    Batch 220/875, Loss: 1.7939\n    Batch 230/875, Loss: 1.6923\n    Batch 240/875, Loss: 0.8684\n    Batch 250/875, Loss: 0.6095\n    Batch 260/875, Loss: 0.5195\n    Batch 270/875, Loss: 0.6047\n    Batch 280/875, Loss: 0.7235\n    Batch 290/875, Loss: 0.9429\n    Batch 300/875, Loss: 0.4234\n    Batch 310/875, Loss: 0.3464\n    Batch 320/875, Loss: 0.3809\n    Batch 330/875, Loss: 0.9100\n    Batch 340/875, Loss: 0.6611\n    Batch 350/875, Loss: 0.3600\n    Batch 360/875, Loss: 1.7490\n    Batch 370/875, Loss: 0.6104\n    Batch 380/875, Loss: 0.4314\n    Batch 390/875, Loss: 0.3229\n    Batch 400/875, Loss: 0.3884\n    Batch 410/875, Loss: 0.5746\n    Batch 420/875, Loss: 0.5091\n    Batch 430/875, Loss: 0.2532\n    Batch 440/875, Loss: 0.6188\n    Batch 450/875, Loss: 0.2016\n    Batch 460/875, Loss: 0.3011\n    Batch 470/875, Loss: 0.5306\n    Batch 480/875, Loss: 0.3954\n    Batch 490/875, Loss: 0.5396\n    Batch 500/875, Loss: 0.4888\n    Batch 510/875, Loss: 0.2517\n    Batch 520/875, Loss: 0.5245\n    Batch 530/875, Loss: 0.4398\n    Batch 540/875, Loss: 0.2617\n    Batch 550/875, Loss: 1.5195\n    Batch 560/875, Loss: 0.2434\n    Batch 570/875, Loss: 0.2506\n    Batch 580/875, Loss: 0.6505\n    Batch 590/875, Loss: 1.2416\n    Batch 600/875, Loss: 0.4592\n    Batch 610/875, Loss: 0.9729\n    Batch 620/875, Loss: 0.3802\n    Batch 630/875, Loss: 1.2198\n    Batch 640/875, Loss: 0.3306\n    Batch 650/875, Loss: 0.2901\n    Batch 660/875, Loss: 0.2983\n    Batch 670/875, Loss: 0.1709\n    Batch 680/875, Loss: 0.1431\n    Batch 690/875, Loss: 0.2973\n    Batch 700/875, Loss: 0.2951\n    Batch 710/875, Loss: 0.5538\n    Batch 720/875, Loss: 0.8323\n    Batch 730/875, Loss: 0.2164\n    Batch 740/875, Loss: 0.3020\n    Batch 750/875, Loss: 0.3930\n    Batch 760/875, Loss: 0.2734\n    Batch 770/875, Loss: 0.2286\n    Batch 780/875, Loss: 1.0865\n    Batch 790/875, Loss: 0.2544\n    Batch 800/875, Loss: 0.1398\n    Batch 810/875, Loss: 0.3474\n    Batch 820/875, Loss: 0.4916\n    Batch 830/875, Loss: 0.2198\n    Batch 840/875, Loss: 0.2639\n    Batch 850/875, Loss: 0.7424\n    Batch 860/875, Loss: 1.4335\n    Batch 870/875, Loss: 0.4610\n    Batch 875/875, Loss: 0.5648\n    First batch loaded, starting training...\n    Batch 10/438, Loss: 1.3889\n    Batch 20/438, Loss: 1.3370\n    Batch 30/438, Loss: 1.2734\n    Batch 40/438, Loss: 1.1964\n    Batch 50/438, Loss: 1.2434\n    Batch 60/438, Loss: 1.1759\n    Batch 70/438, Loss: 1.0402\n    Batch 80/438, Loss: 0.9720\n    Batch 90/438, Loss: 1.0956\n    Batch 100/438, Loss: 0.9897\n    Batch 110/438, Loss: 1.0711\n    Batch 120/438, Loss: 1.0234\n    Batch 130/438, Loss: 1.0912\n    Batch 140/438, Loss: 1.5181\n    Batch 150/438, Loss: 0.9927\n    Batch 160/438, Loss: 0.9213\n    Batch 170/438, Loss: 1.0561\n    Batch 180/438, Loss: 0.9929\n    Batch 190/438, Loss: 0.7898\n    Batch 200/438, Loss: 1.0408\n    Batch 210/438, Loss: 0.8599\n    Batch 220/438, Loss: 0.9252\n    Batch 230/438, Loss: 1.0128\n    Batch 240/438, Loss: 0.9005\n    Batch 250/438, Loss: 0.9182\n    Batch 260/438, Loss: 0.8784\n    Batch 270/438, Loss: 1.3178\n    Batch 280/438, Loss: 1.1494\n    Batch 290/438, Loss: 1.0938\n    Batch 300/438, Loss: 1.3296\n    Batch 310/438, Loss: 1.3436\n    Batch 320/438, Loss: 0.7748\n    Batch 330/438, Loss: 0.8558\n    Batch 340/438, Loss: 0.8290\n    Batch 350/438, Loss: 0.9141\n    Batch 360/438, Loss: 1.2065\n    Batch 370/438, Loss: 0.9921\n    Batch 380/438, Loss: 0.9533\n    Batch 390/438, Loss: 0.9773\n    Batch 400/438, Loss: 0.8847\n    Batch 410/438, Loss: 1.0537\n    Batch 420/438, Loss: 0.9451\n    Batch 430/438, Loss: 0.8637\n    Batch 438/438, Loss: 0.8141\nTuned GWO params: {'a_start': np.float64(2.763281028719307), 'a_end': np.float64(0.05308838208640686)}\nGWO tuning score: 0.7350\n\nPhase 2 results saved to: /kaggle/working/runs/20251224_062739_d69fab59/phase2_algo_param_tuning.csv\n\n############################################################\n# SELECTING TOP METAHEURISTICS\n############################################################\n\n==================================================\nSELECTING TOP METAHEURISTICS\n==================================================\n\nMetaheuristic Rankings:\nalgorithm    score        time source  efficiency  rank\n      PSO 0.840161 1790.982354 phase1    0.000469     1\n      GWO 0.828895 1697.871904 phase1    0.000488     2\n      BAT 0.821778 1838.819566 phase1    0.000447     3\n       DE 0.797766 2474.866984 phase1    0.000322     4\nGWO_tuned 0.735045  130.255906 phase2    0.005643     5\nWOA_tuned 0.690008  118.925990 phase2    0.005802     6\n  Saved: plots/metaheuristic_ranking.png\n\nTop 4 selected: ['PSO', 'GWO', 'BAT', 'DE']\nTop algorithms selected: ['PSO', 'GWO', 'BAT', 'DE']\n","output_type":"stream"}],"execution_count":17},{"id":"df45046c","cell_type":"markdown","source":"## XAI Optimization\n\nOptimize Grad-CAM and SHAP parameters using the top-performing metaheuristic algorithms.","metadata":{}},{"id":"1730f97d","cell_type":"code","source":"print(\"\\n\" + \"#\"*60)\nprint(\"# XAI OPTIMIZATION\")\nprint(\"#\"*60)\n\n# Create and train model for XAI\nmodel = create_model(config, device)\n\n# Quick training in full pipeline\ntrain_epochs = max(1, config.EPOCHS // 2)\n\nprint(f\"\\nTraining model for XAI ({train_epochs} epochs)...\")\nmodel, _, _ = train_model(\n    model, loaders, config, metadata, device, run_dir,\n    epochs=train_epochs, verbose=True\n)\n\n# Run XAI optimization\nxai_params = run_xai_optimization(\n    model, loaders, top_algorithms, config, device, run_dir\n)\n\n# Save XAI params\nxai_params_path = os.path.join(run_dir, 'xai_params_best.json')\nwith open(xai_params_path, 'w') as f:\n    json.dump(xai_params, f, indent=2, default=str)\nprint(f\"XAI parameters saved to: {xai_params_path}\")\n\n# Memory cleanup\ndel model\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T08:57:49.727193Z","iopub.execute_input":"2025-12-24T08:57:49.727627Z","iopub.status.idle":"2025-12-24T09:10:23.233992Z","shell.execute_reply.started":"2025-12-24T08:57:49.727587Z","shell.execute_reply":"2025-12-24T09:10:23.233418Z"}},"outputs":[{"name":"stdout","text":"\n############################################################\n# XAI OPTIMIZATION\n############################################################\n\nModel created:\n  Total parameters: 4,336,512\n  Trainable parameters: 4,336,512\n\nTraining model for XAI (2 epochs)...\nBackbone frozen for first 2 epochs\n\n  Starting Epoch 1/2...\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.7038\n    Batch 20/219, Loss: 0.9451\n    Batch 30/219, Loss: 0.7208\n    Batch 40/219, Loss: 0.7239\n    Batch 50/219, Loss: 1.0572\n    Batch 60/219, Loss: 0.7405\n    Batch 70/219, Loss: 0.5988\n    Batch 80/219, Loss: 0.6459\n    Batch 90/219, Loss: 0.7158\n    Batch 100/219, Loss: 0.5580\n    Batch 110/219, Loss: 0.6682\n    Batch 120/219, Loss: 0.9432\n    Batch 130/219, Loss: 1.0991\n    Batch 140/219, Loss: 0.6657\n    Batch 150/219, Loss: 1.0934\n    Batch 160/219, Loss: 0.5089\n    Batch 170/219, Loss: 0.7797\n    Batch 180/219, Loss: 0.4283\n    Batch 190/219, Loss: 0.5257\n    Batch 200/219, Loss: 0.6812\n    Batch 210/219, Loss: 0.5646\n    Batch 219/219, Loss: 0.5958\nEpoch 1/2 (25.8s) - Train Loss: 0.7270, Val F1: 0.3763, Val Acc: 0.3953\n\n  Starting Epoch 2/2...\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.5756\n    Batch 20/219, Loss: 0.6368\n    Batch 30/219, Loss: 0.7974\n    Batch 40/219, Loss: 0.6771\n    Batch 50/219, Loss: 0.5892\n    Batch 60/219, Loss: 0.5710\n    Batch 70/219, Loss: 0.6322\n    Batch 80/219, Loss: 0.6022\n    Batch 90/219, Loss: 0.5863\n    Batch 100/219, Loss: 0.7345\n    Batch 110/219, Loss: 0.6052\n    Batch 120/219, Loss: 0.6368\n    Batch 130/219, Loss: 0.5906\n    Batch 140/219, Loss: 0.4711\n    Batch 150/219, Loss: 0.5099\n    Batch 160/219, Loss: 0.7111\n    Batch 170/219, Loss: 0.7090\n    Batch 180/219, Loss: 0.6455\n    Batch 190/219, Loss: 0.4920\n    Batch 200/219, Loss: 0.4236\n    Batch 210/219, Loss: 0.4410\n    Batch 219/219, Loss: 0.7215\nEpoch 2/2 (24.6s) - Train Loss: 0.6090, Val F1: 0.4769, Val Acc: 0.5173\n\n==================================================\nXAI OPTIMIZATION\n==================================================\n\n--- Optimizing Grad-CAM parameters ---\n\nUsing PSO for Grad-CAM optimization...\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  PSO Iter 1/4: Best = 0.5713\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  PSO Iter 2/4: Best = 0.5721\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  PSO Iter 3/4: Best = 0.5721\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  PSO Iter 4/4: Best = 0.5721\n  PSO: Score=0.5721, Params={'smoothing_sigma': np.float64(2.9684815819561257), 'n_smooth_samples': 1, 'noise_std': np.float64(0.3)}\n  GradCAM target layer: backbone.conv_head\n  GradCAM heatmap shape: (224, 224), min=0.0000, max=1.0000\n  Heatmap after resize: min=0.0000, max=1.0000\n  Saved Grad-CAM plot: /kaggle/working/runs/20251224_062739_d69fab59/gradcam_PSO.png\n\nUsing GWO for Grad-CAM optimization...\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GWO Iter 1/4: Best = 0.5712\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GWO Iter 2/4: Best = 0.5712\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GWO Iter 3/4: Best = 0.5732\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GWO Iter 4/4: Best = 0.5732\n  GWO: Score=0.5732, Params={'smoothing_sigma': np.float64(1.5288410948063542), 'n_smooth_samples': 1, 'noise_std': np.float64(0.08717728293616414)}\n  GradCAM target layer: backbone.conv_head\n  GradCAM heatmap shape: (224, 224), min=0.0000, max=1.0000\n  Heatmap after resize: min=0.0000, max=1.0000\n  Saved Grad-CAM plot: /kaggle/working/runs/20251224_062739_d69fab59/gradcam_GWO.png\n\nUsing BAT for Grad-CAM optimization...\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  BAT Iter 1/4: Best = 0.5703\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  BAT Iter 2/4: Best = 0.5716\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  BAT Iter 3/4: Best = 0.5716\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  BAT Iter 4/4: Best = 0.5716\n  BAT: Score=0.5716, Params={'smoothing_sigma': np.float64(2.1278848627473144), 'n_smooth_samples': 1, 'noise_std': np.float64(0.29410797834714475)}\n  GradCAM target layer: backbone.conv_head\n  GradCAM heatmap shape: (224, 224), min=0.0000, max=1.0000\n  Heatmap after resize: min=0.0000, max=1.0000\n  Saved Grad-CAM plot: /kaggle/working/runs/20251224_062739_d69fab59/gradcam_BAT.png\n\nUsing DE for Grad-CAM optimization...\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  DE Iter 1/4: Best = 0.5672\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  DE Iter 2/4: Best = 0.5686\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  DE Iter 3/4: Best = 0.5743\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  GradCAM target layer: backbone.conv_head\n  DE Iter 4/4: Best = 0.5743\n  DE: Score=0.5743, Params={'smoothing_sigma': np.float64(1.5691058005075513), 'n_smooth_samples': 1, 'noise_std': np.float64(0.3)}\n  GradCAM target layer: backbone.conv_head\n  GradCAM heatmap shape: (224, 224), min=0.0000, max=1.0000\n  Heatmap after resize: min=0.0000, max=1.0000\n  Saved Grad-CAM plot: /kaggle/working/runs/20251224_062739_d69fab59/gradcam_DE.png\n\n--- Optimizing SHAP parameters ---\n\nUsing PSO for SHAP optimization...\n  PSO Iter 1/4: Best = 0.5636\n  PSO Iter 2/4: Best = 0.5636\n  PSO Iter 3/4: Best = 0.5681\n  PSO Iter 4/4: Best = 0.5681\n  PSO: Score=0.5681, Params={'background_size': 77, 'nsamples': 109}\n  SHAP values shape: (1, 3, 224, 224, 4)\n  SHAP array shape: (1, 3, 224, 224, 4)\n  SHAP heatmap: shape=(224, 4), min=0.000000, max=18.424990\n  Saved SHAP plot: /kaggle/working/runs/20251224_062739_d69fab59/shap_PSO.png\n\nUsing GWO for SHAP optimization...\n  GWO Iter 1/4: Best = 0.6014\n  GWO Iter 2/4: Best = 0.6014\n  GWO Iter 3/4: Best = 0.6014\n  GWO Iter 4/4: Best = 0.6014\n  GWO: Score=0.6014, Params={'background_size': 15, 'nsamples': 91}\n  SHAP values shape: (1, 3, 224, 224, 4)\n  SHAP array shape: (1, 3, 224, 224, 4)\n  SHAP heatmap: shape=(224, 4), min=0.000000, max=14.770081\n  Saved SHAP plot: /kaggle/working/runs/20251224_062739_d69fab59/shap_GWO.png\n\nUsing BAT for SHAP optimization...\n  BAT Iter 1/4: Best = 0.5925\n  BAT Iter 2/4: Best = 0.5925\n  BAT Iter 3/4: Best = 0.5925\n  BAT Iter 4/4: Best = 0.5925\n  BAT: Score=0.5925, Params={'background_size': 24, 'nsamples': 48}\n  SHAP values shape: (1, 3, 224, 224, 4)\n  SHAP array shape: (1, 3, 224, 224, 4)\n  SHAP heatmap: shape=(224, 4), min=0.000000, max=14.287801\n  Saved SHAP plot: /kaggle/working/runs/20251224_062739_d69fab59/shap_BAT.png\n\nUsing DE for SHAP optimization...\n  DE Iter 1/4: Best = 0.5917\n  DE Iter 2/4: Best = 0.6307\n  DE Iter 3/4: Best = 0.6307\n  DE Iter 4/4: Best = 0.6307\n  DE: Score=0.6307, Params={'background_size': 48, 'nsamples': 182}\n  SHAP values shape: (1, 3, 224, 224, 4)\n  SHAP array shape: (1, 3, 224, 224, 4)\n  SHAP heatmap: shape=(224, 4), min=0.000000, max=18.542673\n  Saved SHAP plot: /kaggle/working/runs/20251224_062739_d69fab59/shap_DE.png\n  Saved: plots/xai_algorithm_comparison.png\nXAI parameters saved to: /kaggle/working/runs/20251224_062739_d69fab59/xai_params_best.json\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"85485"},"metadata":{}}],"execution_count":18},{"id":"ae6f4c85","cell_type":"markdown","source":"## Final Training & Explanations\n\nTrain the final model with optimized hyperparameters and generate XAI explanations (Grad-CAM & SHAP).","metadata":{}},{"id":"3ec502f7","cell_type":"code","source":"print(\"\\n\" + \"#\"*60)\nprint(\"# FINAL TRAINING AND EXPLANATIONS\")\nprint(\"#\"*60)\n\n# Final training\nfinal_model, final_metrics, baseline_metrics = run_final_training(\n    datasets, metadata, best_hyperparams, feature_mask,\n    config, device, run_dir\n)\n\n# Generate explanations\nprint(\"\\nGenerating final explanations...\")\ngenerate_final_explanations(\n    final_model, loaders, metadata, xai_params,\n    config, device, run_dir,\n    best_hyperparams=best_hyperparams,\n    final_metrics=final_metrics\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T09:10:23.234950Z","iopub.execute_input":"2025-12-24T09:10:23.235182Z","iopub.status.idle":"2025-12-24T09:16:59.113964Z","shell.execute_reply.started":"2025-12-24T09:10:23.235161Z","shell.execute_reply":"2025-12-24T09:16:59.113273Z"}},"outputs":[{"name":"stdout","text":"\n############################################################\n# FINAL TRAINING AND EXPLANATIONS\n############################################################\n\n==================================================\nBASELINE MODEL TRAINING\n==================================================\n\nModel created:\n  Total parameters: 4,336,512\n  Trainable parameters: 4,336,512\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 1.4078\n    Batch 20/219, Loss: 1.1222\n    Batch 30/219, Loss: 1.1112\n    Batch 40/219, Loss: 1.1086\n    Batch 50/219, Loss: 1.0479\n    Batch 60/219, Loss: 1.2598\n    Batch 70/219, Loss: 0.9605\n    Batch 80/219, Loss: 0.9561\n    Batch 90/219, Loss: 0.9540\n    Batch 100/219, Loss: 1.0617\n    Batch 110/219, Loss: 1.1273\n    Batch 120/219, Loss: 1.1844\n    Batch 130/219, Loss: 1.1357\n    Batch 140/219, Loss: 0.9962\n    Batch 150/219, Loss: 0.9664\n    Batch 160/219, Loss: 0.9353\n    Batch 170/219, Loss: 1.0676\n    Batch 180/219, Loss: 0.9708\n    Batch 190/219, Loss: 1.0231\n    Batch 200/219, Loss: 0.9700\n    Batch 210/219, Loss: 1.2507\n    Batch 219/219, Loss: 1.1005\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 1.1430\n    Batch 20/219, Loss: 0.9214\n    Batch 30/219, Loss: 1.0869\n    Batch 40/219, Loss: 0.9213\n    Batch 50/219, Loss: 1.1042\n    Batch 60/219, Loss: 1.2405\n    Batch 70/219, Loss: 0.8940\n    Batch 80/219, Loss: 1.1737\n    Batch 90/219, Loss: 0.9052\n    Batch 100/219, Loss: 0.9255\n    Batch 110/219, Loss: 0.9256\n    Batch 120/219, Loss: 0.8944\n    Batch 130/219, Loss: 1.0064\n    Batch 140/219, Loss: 1.0677\n    Batch 150/219, Loss: 1.2581\n    Batch 160/219, Loss: 0.8854\n    Batch 170/219, Loss: 0.9581\n    Batch 180/219, Loss: 0.9703\n    Batch 190/219, Loss: 0.8978\n    Batch 200/219, Loss: 0.7808\n    Batch 210/219, Loss: 0.9411\n    Batch 219/219, Loss: 0.8686\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 1.1908\n    Batch 20/219, Loss: 0.7862\n    Batch 30/219, Loss: 1.0020\n    Batch 40/219, Loss: 0.6749\n    Batch 50/219, Loss: 0.7357\n    Batch 60/219, Loss: 1.0625\n    Batch 70/219, Loss: 1.0307\n    Batch 80/219, Loss: 0.6690\n    Batch 90/219, Loss: 0.7737\n    Batch 100/219, Loss: 0.6341\n    Batch 110/219, Loss: 0.9111\n    Batch 120/219, Loss: 0.4119\n    Batch 130/219, Loss: 0.8493\n    Batch 140/219, Loss: 0.8777\n    Batch 150/219, Loss: 0.6621\n    Batch 160/219, Loss: 0.3926\n    Batch 170/219, Loss: 0.7487\n    Batch 180/219, Loss: 0.7713\n    Batch 190/219, Loss: 0.4772\n    Batch 200/219, Loss: 0.3818\n    Batch 210/219, Loss: 0.5135\n    Batch 219/219, Loss: 0.4776\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.4705\n    Batch 20/219, Loss: 0.4738\n    Batch 30/219, Loss: 0.3161\n    Batch 40/219, Loss: 0.3719\n    Batch 50/219, Loss: 0.4216\n    Batch 60/219, Loss: 0.5036\n    Batch 70/219, Loss: 0.4372\n    Batch 80/219, Loss: 0.4778\n    Batch 90/219, Loss: 0.3281\n    Batch 100/219, Loss: 0.2376\n    Batch 110/219, Loss: 0.6326\n    Batch 120/219, Loss: 0.1900\n    Batch 130/219, Loss: 0.2570\n    Batch 140/219, Loss: 0.2934\n    Batch 150/219, Loss: 0.3542\n    Batch 160/219, Loss: 0.6755\n    Batch 170/219, Loss: 0.3835\n    Batch 180/219, Loss: 0.4932\n    Batch 190/219, Loss: 0.3949\n    Batch 200/219, Loss: 0.5844\n    Batch 210/219, Loss: 0.3816\n    Batch 219/219, Loss: 0.4734\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.4399\n    Batch 20/219, Loss: 0.2600\n    Batch 30/219, Loss: 0.2421\n    Batch 40/219, Loss: 0.3327\n    Batch 50/219, Loss: 0.5473\n    Batch 60/219, Loss: 0.2487\n    Batch 70/219, Loss: 0.6439\n    Batch 80/219, Loss: 0.4435\n    Batch 90/219, Loss: 0.1668\n    Batch 100/219, Loss: 0.1998\n    Batch 110/219, Loss: 0.2283\n    Batch 120/219, Loss: 0.3717\n    Batch 130/219, Loss: 0.2079\n    Batch 140/219, Loss: 0.3341\n    Batch 150/219, Loss: 0.3369\n    Batch 160/219, Loss: 0.1805\n    Batch 170/219, Loss: 0.2592\n    Batch 180/219, Loss: 0.2750\n    Batch 190/219, Loss: 0.2294\n    Batch 200/219, Loss: 0.2461\n    Batch 210/219, Loss: 0.3491\n    Batch 219/219, Loss: 0.2195\n\nBaseline Results:\n  Accuracy: 0.8520\n  Balanced Accuracy: 0.8787\n  Macro F1: 0.8747\n\n==================================================\nFINAL MODEL TRAINING\n==================================================\nBest hyperparameters: {'lr': np.float64(0.0009333928602728289), 'weight_decay': np.float64(0.0038949077998631262), 'dropout': np.float64(0.3120520836413773), 'unfreeze_epoch': 0, 'batch_size': 32, 'augment_strength': np.float64(0.5776677691991392)}\n\nModel created:\n  Total parameters: 4,336,512\n  Trainable parameters: 4,336,512\nApplied feature mask: 640/1280 features\n\n  Starting Epoch 1/7...\nBackbone unfrozen\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.6880\n    Batch 20/219, Loss: 1.1605\n    Batch 30/219, Loss: 0.4021\n    Batch 40/219, Loss: 0.4374\n    Batch 50/219, Loss: 0.5555\n    Batch 60/219, Loss: 0.5608\n    Batch 70/219, Loss: 0.4444\n    Batch 80/219, Loss: 0.4334\n    Batch 90/219, Loss: 0.1880\n    Batch 100/219, Loss: 0.3356\n    Batch 110/219, Loss: 0.3799\n    Batch 120/219, Loss: 0.3119\n    Batch 130/219, Loss: 0.2309\n    Batch 140/219, Loss: 0.8167\n    Batch 150/219, Loss: 0.3078\n    Batch 160/219, Loss: 0.2770\n    Batch 170/219, Loss: 0.2171\n    Batch 180/219, Loss: 0.1684\n    Batch 190/219, Loss: 0.2060\n    Batch 200/219, Loss: 0.2377\n    Batch 210/219, Loss: 0.1891\n    Batch 219/219, Loss: 0.2033\nEpoch 1/7 (29.0s) - Train Loss: 0.4290, Val F1: 0.7183, Val Acc: 0.6707\n\n  Starting Epoch 2/7...\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.1277\n    Batch 20/219, Loss: 0.6188\n    Batch 30/219, Loss: 0.2906\n    Batch 40/219, Loss: 0.2764\n    Batch 50/219, Loss: 0.1062\n    Batch 60/219, Loss: 0.2014\n    Batch 70/219, Loss: 0.1820\n    Batch 80/219, Loss: 0.2736\n    Batch 90/219, Loss: 0.1977\n    Batch 100/219, Loss: 0.2206\n    Batch 110/219, Loss: 0.1861\n    Batch 120/219, Loss: 0.1905\n    Batch 130/219, Loss: 0.1607\n    Batch 140/219, Loss: 0.1205\n    Batch 150/219, Loss: 0.4426\n    Batch 160/219, Loss: 0.3328\n    Batch 170/219, Loss: 0.1249\n    Batch 180/219, Loss: 0.2371\n    Batch 190/219, Loss: 0.1300\n    Batch 200/219, Loss: 0.1578\n    Batch 210/219, Loss: 0.1433\n    Batch 219/219, Loss: 0.0343\nEpoch 2/7 (28.6s) - Train Loss: 0.2281, Val F1: 0.8730, Val Acc: 0.8420\n\n  Starting Epoch 3/7...\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.1983\n    Batch 20/219, Loss: 0.1373\n    Batch 30/219, Loss: 0.1654\n    Batch 40/219, Loss: 0.1679\n    Batch 50/219, Loss: 0.1915\n    Batch 60/219, Loss: 0.1679\n    Batch 70/219, Loss: 0.0844\n    Batch 80/219, Loss: 0.7901\n    Batch 90/219, Loss: 0.1097\n    Batch 100/219, Loss: 0.2528\n    Batch 110/219, Loss: 0.1214\n    Batch 120/219, Loss: 0.1929\n    Batch 130/219, Loss: 0.3092\n    Batch 140/219, Loss: 0.3176\n    Batch 150/219, Loss: 0.1937\n    Batch 160/219, Loss: 0.1644\n    Batch 170/219, Loss: 0.1723\n    Batch 180/219, Loss: 0.2428\n    Batch 190/219, Loss: 0.1072\n    Batch 200/219, Loss: 0.2940\n    Batch 210/219, Loss: 0.1342\n    Batch 219/219, Loss: 0.1464\nEpoch 3/7 (29.0s) - Train Loss: 0.2055, Val F1: 0.8610, Val Acc: 0.8240\n\n  Starting Epoch 4/7...\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.1106\n    Batch 20/219, Loss: 0.1082\n    Batch 30/219, Loss: 0.2925\n    Batch 40/219, Loss: 0.1848\n    Batch 50/219, Loss: 0.1072\n    Batch 60/219, Loss: 0.5895\n    Batch 70/219, Loss: 0.1004\n    Batch 80/219, Loss: 0.6063\n    Batch 90/219, Loss: 0.2209\n    Batch 100/219, Loss: 0.0883\n    Batch 110/219, Loss: 0.1008\n    Batch 120/219, Loss: 0.1201\n    Batch 130/219, Loss: 0.1105\n    Batch 140/219, Loss: 0.1686\n    Batch 150/219, Loss: 0.0966\n    Batch 160/219, Loss: 0.1399\n    Batch 170/219, Loss: 0.0963\n    Batch 180/219, Loss: 0.0949\n    Batch 190/219, Loss: 0.2759\n    Batch 200/219, Loss: 0.1239\n    Batch 210/219, Loss: 0.0317\n    Batch 219/219, Loss: 0.1661\nEpoch 4/7 (28.9s) - Train Loss: 0.1665, Val F1: 0.8347, Val Acc: 0.7893\n\n  Starting Epoch 5/7...\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.1148\n    Batch 20/219, Loss: 0.1552\n    Batch 30/219, Loss: 0.0823\n    Batch 40/219, Loss: 0.1202\n    Batch 50/219, Loss: 0.8155\n    Batch 60/219, Loss: 0.1252\n    Batch 70/219, Loss: 0.7020\n    Batch 80/219, Loss: 0.2198\n    Batch 90/219, Loss: 0.1401\n    Batch 100/219, Loss: 0.1026\n    Batch 110/219, Loss: 0.1533\n    Batch 120/219, Loss: 0.1618\n    Batch 130/219, Loss: 0.1120\n    Batch 140/219, Loss: 0.1685\n    Batch 150/219, Loss: 0.0800\n    Batch 160/219, Loss: 0.1341\n    Batch 170/219, Loss: 0.0548\n    Batch 180/219, Loss: 0.2489\n    Batch 190/219, Loss: 0.0400\n    Batch 200/219, Loss: 0.0967\n    Batch 210/219, Loss: 0.1998\n    Batch 219/219, Loss: 0.1131\nEpoch 5/7 (31.3s) - Train Loss: 0.1546, Val F1: 0.9182, Val Acc: 0.8980\n\n  Starting Epoch 6/7...\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.2704\n    Batch 20/219, Loss: 0.0662\n    Batch 30/219, Loss: 0.1428\n    Batch 40/219, Loss: 0.1308\n    Batch 50/219, Loss: 0.1015\n    Batch 60/219, Loss: 0.1858\n    Batch 70/219, Loss: 0.0501\n    Batch 80/219, Loss: 0.0654\n    Batch 90/219, Loss: 0.0563\n    Batch 100/219, Loss: 0.2127\n    Batch 110/219, Loss: 0.1536\n    Batch 120/219, Loss: 0.0241\n    Batch 130/219, Loss: 0.0320\n    Batch 140/219, Loss: 0.1122\n    Batch 150/219, Loss: 0.1003\n    Batch 160/219, Loss: 0.0735\n    Batch 170/219, Loss: 0.1127\n    Batch 180/219, Loss: 0.0696\n    Batch 190/219, Loss: 0.0816\n    Batch 200/219, Loss: 0.0405\n    Batch 210/219, Loss: 0.0159\n    Batch 219/219, Loss: 0.1373\nEpoch 6/7 (28.8s) - Train Loss: 0.1039, Val F1: 0.9395, Val Acc: 0.9267\n\n  Starting Epoch 7/7...\n    First batch loaded, starting training...\n    Batch 10/219, Loss: 0.0776\n    Batch 20/219, Loss: 0.1263\n    Batch 30/219, Loss: 0.0258\n    Batch 40/219, Loss: 0.0554\n    Batch 50/219, Loss: 0.0864\n    Batch 60/219, Loss: 0.1334\n    Batch 70/219, Loss: 0.1961\n    Batch 80/219, Loss: 0.0333\n    Batch 90/219, Loss: 0.0642\n    Batch 100/219, Loss: 0.1870\n    Batch 110/219, Loss: 0.2026\n    Batch 120/219, Loss: 0.0572\n    Batch 130/219, Loss: 0.4040\n    Batch 140/219, Loss: 0.0553\n    Batch 150/219, Loss: 0.1426\n    Batch 160/219, Loss: 0.1185\n    Batch 170/219, Loss: 0.0908\n    Batch 180/219, Loss: 0.0487\n    Batch 190/219, Loss: 0.0563\n    Batch 200/219, Loss: 0.1407\n    Batch 210/219, Loss: 0.0895\n    Batch 219/219, Loss: 0.0714\nEpoch 7/7 (29.4s) - Train Loss: 0.1382, Val F1: 0.9126, Val Acc: 0.8933\n\nFinal Test Results:\n  Accuracy: 0.9100\n  Balanced Accuracy: 0.9256\n  Macro F1: 0.9243\n\n==================================================\nPERFORMANCE COMPARISON\n==================================================\nMetric               Baseline   Optimized  Improvement \n------------------------------------------------------\nAccuracy             0.8520     0.9100     +0.0580     \nBalanced Accuracy    0.8787     0.9256     +0.0469     \nMacro F1             0.8747     0.9243     +0.0496     \n\nOptimization Summary:\n  - Feature Selection: 640/1280 features retained\n  - Hyperparameter Optimization:  Completed\n\nGenerating comparison plots...\n  Saved: plots/baseline_vs_optimized.png\n  Saved: plots/confusion_matrix_comparison.png\n\nGenerating final explanations...\n\n==================================================\nGENERATING FINAL EXPLANATIONS\n==================================================\n  GradCAM target layer: backbone.conv_head\n\nGenerating Grad-CAM visualizations...\n\nGenerating SHAP visualizations...\n  SHAP shape for Mild Dementia: (1, 3, 224, 224, 4)\n  SHAP shape for Mild Dementia: (1, 3, 224, 224, 4)\n  SHAP shape for Moderate Dementia: (1, 3, 224, 224, 4)\n  SHAP shape for Moderate Dementia: (1, 3, 224, 224, 4)\n  SHAP shape for Non Demented: (1, 3, 224, 224, 4)\n  SHAP shape for Non Demented: (1, 3, 224, 224, 4)\n  SHAP shape for Very mild Dementia: (1, 3, 224, 224, 4)\n  SHAP shape for Very mild Dementia: (1, 3, 224, 224, 4)\n\nExplanations saved to: /kaggle/working/runs/20251224_062739_d69fab59/xai_outputs\n  Saved HTML summary: /kaggle/working/runs/20251224_062739_d69fab59/xai_outputs/summary.html\n","output_type":"stream"}],"execution_count":19},{"id":"205880ca","cell_type":"markdown","source":"## Final Summary\n\nDisplay the summary of all outputs and results from the pipeline.","metadata":{}},{"id":"23b1acc7","cell_type":"code","source":"print(\"\\n\" + \"=\"*60)\nprint(\"PIPELINE COMPLETED SUCCESSFULLY\")\nprint(\"=\"*60)\nprint(f\"\\nResults saved to: {run_dir}\")\nprint(f\"\\nOutput files:\")\n\noutputs = [\n    \"config.json\",\n]\nif feature_mask is not None:\n    outputs.append(\"aco_results.json\")\nif phase1_results is not None:\n    outputs.append(\"phase1_results.csv\")\nif phase2_results:\n    outputs.append(\"phase2_algo_param_tuning.csv\")\nif phase2_results and phase1_results is not None:\n    outputs.append(\"metaheuristic_ranking.csv\")\noutputs.append(\"xai_params_best.json\")\noutputs += [\n    \"checkpoints/best.pt\",\n    \"checkpoints/final_model.pt\",\n    \"final_metrics.json\",\n    \"logs/\",\n    \"plots/\",\n    \"xai_outputs/\"\n]\n\nfor out in outputs:\n    print(f\"  - {out}\")\n\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T09:16:59.115660Z","iopub.execute_input":"2025-12-24T09:16:59.115952Z","iopub.status.idle":"2025-12-24T09:16:59.122200Z","shell.execute_reply.started":"2025-12-24T09:16:59.115928Z","shell.execute_reply":"2025-12-24T09:16:59.121675Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nPIPELINE COMPLETED SUCCESSFULLY\n============================================================\n\nResults saved to: /kaggle/working/runs/20251224_062739_d69fab59\n\nOutput files:\n  - config.json\n  - aco_results.json\n  - phase1_results.csv\n  - phase2_algo_param_tuning.csv\n  - metaheuristic_ranking.csv\n  - xai_params_best.json\n  - checkpoints/best.pt\n  - checkpoints/final_model.pt\n  - final_metrics.json\n  - logs/\n  - plots/\n  - xai_outputs/\n============================================================\n","output_type":"stream"}],"execution_count":20}]}